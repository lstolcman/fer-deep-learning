{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys_version: 2.7.5 (default, Apr  3 2014, 04:38:53) [GCC 4.7.3]\n",
      "virtual_env /home/mratkovic/.virtualenvs/py27_tf_env\n",
      "pwd /home/mratkovic/fer-deep-learning/DU3\n",
      "np  1.11.2\n",
      "tf  0.10.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pdb\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print('sys_version:', sys.version.replace('\\n', ''))\n",
    "print('virtual_env', os.environ.get('VIRTUAL_ENV', 'None'))\n",
    "print('pwd', os.getcwd())\n",
    "print('np ', np.__version__)\n",
    "print('tf ', tf.__version__)\n",
    "\n",
    "\n",
    "from IPython.core.debugger import Tracer\n",
    "BREAK_POINT = lambda: Tracer()()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0\n",
      "False\n",
      "X [['a', 'b', 'c'], ['g', 'h', 'i'], ['m', 'n', 'o']]\n",
      "Y [['b', 'c', 'd'], ['h', 'i', 'j'], ['n', 'o', 'p']]\n",
      "\n",
      "\n",
      "\n",
      "Batch: 1\n",
      "False\n",
      "X [['d', 'e', 'f'], ['j', 'k', 'l'], ['p', 'q', 'r']]\n",
      "Y [['e', 'f', 'g'], ['k', 'l', 'm'], ['q', 'r', 's']]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test 1\n",
    "dat = Dataset(3, 3)\n",
    "dat.preprocess(\"test.txt\")\n",
    "txt = \"hjdhasjdhjasdhja\"\n",
    "assert txt != dat.decode(dat.encode(txt))\n",
    "\n",
    "\n",
    "dat.create_minibatches()\n",
    "for i in range(dat.num_batches):\n",
    "    print(\"Batch:\", i)\n",
    "    f, s, t = dat.next_minibatch()\n",
    "    print(f)\n",
    "    print(\"X\", list(map(dat.decode, s)))\n",
    "    print(\"Y\", list(map(dat.decode, t)))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pdb\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "\n",
    "class RNN:\n",
    "    \n",
    "    def __init__(self, hidden_size, sequence_length, vocab_size, learning_rate):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Xavier\n",
    "        self.U = np.random.normal(size=[vocab_size, hidden_size], scale=1.0 / np.sqrt(hidden_size))  # ... input projection\n",
    "        self.W = np.random.normal(size=[hidden_size, hidden_size], scale=1.0 / np.sqrt(hidden_size))  # ... hidden-to-hidden projection\n",
    "        self.b = np.zeros([1, hidden_size])\n",
    "        \n",
    "        \n",
    "        self.V = np.random.normal(size=[hidden_size, vocab_size], scale=1.0 / np.sqrt(vocab_size))  # ... output projection\n",
    "        self.c = np.zeros([1, vocab_size]) # ... output bias\n",
    "\n",
    "        \n",
    "        # memory of past gradients - rolling sum of squares for Adagrad\n",
    "        self.memory_U, self.memory_W, self.memory_V = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "        self.memory_b, self.memory_c = np.zeros_like(self.b), np.zeros_like(self.c)\n",
    "    \n",
    "    \n",
    "    def rnn_step_forward(self, x, h_prev,  U, W, b):\n",
    "        # A single time step forward of a recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity.\n",
    "\n",
    "        # x - input data (minibatch size x input dimension)\n",
    "        # h_prev - previous hidden state (minibatch size x hidden size)\n",
    "        # U - input projection matrix (input dimension x hidden size)\n",
    "        # W - hidden to hidden projection matrix (hidden size x hidden size)\n",
    "        # b - bias of shape (hidden size x 1).T\n",
    "        \n",
    "        h_current = np.tanh(np.dot(h_prev, W) + np.dot(x, U) + b)\n",
    "        cache = (W, x, h_prev, h_current)\n",
    "        return h_current, cache\n",
    "    \n",
    "    def rnn_forward(self, x, h0,  U, W, b):\n",
    "        # Full unroll forward of the recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity\n",
    "\n",
    "        # x - input data for the whole time-series (minibatch size x sequence_length x input dimension)\n",
    "        # h0 - initial hidden state (minibatch size x hidden size)\n",
    "        # U - input projection matrix (input dimension x hidden size)\n",
    "        # W - hidden to hidden projection matrix (hidden size x hidden size)\n",
    "        # b - bias of shape (hidden size x 1).T\n",
    "        \n",
    "        h, cache = [h0], []\n",
    "        for t in range(self.sequence_length):\n",
    "            data = x[:, t, :] #t-th entry\n",
    "            current_h, current_cache = self.rnn_step_forward(data, h[-1], U, W, b)\n",
    "            h.append(current_h)\n",
    "            cache.append(current_cache)\n",
    "\n",
    "\n",
    "        # return the hidden states for the whole time series (T+1) and a tuple of values needed for the backward step\n",
    "        h = np.array(h[1:]).transpose((1, 0, 2)) # skip initial state\n",
    "        return h, cache\n",
    "\n",
    "    \n",
    "    \n",
    "    def rnn_step_backward(self, grad_next, cache):\n",
    "        # A single time step backward of a recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity.\n",
    "\n",
    "        # grad_next - upstream gradient of the loss with respect to the next hidden state and current output\n",
    "        # cache - cached information from the forward pass\n",
    "        \n",
    "        W, x, h_prev, h_curr = cache\n",
    "        dz = grad_next * (1 - h_curr**2)\n",
    "        \n",
    "        dh_prev = np.dot(dz, W.T)\n",
    "        dU = np.dot(x.T, dz)\n",
    "        dW = np.dot(h_prev.T, dz)\n",
    "        db = np.sum(dz, axis=0)\n",
    "        \n",
    "        return dh_prev, dU, dW, db\n",
    "\n",
    "\n",
    "    def rnn_backward(self, dh, cache):\n",
    "        # Full unroll forward of the recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity\n",
    "        dU, dW, db = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.b)\n",
    "        \n",
    "\n",
    "        # compute and return gradients with respect to each parameter\n",
    "        # for the whole time series.\n",
    "        upstream_grad = np.zeros_like(dh[0])\n",
    "        for dh_t, cache_t in reversed(list(zip(dh, cache))):\n",
    "            upstream_grad, dU_t, dW_t, db_t = self.rnn_step_backward(dh_t + upstream_grad, cache_t)\n",
    "            dU += dU_t\n",
    "            dW += dW_t\n",
    "            db += db_t; \n",
    "\n",
    "        clip = lambda x: np.clip(x, -5, 5)\n",
    "        return clip(dU), clip(dW), clip(db)\n",
    "    \n",
    "    \n",
    "    def output(self, h, V, c):\n",
    "        # Calculate the output probabilities of the network\n",
    "        return np.dot(h, V) + c\n",
    "    \n",
    "    def softmax(self, o):\n",
    "        exp = np.exp(o)\n",
    "        s = exp / np.sum(exp, axis=1, keepdims=True)\n",
    "        return s\n",
    "    \n",
    "    def output_loss_and_grads(self, h, V, c, y):\n",
    "        # Calculate the loss of the network for each of the outputs\n",
    "\n",
    "        # h - hidden states of the network for each timestep. \n",
    "        #     the dimensionality of h is (batch size x sequence length x hidden size (the initial state is irrelevant for the output)\n",
    "        # V - the output projection matrix of dimension hidden size x vocabulary size\n",
    "        # c - the output bias of dimension vocabulary size x 1\n",
    "        # y - the true class distribution - a one-hot vector of dimension \n",
    "        #     vocabulary size x 1 - you need to do this conversion prior to\n",
    "        #     passing the argument. A fast way to create a one-hot vector from\n",
    "        #     an id could be something like the following code:\n",
    "\n",
    "        #   y[timestep] = np.zeros((vocabulary_size, 1))\n",
    "        #   y[timestep][batch_y[timestep]] = 1\n",
    "\n",
    "        #     where y might be a dictionary.\n",
    "\n",
    "        loss, dh, dV, dc = 0.0, [], np.zeros_like(self.V), np.zeros_like(self.c)\n",
    "        batch_size = len(h)\n",
    "        \n",
    "        for t in range(self.sequence_length):\n",
    "            yp = y[:, t, :]\n",
    "            h_t = h[:, t, :]\n",
    "            \n",
    "            o = self.output(h_t, V, c)\n",
    "            s = self.softmax(o)\n",
    "            \n",
    "            loss += -np.sum(np.log(s)*yp) / batch_size\n",
    "            dO = (s - yp) / batch_size\n",
    "            \n",
    "            dV += np.dot(h_t.T, dO)\n",
    "            dc += np.sum(dO, axis=0)\n",
    "            \n",
    "            dh_t = np.dot(dO, V.T)\n",
    "            dh.append(dh_t)\n",
    "            \n",
    "        return loss, dh, dV, dc\n",
    "    \n",
    "    \n",
    "    \n",
    "    def update(self, batch_size, dU, dW, db, dV, dc):\n",
    "        eps = 1e-7\n",
    "        \n",
    "        # update memory matrices\n",
    "        # perform the Adagrad update of parameters\n",
    "        \n",
    "        self.memory_U += np.square(dU)\n",
    "        self.memory_W += np.square(dW)\n",
    "        self.memory_b += np.square(db)\n",
    "        self.memory_V += np.square(dV)\n",
    "        self.memory_c += np.square(dc)\n",
    "        \n",
    "        update_param = lambda dx, mem_x: self.learning_rate * dx / np.sqrt(mem_x + eps)\n",
    "        \n",
    "        self.U -= update_param(dU, self.memory_U)\n",
    "        self.W -= update_param(dW, self.memory_W)\n",
    "        self.b -= update_param(db, self.memory_b)\n",
    "        self.V -= update_param(dV, self.memory_V)\n",
    "        self.c -= update_param(dc, self.memory_c)\n",
    "        \n",
    "    def step(self, h, x, y):\n",
    "        h, cache = self.rnn_forward(x, h, self.U, self.W, self.b)\n",
    "        loss, dh, dV, dc = self.output_loss_and_grads(h, self.V, self.c, y)\n",
    "        dU, dW, db = self.rnn_backward(dh, cache)\n",
    "        self.update(len(x), dU, dW, db, dV, dc)\n",
    "        return loss, h[:, -1, :]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pdb\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "from dataset import Dataset\n",
    "#from rnn import RNN\n",
    "\n",
    "\n",
    "def sample(rnn, seed, n_sample, dataset):\n",
    "    h0 = np.zeros([1, rnn.hidden_size])\n",
    "    seed_oh = dataset.one_hot(dataset.encode(seed))\n",
    "    \n",
    "    sampled = []\n",
    "    \n",
    "    for c_oh in seed_oh:\n",
    "        h0, _ = rnn.rnn_step_forward(c_oh.reshape([1, -1]), h0, rnn.U, rnn.W, rnn.b)\n",
    "        sampled.append(np.argmax(c_oh))\n",
    "        \n",
    "    for i in range(len(seed), n_sample):\n",
    "        prev_out = np.array([sampled[-1]])\n",
    "        in_oh = dataset.one_hot(prev_out)\n",
    "        h0, _ = rnn.rnn_step_forward(in_oh, h0, rnn.U, rnn.W, rnn.b)\n",
    "        \n",
    "        out = rnn.output(h0, rnn.V, rnn.c)\n",
    "        sampled.append(np.argmax(out))\n",
    "  \n",
    "    return dataset.decode(sampled)\n",
    "\n",
    "\n",
    "def run_language_model(dataset, max_epochs, hidden_size=100, sequence_length=30, learning_rate=1e-1, sample_every=100):\n",
    "    \n",
    "    vocab_size = len(dataset.sorted_chars)\n",
    "    rnn = RNN(hidden_size, sequence_length, vocab_size, learning_rate)\n",
    "\n",
    "    current_epoch = 0 \n",
    "    batch = 0\n",
    "\n",
    "    h0 = np.zeros((dataset.batch_size, hidden_size))\n",
    "    cum_loss = 0\n",
    "\n",
    "    while current_epoch < max_epochs: \n",
    "        e, x, y = dataset.next_minibatch()\n",
    "        \n",
    "        if e: \n",
    "            current_epoch += 1\n",
    "            h0 = np.zeros((dataset.batch_size, hidden_size))\n",
    "\n",
    "        # One-hot transform the x and y batches\n",
    "        x_oh, y_oh = dataset.one_hot(x), dataset.one_hot(y)\n",
    "\n",
    "\n",
    "        loss, h0 = rnn.step(h0, x_oh, y_oh)\n",
    "        cum_loss += loss\n",
    "        \n",
    "        if batch % sample_every == 0: \n",
    "            seed = \"HAN:\\nIs that good or bad?\\n\\n\"\n",
    "            n_sample = 300\n",
    "            sampled = sample(rnn, seed, n_sample, dataset)\n",
    "            print(''.join(sampled))\n",
    "            print()\n",
    "            \n",
    "           \n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            current_batch = batch % dataset.num_batches\n",
    "            print(\"epoch: %06d:\\tbatch: %4d/%d\\t\" % (current_epoch, current_batch, dataset.num_batches), end=\"\")\n",
    "            print(\"Average_loss: %.4f\" % (cum_loss/batch))\n",
    "            \n",
    "        batch += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "dataset = Dataset(batch_size=5, sequence_length=30)\n",
    "dataset.preprocess(\"dataset/selected_conversations.txt\")\n",
    "dataset.create_minibatches()\n",
    "run_language_model(dataset, 100000, sequence_length=dataset.sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" not.  It's Fredereck Fronkons\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(dataset.decode(dataset.next_minibatch()[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
