{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys_version: 3.5.2 (default, Nov 17 2016, 17:05:23) [GCC 5.4.0 20160609]\n",
      "virtual_env None\n",
      "pwd /home/marko/Projects/faks/DU/DU3\n",
      "np  1.11.1\n",
      "tf  0.10.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pdb\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print('sys_version:', sys.version.replace('\\n', ''))\n",
    "print('virtual_env', os.environ.get('VIRTUAL_ENV', 'None'))\n",
    "print('pwd', os.getcwd())\n",
    "print('np ', np.__version__)\n",
    "print('tf ', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    \n",
    "    def __init__(self, batch_size, sequence_length):\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.batch_index = 0\n",
    "    \n",
    "    def preprocess(self, input_file):\n",
    "        with open(input_file, \"r\") as f:\n",
    "            data = f.read()\n",
    "\n",
    "        # count and sort most frequent characters\n",
    "        chars, cnts = np.unique(list(data), return_index=True)\n",
    "        self.sorted_chars = chars[np.argsort(-cnts)]\n",
    "        \n",
    "        # other way\n",
    "        #cntr = Counter(data)\n",
    "        #self.sorted_chars = sorted(cntr.keys(), key=cntr.get, reverse=True)\n",
    "\n",
    "        # self.sorted chars contains just the characters ordered descending by frequency\n",
    "        self.char2id = dict(zip(self.sorted_chars, range(len(self.sorted_chars)))) \n",
    "        self.id2char = {k:v for v,k in self.char2id.items()}\n",
    "        self.x = np.array(list(map(self.char2id.get, data)))\n",
    "\n",
    "    def encode(self, sequence):\n",
    "        return [self.char2id[c] for c in sequence]\n",
    "\n",
    "    def decode(self, encoded_sequence):\n",
    "        return [self.id2char[c] for c in encoded_sequence]\n",
    "        \n",
    "    def create_minibatches(self):\n",
    "        data_len = len(self.x)\n",
    "        chars_per_batch = self.batch_size * self.sequence_length\n",
    "        self.num_batches = int((data_len-1) / chars_per_batch) \n",
    "        \n",
    "        samples = []\n",
    "        for i in range(self.num_batches * self.batch_size):\n",
    "            s = i * self.sequence_length\n",
    "            e = s + self.sequence_length + 1 \n",
    "            samples += [self.x[s:e]]\n",
    " \n",
    "        self.batches = np.zeros([self.num_batches, self.batch_size, self.sequence_length + 1], dtype=np.int32)      \n",
    "        sample_index, batch_index = zip(*product(np.arange(self.batch_size), np.arange(self.num_batches)))  \n",
    "        for b, s, sample in zip(batch_index, sample_index, samples):\n",
    "            self.batches[b, s, :] = sample\n",
    "                        \n",
    "        self.batch_index = 0\n",
    "\n",
    "    def next_minibatch(self):\n",
    "        new_epoch = self.batch_index == self.num_batches\n",
    "        if new_epoch:\n",
    "            self.batch_index = 0\n",
    "\n",
    "        batch = self.batches[self.batch_index, :, :]\n",
    "        self.batch_index += 1\n",
    "        \n",
    "        batch_x = batch[:, :-1]\n",
    "        batch_y = batch[:, 1:]\n",
    "        return new_epoch, batch_x, batch_y\n",
    "    \n",
    "    def _as_one_hot(self, x, vocab):\n",
    "        n = len(x)\n",
    "        Yoh = np.zeros((n, vocab))\n",
    "        Yoh[np.arange(n), x] = 1\n",
    "        return Yoh\n",
    "    \n",
    "\n",
    "    def one_hot(self, batch, vocab):\n",
    "        if batch.ndim == 1:\n",
    "            return self._as_one_hot(batch, vocab)\n",
    "        else:\n",
    "            return np.array([self._as_one_hot(s, vocab) for s in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0\n",
      "False\n",
      "X [['a', 'b', 'c'], ['g', 'h', 'i'], ['m', 'n', 'o']]\n",
      "Y [['b', 'c', 'd'], ['h', 'i', 'j'], ['n', 'o', 'p']]\n",
      "\n",
      "\n",
      "\n",
      "Batch: 1\n",
      "False\n",
      "X [['d', 'e', 'f'], ['j', 'k', 'l'], ['p', 'q', 'r']]\n",
      "Y [['e', 'f', 'g'], ['k', 'l', 'm'], ['q', 'r', 's']]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test 1\n",
    "dat = Dataset(3, 3)\n",
    "dat.preprocess(\"test.txt\")\n",
    "txt = \"hjdhasjdhjasdhja\"\n",
    "out = dat.encode(txt)\n",
    "assert txt != dat.decode(out)\n",
    "\n",
    "\n",
    "dat.create_minibatches()\n",
    "for i in range(dat.num_batches):\n",
    "    print(\"Batch:\", i)\n",
    "    f, s, t = dat.next_minibatch()\n",
    "    print(f)\n",
    "    print(\"X\", list(map(dat.decode, s)))\n",
    "    print(\"Y\", list(map(dat.decode, t)))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predlo탑eni po훾etni hiperparametri:\n",
    "\n",
    "# veli훾ina skrivenog sloja 100\n",
    "# duljina vremenskog odmatanja 30 \n",
    "# stopa u훾enja 1e-1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \n",
    "    def __init__(self, hidden_size, sequence_length, vocab_size, learning_rate):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Xavier\n",
    "        self.U = np.random.normal(size=[vocab_size, hidden_size], scale=1.0 / np.sqrt(hidden_size))  # ... input projection\n",
    "        self.W = np.random.normal(size=[hidden_size, hidden_size], scale=1.0 / np.sqrt(hidden_size))  # ... hidden-to-hidden projection\n",
    "        self.b = np.zeros([1, hidden_size])\n",
    "        \n",
    "        \n",
    "        self.V = np.random.normal(size=[hidden_size, vocab_size], scale=1.0 / np.sqrt(vocab_size))  # ... output projection\n",
    "        self.c = np.zeros([1, vocab_size]) # ... output bias\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        # memory of past gradients - rolling sum of squares for Adagrad\n",
    "        self.memory_U, self.memory_W, self.memory_V = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "        self.memory_b, self.memory_c = np.zeros_like(self.b), np.zeros_like(self.c)\n",
    "    \n",
    "    \n",
    "    def rnn_step_forward(self, x, h_prev, U, W, b):\n",
    "        # A single time step forward of a recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity.\n",
    "\n",
    "        # x - input data (minibatch size x input dimension)\n",
    "        # h_prev - previous hidden state (minibatch size x hidden size)\n",
    "        # U - input projection matrix (input dimension x hidden size)\n",
    "        # W - hidden to hidden projection matrix (hidden size x hidden size)\n",
    "        # b - bias of shape (hidden size x 1).T\n",
    "        \n",
    "        h_current = np.tanh(np.dot(h_prev, W) + np.dot(x, U) + b)\n",
    "        cache = (W, x, h_prev, h_current)\n",
    "        return h_current, cache\n",
    "    \n",
    "    def rnn_forward(self, x, h0, U, W, b):\n",
    "        # Full unroll forward of the recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity\n",
    "\n",
    "        # x - input data for the whole time-series (minibatch size x sequence_length x input dimension)\n",
    "        # h0 - initial hidden state (minibatch size x hidden size)\n",
    "        # U - input projection matrix (input dimension x hidden size)\n",
    "        # W - hidden to hidden projection matrix (hidden size x hidden size)\n",
    "        # b - bias of shape (hidden size x 1).T\n",
    "        \n",
    "        h, cache = [h0], []\n",
    "        for t in range(self.sequence_length):\n",
    "            data = x[:, t, :] #t-th entry\n",
    "            current_h, current_cache = self.rnn_step_forward(data, h[-1], U, W, b)\n",
    "            h.append(current_h)\n",
    "            cache.append(current_cache)\n",
    "\n",
    "\n",
    "        # return the hidden states for the whole time series (T+1) and a tuple of values needed for the backward step\n",
    "        return h, cache\n",
    "\n",
    "    \n",
    "    \n",
    "    def rnn_step_backward(self, grad_next, cache):\n",
    "        # A single time step backward of a recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity.\n",
    "\n",
    "        # grad_next - upstream gradient of the loss with respect to the next hidden state and current output\n",
    "        # cache - cached information from the forward pass\n",
    "        \n",
    "        W, x, h_prev, h_curr = cache\n",
    "        dz = grad_next * (1 - h_curr**2)\n",
    "        \n",
    "        dh_prev = np.dot(dz, W.T)\n",
    "        dU = np.dot(x.T, dz)\n",
    "        dW = np.dot(h_prev.T, dz)\n",
    "        db = np.sum(dz, axis=0)\n",
    "        \n",
    "        return dh_prev, dU, dW, db\n",
    "\n",
    "\n",
    "    def rnn_backward(self, dh, cache):\n",
    "        # Full unroll forward of the recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity\n",
    "        dU, dW, db = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.b)\n",
    "        \n",
    "\n",
    "        # compute and return gradients with respect to each parameter\n",
    "        # for the whole time series.\n",
    "        upstream_grad = np.zeros_like(dh[0])\n",
    "        for dh_t, cache_t in reversed(zip(dh, cache)):\n",
    "            upstream_grad, dU_t, dW_t, db_t = self.rnn_step_backward(dh_t + upstream_grad, cache_t)\n",
    "            dU += dU_t; dW += dW_t; db += db_t; \n",
    "        \n",
    "        clip = lambda x: np.clip(x, -5, 5)\n",
    "        return clip(dU), clip(dW), clip(db)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def output(h, V, c):\n",
    "        # Calculate the output probabilities of the network\n",
    "        return np.dot(h, V) + c\n",
    "    \n",
    "    def softmax(self, o):\n",
    "        exp = np.exp(o)\n",
    "        s = exp / np.sum(exp, axis=1, keepdims=True)\n",
    "        return s\n",
    "    \n",
    "    def output_loss_and_grads(self, h, V, c, y):\n",
    "        # Calculate the loss of the network for each of the outputs\n",
    "\n",
    "        # h - hidden states of the network for each timestep. \n",
    "        #     the dimensionality of h is (batch size x sequence length x hidden size (the initial state is irrelevant for the output)\n",
    "        # V - the output projection matrix of dimension hidden size x vocabulary size\n",
    "        # c - the output bias of dimension vocabulary size x 1\n",
    "        # y - the true class distribution - a one-hot vector of dimension \n",
    "        #     vocabulary size x 1 - you need to do this conversion prior to\n",
    "        #     passing the argument. A fast way to create a one-hot vector from\n",
    "        #     an id could be something like the following code:\n",
    "\n",
    "        #   y[timestep] = np.zeros((vocabulary_size, 1))\n",
    "        #   y[timestep][batch_y[timestep]] = 1\n",
    "\n",
    "        #     where y might be a dictionary.\n",
    "\n",
    "        loss, dh, dV, dc = 0.0, [], np.zeros_like(self.V), np.zeros_like(self.c)\n",
    "        for t in range(self.sequence_length):\n",
    "            yp = y[:, t, :]\n",
    "            h_t = h[:, t, :]\n",
    "            \n",
    "            o = self.output(h_t, V, c)\n",
    "            s = self.softmax(o)\n",
    "            \n",
    "            dO = s - yp\n",
    "            \n",
    "            dV += np.dot(h_t.T, dO)\n",
    "            dc += np.sum(dO, axis=0)\n",
    "            \n",
    "            dh_t = np.dot(dO, V.T)\n",
    "            dh.append(dh_t)\n",
    "            loss += -np.sum(np.log(s)*yp)\n",
    "            \n",
    "        loss /= self.batch_size\n",
    "        return loss, dh, dV, dc\n",
    "    \n",
    "    \n",
    "    \n",
    "    def update(self, dU, dW, db, dV, dc):\n",
    "        eps = 1e-7\n",
    "        \n",
    "        # update memory matrices\n",
    "        # perform the Adagrad update of parameters\n",
    "        mean = lambda values: [v/self.batch_size for v in values]\n",
    "        dU, dW, db, dV, dc = mean([dU, dW, db, dV, dc])\n",
    "        \n",
    "        self.memory_U += np.square(dU)\n",
    "        self.memory_W += np.square(dW)\n",
    "        self.memory_b += np.square(db)\n",
    "        self.memory_V += np.square(dV)\n",
    "        self.memory_c += np.square(dc)\n",
    "        \n",
    "        update_param = lambda dx, mem_x: self.learning_rate * dx / np.sqrt(mem_x + eps)\n",
    "        \n",
    "        self.U -= update_param(dU, self.memory_U)\n",
    "        self.W -= update_param(dW, self.memory_W)\n",
    "        self.b -= update_param(db, self.memory_b)\n",
    "        self.V -= update_param(dV, self.memory_V)\n",
    "        self.c -= update_param(dc, self.memory_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_language_model(dataset, max_epochs, hidden_size=100, sequence_length=30, learning_rate=1e-1, sample_every=100):\n",
    "    \n",
    "    vocab_size = len(dataset.sorted_chars)\n",
    "    RNN = None # initialize the recurrent network\n",
    "\n",
    "    current_epoch = 0 \n",
    "    batch = 0\n",
    "\n",
    "    h0 = np.zeros((hidden_size, 1))\n",
    "\n",
    "    average_loss = 0\n",
    "\n",
    "    while current_epoch < max_epochs: \n",
    "        e, x, y = dataset.next_minibatch()\n",
    "        \n",
    "        if e: \n",
    "            current_epoch += 1\n",
    "            h0 = np.zeros((hidden_size, 1))\n",
    "            # why do we reset the hidden state here?\n",
    "\n",
    "        # One-hot transform the x and y batches\n",
    "        x_oh, y_oh = None, None\n",
    "\n",
    "        # Run the recurrent network on the current batch\n",
    "        # Since we are using windows of a short length of characters,\n",
    "        # the step function should return the hidden state at the end\n",
    "        # of the unroll. You should then use that hidden state as the\n",
    "        # input for the next minibatch. In this way, we artificially\n",
    "        # preserve context between batches.\n",
    "        loss, h0 = RNN.step(h0, x_oh, y_oh)\n",
    "\n",
    "        if batch % sample_every == 0: \n",
    "            # run sampling (2.2)\n",
    "            pass\n",
    "        batch += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = list('dhjahdjashdjahjdhasjdhajdhjasdhjahdjadhjadhjaf')\n",
    "chars, cnts = np.unique(data, return_counts=True)\n",
    "c1 = chars[np.argsort(-cnts)]\n",
    "cntr = Counter(data)\n",
    "c2 = sorted(cntr.keys(), key=cntr.get, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False,  True,  True], dtype=bool)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1 == c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'a': 10, 'd': 10, 'f': 1, 'h': 11, 'j': 11, 's': 3})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cntr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a' 'd' 'f' 'h' 'j' 's']\n",
      "[10 10  1 11 11  3]\n"
     ]
    }
   ],
   "source": [
    "print(chars)\n",
    "print(cnts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a', 'b', 'a', 'b', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b'], \n",
       "      dtype='<U1')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list('ababbabababab'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a', 'b', 'c', 'd'], \n",
       "      dtype='<U1')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'c', 'd', 'b']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 5]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list([1,2,3,4,5,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.array([0,1,2,3,4,5,6,7,8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = np.array([1,2,3])\n",
    "e = np.array([4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "failed to coerce slice entry of type numpy.ndarray to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-dcb73b954d30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: failed to coerce slice entry of type numpy.ndarray to integer"
     ]
    }
   ],
   "source": [
    "a[s:e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
