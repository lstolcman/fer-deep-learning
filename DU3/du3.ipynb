{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys_version: 3.5.2 (default, Nov 17 2016, 17:05:23) [GCC 5.4.0 20160609]\n",
      "virtual_env None\n",
      "pwd /home/marko/Projects/faks/DU/DU3\n",
      "np  1.11.1\n",
      "tf  0.10.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pdb\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print('sys_version:', sys.version.replace('\\n', ''))\n",
    "print('virtual_env', os.environ.get('VIRTUAL_ENV', 'None'))\n",
    "print('pwd', os.getcwd())\n",
    "print('np ', np.__version__)\n",
    "print('tf ', tf.__version__)\n",
    "\n",
    "\n",
    "from IPython.core.debugger import Tracer\n",
    "BREAK_POINT = lambda: Tracer()()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0\n",
      "False\n",
      "X [['a', 'b', 'c'], ['g', 'h', 'i'], ['m', 'n', 'o']]\n",
      "Y [['b', 'c', 'd'], ['h', 'i', 'j'], ['n', 'o', 'p']]\n",
      "\n",
      "\n",
      "\n",
      "Batch: 1\n",
      "False\n",
      "X [['d', 'e', 'f'], ['j', 'k', 'l'], ['p', 'q', 'r']]\n",
      "Y [['e', 'f', 'g'], ['k', 'l', 'm'], ['q', 'r', 's']]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test 1\n",
    "dat = Dataset(3, 3)\n",
    "dat.preprocess(\"test.txt\")\n",
    "txt = \"hjdhasjdhjasdhja\"\n",
    "assert txt != dat.decode(dat.encode(txt))\n",
    "\n",
    "\n",
    "dat.create_minibatches()\n",
    "for i in range(dat.num_batches):\n",
    "    print(\"Batch:\", i)\n",
    "    f, s, t = dat.next_minibatch()\n",
    "    print(f)\n",
    "    print(\"X\", list(map(dat.decode, s)))\n",
    "    print(\"Y\", list(map(dat.decode, t)))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pdb\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "\n",
    "class RNN:\n",
    "    \n",
    "    def __init__(self, hidden_size, sequence_length, vocab_size, learning_rate):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Xavier\n",
    "        self.U = np.random.normal(size=[vocab_size, hidden_size], scale=1.0 / np.sqrt(hidden_size))  # ... input projection\n",
    "        self.W = np.random.normal(size=[hidden_size, hidden_size], scale=1.0 / np.sqrt(hidden_size))  # ... hidden-to-hidden projection\n",
    "        self.b = np.zeros([1, hidden_size])\n",
    "        \n",
    "        \n",
    "        self.V = np.random.normal(size=[hidden_size, vocab_size], scale=1.0 / np.sqrt(vocab_size))  # ... output projection\n",
    "        self.c = np.zeros([1, vocab_size]) # ... output bias\n",
    "\n",
    "        \n",
    "        # memory of past gradients - rolling sum of squares for Adagrad\n",
    "        self.memory_U, self.memory_W, self.memory_V = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "        self.memory_b, self.memory_c = np.zeros_like(self.b), np.zeros_like(self.c)\n",
    "    \n",
    "    \n",
    "    def rnn_step_forward(self, x, h_prev,  U, W, b):\n",
    "        # A single time step forward of a recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity.\n",
    "\n",
    "        # x - input data (minibatch size x input dimension)\n",
    "        # h_prev - previous hidden state (minibatch size x hidden size)\n",
    "        # U - input projection matrix (input dimension x hidden size)\n",
    "        # W - hidden to hidden projection matrix (hidden size x hidden size)\n",
    "        # b - bias of shape (hidden size x 1).T\n",
    "        \n",
    "        h_current = np.tanh(np.dot(h_prev, W) + np.dot(x, U) + b)\n",
    "        cache = (W, x, h_prev, h_current)\n",
    "        return h_current, cache\n",
    "    \n",
    "    def rnn_forward(self, x, h0,  U, W, b):\n",
    "        # Full unroll forward of the recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity\n",
    "\n",
    "        # x - input data for the whole time-series (minibatch size x sequence_length x input dimension)\n",
    "        # h0 - initial hidden state (minibatch size x hidden size)\n",
    "        # U - input projection matrix (input dimension x hidden size)\n",
    "        # W - hidden to hidden projection matrix (hidden size x hidden size)\n",
    "        # b - bias of shape (hidden size x 1).T\n",
    "        \n",
    "        h, cache = [h0], []\n",
    "        for t in range(self.sequence_length):\n",
    "            data = x[:, t, :] #t-th entry\n",
    "            current_h, current_cache = self.rnn_step_forward(data, h[-1], U, W, b)\n",
    "            h.append(current_h)\n",
    "            cache.append(current_cache)\n",
    "\n",
    "\n",
    "        # return the hidden states for the whole time series (T+1) and a tuple of values needed for the backward step\n",
    "        h = np.array(h[1:]).transpose((1, 0, 2)) # skip initial state\n",
    "        return h, cache\n",
    "\n",
    "    \n",
    "    \n",
    "    def rnn_step_backward(self, grad_next, cache):\n",
    "        # A single time step backward of a recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity.\n",
    "\n",
    "        # grad_next - upstream gradient of the loss with respect to the next hidden state and current output\n",
    "        # cache - cached information from the forward pass\n",
    "        \n",
    "        W, x, h_prev, h_curr = cache\n",
    "        dz = grad_next * (1 - h_curr**2)\n",
    "        \n",
    "        dh_prev = np.dot(dz, W.T)\n",
    "        dU = np.dot(x.T, dz)\n",
    "        dW = np.dot(h_prev.T, dz)\n",
    "        db = np.sum(dz, axis=0)\n",
    "        \n",
    "        return dh_prev, dU, dW, db\n",
    "\n",
    "\n",
    "    def rnn_backward(self, dh, cache):\n",
    "        # Full unroll forward of the recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity\n",
    "        dU, dW, db = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.b)\n",
    "        \n",
    "\n",
    "        # compute and return gradients with respect to each parameter\n",
    "        # for the whole time series.\n",
    "        upstream_grad = np.zeros_like(dh[0])\n",
    "        for dh_t, cache_t in reversed(list(zip(dh, cache))):\n",
    "            upstream_grad, dU_t, dW_t, db_t = self.rnn_step_backward(dh_t + upstream_grad, cache_t)\n",
    "            dU += dU_t\n",
    "            dW += dW_t\n",
    "            db += db_t; \n",
    "\n",
    "        clip = lambda x: np.clip(x, -5, 5)\n",
    "        return clip(dU), clip(dW), clip(db)\n",
    "    \n",
    "    \n",
    "    def output(self, h, V, c):\n",
    "        # Calculate the output probabilities of the network\n",
    "        return np.dot(h, V) + c\n",
    "    \n",
    "    def output_probas(self, h, V, c):\n",
    "        return self.softmax(self.output(h, V, c))\n",
    "    \n",
    "    def softmax(self, o):\n",
    "        exp = np.exp(o)\n",
    "        s = exp / np.sum(exp, axis=1, keepdims=True)\n",
    "        return s\n",
    "    \n",
    "    def output_loss_and_grads(self, h, V, c, y):\n",
    "        # Calculate the loss of the network for each of the outputs\n",
    "\n",
    "        # h - hidden states of the network for each timestep. \n",
    "        #     the dimensionality of h is (batch size x sequence length x hidden size (the initial state is irrelevant for the output)\n",
    "        # V - the output projection matrix of dimension hidden size x vocabulary size\n",
    "        # c - the output bias of dimension vocabulary size x 1\n",
    "        # y - the true class distribution - a one-hot vector of dimension \n",
    "        #     vocabulary size x 1 - you need to do this conversion prior to\n",
    "        #     passing the argument. A fast way to create a one-hot vector from\n",
    "        #     an id could be something like the following code:\n",
    "\n",
    "        #   y[timestep] = np.zeros((vocabulary_size, 1))\n",
    "        #   y[timestep][batch_y[timestep]] = 1\n",
    "\n",
    "        #     where y might be a dictionary.\n",
    "\n",
    "        loss, dh, dV, dc = 0.0, [], np.zeros_like(self.V), np.zeros_like(self.c)\n",
    "        batch_size = len(h)\n",
    "        \n",
    "        for t in range(self.sequence_length):\n",
    "            yp = y[:, t, :]\n",
    "            h_t = h[:, t, :]\n",
    "            \n",
    "            o = self.output(h_t, V, c)\n",
    "            s = self.softmax(o)\n",
    "            \n",
    "            loss += -np.sum(np.log(s)*yp) / batch_size\n",
    "            dO = (s - yp) / batch_size\n",
    "            \n",
    "            dV += np.dot(h_t.T, dO)\n",
    "            dc += np.sum(dO, axis=0)\n",
    "            \n",
    "            dh_t = np.dot(dO, V.T)\n",
    "            dh.append(dh_t)\n",
    "            \n",
    "        return loss, dh, dV, dc\n",
    "    \n",
    "    \n",
    "    \n",
    "    def update(self, batch_size, dU, dW, db, dV, dc):\n",
    "        eps = 1e-7\n",
    "        \n",
    "        # perform the Adagrad update of parameters\n",
    "        params = [self.U, self.W, self.b, self.V, self.c]\n",
    "        ders = [dU, dW, db, dV, dc]\n",
    "        mems = [self.memory_U, self.memory_W, self.memory_b, self.memory_V, self.memory_c]\n",
    "        \n",
    "        for x, dx, mem_x in zip(params, ders, mems):\n",
    "            mem_x += np.square(dx)\n",
    "            x -= self.learning_rate * dx / np.sqrt(mem_x + eps)\n",
    "\n",
    "        \n",
    "    def step(self, h, x, y):\n",
    "        h, cache = self.rnn_forward(x, h, self.U, self.W, self.b)\n",
    "        loss, dh, dV, dc = self.output_loss_and_grads(h, self.V, self.c, y)\n",
    "        dU, dW, db = self.rnn_backward(dh, cache)\n",
    "        self.update(len(x), dU, dW, db, dV, dc)\n",
    "        return loss, h[:, -1, :]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pdb\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "from dataset import Dataset\n",
    "#from rnn import RNN\n",
    "\n",
    "\n",
    "def sample(rnn, seed, n_sample, dataset):\n",
    "    h0 = np.zeros([1, rnn.hidden_size])\n",
    "    seed_oh = dataset.one_hot(dataset.encode(seed))\n",
    "    \n",
    "    sampled = []\n",
    "    \n",
    "    for c_oh in seed_oh:\n",
    "        h0, _ = rnn.rnn_step_forward(c_oh.reshape([1, -1]), h0, rnn.U, rnn.W, rnn.b)\n",
    "        sampled.append(np.argmax(c_oh))\n",
    "        \n",
    "    for i in range(len(seed), n_sample):\n",
    "        prev_out = np.array([sampled[-1]])\n",
    "        in_oh = dataset.one_hot(prev_out)\n",
    "        h0, _ = rnn.rnn_step_forward(in_oh, h0, rnn.U, rnn.W, rnn.b)\n",
    "        \n",
    "        probas = rnn.output_probas(h0, rnn.V, rnn.c)\n",
    "        out_char_oh = np.random.choice(range(dataset.vocab_size), p=probas.ravel()) \n",
    "        sampled.append(out_char_oh)\n",
    "  \n",
    "    return dataset.decode(sampled)\n",
    "\n",
    "import pickle\n",
    "\n",
    "def run_language_model(dataset, max_epochs, hidden_size=100, sequence_length=30, learning_rate=1e-1, sample_every=1000, dump_path='./model'):\n",
    "    \n",
    "    vocab_size = len(dataset.sorted_chars)\n",
    "    rnn = RNN(hidden_size, sequence_length, vocab_size, learning_rate)\n",
    "\n",
    "    current_epoch = 0 \n",
    "    batch = 0\n",
    "\n",
    "    h0 = np.zeros((dataset.batch_size, hidden_size))\n",
    "    cum_loss = 0\n",
    "\n",
    "    while current_epoch < max_epochs: \n",
    "        e, x, y = dataset.next_minibatch()\n",
    "        \n",
    "        if e: \n",
    "            current_epoch += 1\n",
    "            h0 = np.zeros((dataset.batch_size, hidden_size))\n",
    "\n",
    "        # One-hot transform the x and y batches\n",
    "        x_oh, y_oh = dataset.one_hot(x), dataset.one_hot(y)\n",
    "\n",
    "\n",
    "        loss, h0 = rnn.step(h0, x_oh, y_oh)\n",
    "        cum_loss += loss\n",
    "        \n",
    "        if batch % sample_every == 0: \n",
    "            seed = \"HAN:\\nIs that good or bad?\\n\\n\"\n",
    "            n_sample = 300\n",
    "            sampled = sample(rnn, seed, n_sample, dataset)\n",
    "            print(''.join(sampled))\n",
    "            print()\n",
    "            with open(dump_path, \"wb\") as f:\n",
    "                pickle.dump(rnn, f)\n",
    "                print('> Dumped to:', dump_path)\n",
    "            \n",
    "           \n",
    "        \n",
    "        if batch % 1000 == 0:\n",
    "            current_batch = batch % dataset.num_batches\n",
    "            print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "            print(\"epoch: %06d:\\tbatch: %4d/%d\\t\" % (current_epoch, current_batch, dataset.num_batches), end=\"\")\n",
    "            print(\"Average_loss: %.4f; Last batch loss: %.4f\" % (cum_loss/batch, loss))\n",
    "            print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "            \n",
    "        batch += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "svv.\n",
      "mh9ANt:h0sst.oh,ks cgh ffe`h'h.ttthmHftdtohvftKohhaNt0whIvk t gIf:to,oAndhmohft,Ntts,et. monketGwsH1NRTs.juNhcmInd.gwsA1.tThh4tC0hTcio 2hgf4thT9Inn. hBssfN:hh9si0t0gssn.hch!uUY'o`fvtMchHHnnKwmIsLt?gwssueWhv6tnco gsnt8hw fttoghhHetogFsneN\n",
      "yont VdMIHOcwhh se.ohKtst.ghUw\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000000:\tbatch:    0/3947\tAverage_loss: inf; Last batch loss: 128.1473\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py:80: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "cYFTOIR.\n",
      "\n",
      "JEZMAFDR5FFSINKFFRECaAIs tasl\n",
      "\n",
      "OeANewte thanwaiIlPthams dhot'sklly Fit?\n",
      "\n",
      "JEEYET:\n",
      "Yok. TINan's at 'lin\n",
      "Mhat gnh g iwsy Jant lin yon?\n",
      "J wIton.on 6ing ton?\n",
      "\n",
      "JEFuS IA yone sonoy youPs o svint.o ton that doalald .onlik n6stokcank win I yon ghiilll nang to y at. he.\n",
      "\n",
      "D\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000000:\tbatch: 1000/3947\tAverage_loss: 89.4941; Last batch loss: 76.7048\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "EEON:\n",
      ".\n",
      "\n",
      "OMACKRIWhighat ctherbee lome, .\n",
      "\n",
      "JOACB:\n",
      "Deay.b3y t diyd!!\n",
      "\n",
      "DoANKALDl?.\n",
      "\n",
      "DECp'sddive ball aikdasher kibl\n",
      "\n",
      "DEN:\n",
      "You bescseml, bot got?\n",
      "\n",
      "DEH:\n",
      "Art avim. Cn'emes buy yaan,be.\n",
      "\n",
      "\n",
      "JEAND:\n",
      "Dor you wholahlers gore thasmy?\n",
      "\n",
      "FAED4AKD:\n",
      "You thaqd..y bustar mfalme wavo  ig blorip\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000000:\tbatch: 2000/3947\tAverage_loss: 79.2179; Last batch loss: 65.4104\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "EELTOOR:\n",
      "Fon't gek'm she Taue. we cow tell.\n",
      "\n",
      "SOORONR:\n",
      "\n",
      "fo kisthars grt't to hths?\n",
      "\n",
      "JECK:\n",
      "Tht ..  hat afk fas ane bo to thex.\n",
      "\n",
      "BOK:\n",
      "Lith, ul rve mer.  Bor gonk, Bre you gos Soull.\n",
      "\n",
      "\n",
      "TEUMON:\n",
      "Thon to kal ghe not luger tor, nimhst the heafe.\n",
      "\n",
      "LNSOOLBINe? fhthin's Tow lican'm o\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000000:\tbatch: 3000/3947\tAverage_loss: 74.5826; Last batch loss: 58.3299\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "ST:\n",
      "We yo\n",
      "DONgh fwhe you dowa I gocack en ame ceed hor you ler goin y de. I ard you's whit and you gos a hve se home tonc.\n",
      "\n",
      "TO N:\n",
      "I lesl... I lat than't dend. I west. OVEND:\n",
      "I st'r goe the gat you fike thatn stiel wascctry Gofked, man an itul ann wo on afoncy st wuo.... ra\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000001:\tbatch:   53/3947\tAverage_loss: 71.8142; Last batch loss: 69.5997\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "ELECK:\n",
      "I chid her am?\n",
      "\n",
      "CKARDY:\n",
      ". I Womy. Whit.\n",
      "\n",
      "SANDIbPASABMRE:\n",
      "He hat the I zere'cacars I've I alr ar untsh, paaads co fustakne?\n",
      "\n",
      "Dowhaogus, wartto. Yousen. Ton...\n",
      "\n",
      "MA. THARDY:\n",
      "Waming.\n",
      "\n",
      "MR. OOERSI SCE:\n",
      "I hawk a vacd gotrin..\n",
      "T fre hre tof gver orered welly I've.\n",
      "\n",
      "LII FuON\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000001:\tbatch: 1053/3947\tAverage_loss: 69.7830; Last batch loss: 60.0718\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "ELO:\n",
      " walva goke our whade the her Fe'cr but cors.\n",
      "\n",
      "DECKARD! I wistad, Gight nacky be seribers?\n",
      "\n",
      "CEGOMMACKA. I T I 'r.\n",
      "\n",
      "\n",
      "MANAY:\n",
      "Touple.!\n",
      "\n",
      "DEKE:\n",
      "Millilett tald onee this thonkel, ay dael.\n",
      "\n",
      "IOTFRSOCY:\n",
      "Wer your , se Ma leahine was a at bis por ulrseal you'alerlligone, Mas, yo\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000001:\tbatch: 2053/3947\tAverage_loss: 68.2394; Last batch loss: 60.4237\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "FOOAN:\n",
      "Well in must get.\n",
      "\n",
      "FAEDETE:\n",
      "You aber. He r Dor, ony to sals.\n",
      "\n",
      "DONN:\n",
      "Now The hal' the Seing ack in to ckin'. restit St... of.  You kust sime. So thang theoilen if minooing!.. bonoy sey lorsik!d. Hophill yas callerid.....re your shore tapte ve?\n",
      "\n",
      "BENN:\n",
      "You cho you Mish\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000001:\tbatch: 3053/3947\tAverage_loss: 67.1005; Last batch loss: 64.2720\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "EUNER:\n",
      "Walt osts cijus thate in werl yed doter! It'de fatanicring you wectar mr?\n",
      "\n",
      "BVERE:\n",
      "Vosk. What fon't me to to cagoce, se now pure ke tot onl fich oo on hram owkrooling ont youeal mevemets is wh ther it wisime sere?\n",
      "\n",
      "ERNIN:\n",
      "We jof tring wo huEN. Whouce donnstoys, letbm\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000002:\tbatch:  106/3947\tAverage_loss: 66.1572; Last batch loss: 62.3264\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "WERMAY:\n",
      "Oh nat gise wooline.  I mugd.\n",
      "\n",
      "GOMFFREY:\n",
      "What's havy and you de the thy.\n",
      "\n",
      "AVERICK:\n",
      "The?\n",
      "\n",
      "LONN:\n",
      "Arcem.\n",
      "\n",
      "CHIUK.:\n",
      "I toubly thinking ent you you'cO gWer yould.\n",
      "\n",
      "GETE:\n",
      "Thisa dight. Hevey, youn't upper andedern not's a ivits.  I!  he dink of the!p do wet's here overy, yo\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000002:\tbatch: 1106/3947\tAverage_loss: 65.1432; Last batch loss: 54.4365\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "AUMA:\n",
      "INfurth!\n",
      "\n",
      "MAFCKERE:\n",
      "Dorad a oind duekins goommys not ale popem, haply tial...... going.\n",
      "\n",
      "ELAY:\n",
      "Ok nollent a lris.\n",
      "\n",
      "MR.!\n",
      "\n",
      "JED:\n",
      "IAre'd!\n",
      "\n",
      "SARDY:\n",
      "My.\n",
      "\n",
      "MARD:\n",
      "I what pruppors cree?\n",
      "\n",
      "THARESMONDE I prel I.\n",
      "\n",
      "DTEISTHANDY:\n",
      "Wedlin?\n",
      "\n",
      "MOLD:\n",
      "Are amh are you do.\n",
      "\n",
      "ILANY:\n",
      "Araller conl\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000002:\tbatch: 2106/3947\tAverage_loss: 64.3536; Last batch loss: 58.8884\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DEESTY:\n",
      "She is saad tigl this hill thated and Sres.\n",
      "\n",
      "SEREEDEE:\n",
      "Sibly ip golas goootl. qule mu!t forer con't think sob.re to !\n",
      "\n",
      "MRYt I noy lathioms me is it.\n",
      "\n",
      "RUKERLEN:\n",
      "Heal, Callly gainen to I dightu.  I're.\n",
      "\n",
      "KORNY:\n",
      "Docke  upcrake . I'ngsiblup. This teaightue?\n",
      "\n",
      "BACHA:\n",
      "Hase\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000002:\tbatch: 3106/3947\tAverage_loss: 63.7348; Last batch loss: 59.4915\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "MEIZO:\n",
      "Vaik! Ael, elyen sann nome me hids be samen tell you knowel.\n",
      "\n",
      "SEKEN gho I wisn's Lind ever ssir is Jean dop sow ale what Che.\n",
      "\n",
      "JEFFREY:\n",
      "Lo ke me gditlasisn hialing was ellus me te you fance it verd kasiow. Me wan anding is ale ne mon'a arbar be. Wo can ores, Hom the\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000003:\tbatch:  159/3947\tAverage_loss: 63.1791; Last batch loss: 55.3454\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DAVEURENI MANDDUNE:\n",
      "Dton hell a the knen.\n",
      "\n",
      "GURMAN:\n",
      "What a litnt stark? Whos?\n",
      "\n",
      "Jrm there mo the the battagrs. We nond sone, pen's hive an, thit elaik you go do hamp as atle fack.\n",
      "WREFGILIE:\n",
      "You? Well sive?\n",
      "\n",
      "JOFFRIVAN:\n",
      "He Veeplyer to hom you.\n",
      "\n",
      "REDDUTE:\n",
      "If chen the just leflo\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000003:\tbatch: 1159/3947\tAverage_loss: 62.5430; Last batch loss: 50.9483\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DAGETD:\n",
      "Dowite you could the dow thee'VEE:\n",
      "Shexdive thinge freld thep if uathtred ow, have the badjeel tAry.\n",
      "\n",
      "DE INO:\n",
      "Wasm. But the want not futt's in the A doul ald?\n",
      "\n",
      "BASTY:\n",
      "How.\n",
      "\n",
      "TTURA:\n",
      "Dedalwing esuly bead anaise encer to but norins she, herg mey?\n",
      "\n",
      "JIMAR COKEANRY:\n",
      "You'r\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000003:\tbatch: 2159/3947\tAverage_loss: 62.0296; Last batch loss: 49.6413\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "GANDY:\n",
      "Regertid.\n",
      "\n",
      "DON:\n",
      "I hhe tall and we'll Pave My.\n",
      "\n",
      "SALAIA:\n",
      "Bak, Do. I.l the Fust, boc Fring him.\n",
      "\n",
      "SHSTH:\n",
      "We is the withing get bated of cout?\n",
      "\n",
      "SARAH:\n",
      "You geved I'm net forith?!, aboughelsmer.. youf I lemeds....\n",
      "\n",
      "KORNENT:\n",
      "Yes I mat mo and parb... Good it think your lory?\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000003:\tbatch: 3159/3947\tAverage_loss: 61.6097; Last batch loss: 46.7453\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DY:\n",
      "Dlelonions es up aplad's where, me the tho high. What's pand.\n",
      "\n",
      "BUFFFEETOR:\n",
      "Whes you here't wewy allyst...\n",
      "\n",
      "LLIEWTITS:\n",
      "He'd, worlorss, she put your is a fnoes, mo, what lete be id that doont pnotrdloce, wop blan'ssse futn't just tele and bett? I sace I'llsey go . I deph\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000004:\tbatch:  212/3947\tAverage_loss: 61.2358; Last batch loss: 47.8314\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "EELMAN:\n",
      "Sey wis dich a pich puck frur. non real.\n",
      "\n",
      "MANSTY:\n",
      "Five gapke.\n",
      "\n",
      "MR. NHTE:\n",
      "Rugunay go, wave!\n",
      "\n",
      "HENREN:\n",
      "You iny fan off fourtent a jove apded oo yow quod Frive ficke, whit, in mo bwan. Cor showeste to wx ay eresed is aniecty Kire mo your shought came ase just I wassitt\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000004:\tbatch: 1212/3947\tAverage_loss: 60.7886; Last batch loss: 57.3461\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "MESIT:\n",
      "What, abloter wercem lore wend cere here you latevt.\n",
      "SRoned so fucce.\n",
      "\n",
      "LAUR:\n",
      "The beneesth pofpabiline. So sid you I whemmin?\n",
      "\n",
      "BAROL:\n",
      " whem and!\n",
      "\n",
      "BEWI:\n",
      "You youf bun.\n",
      "\n",
      "DECKARD:\n",
      "The tigh. LObor.\n",
      "\n",
      "BETFUSAPOR:\n",
      "You just a miculd is, I mince you haut me to that excomestion\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000004:\tbatch: 2212/3947\tAverage_loss: 60.4076; Last batch loss: 54.2377\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "FREY:\n",
      "Lo you 'kire't?\n",
      "\n",
      "PRE!DA ACHIE:\n",
      "Thought?\n",
      "\n",
      "DOCH:\n",
      "The puckdaple I though.  I dayfond there sor you to you dase mugg inappigor, anduen div.ldomadint of a kiduackine... lot that Can the battabpodpl, mofifelitevive ortments tist the shates, ist mone.\n",
      "\n",
      "THAN:\n",
      "Fnealy.\n",
      "\n",
      "TERMY:\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000004:\tbatch: 3212/3947\tAverage_loss: 60.1074; Last batch loss: 54.5395\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DEIN:\n",
      "OU.\n",
      "\n",
      "GONZO:\n",
      "I wall be aparrleenccarw buck. The they you?\n",
      "\n",
      "MECIVIG:\n",
      "You're Was sut you for hals whick.\n",
      "SR gouded. Whound a renf getod, lle seed fastf.  I know? Yes, was'ad ufhich ad aid yim to know elkn about poks. ACk ass time lell and menaber sonuth the 'll whatire.\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000005:\tbatch:  265/3947\tAverage_loss: 59.8268; Last batch loss: 54.4440\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "ELAIA MANKON:\n",
      "You stods when?\n",
      "\n",
      "FREDDY:\n",
      "Cemerizelingoomey.\n",
      "\n",
      "RURMADEN:\n",
      "You dought onsing ouly?\n",
      "\n",
      "MR. IHABE, SKAMPINED:\n",
      "You aron'er good in rcor onettoon an the meart suer the go don't sid it. Thound a spemser toor the pickagnale?  I knab, Blonto saind.\n",
      "\n",
      "EDDY:\n",
      "He divinan's on \n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000005:\tbatch: 1265/3947\tAverage_loss: 59.4892; Last batch loss: 51.8811\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "MARLIENER:\n",
      "I ken neve of mave lething.irgee of minalby you wakefon it.\n",
      "\n",
      "BANTY:\n",
      "Thoug all you nease that us aceadly you blisn't.\n",
      "\n",
      "RETTY:\n",
      "The lighade.\n",
      "\n",
      "HAMON:\n",
      "Ben't riM.  be the have I 'll lot ust a sech makink not you, fided have but gaine cat you to make nool a licefa on i\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000005:\tbatch: 2265/3947\tAverage_loss: 59.2008; Last batch loss: 53.1279\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "ELVERbous it. I'll about has a qunfilittle Hiver and thine, nest.\n",
      "\n",
      "SAUSE:\n",
      "Rop chaspout roogenmzing in but hix tapdigher methench they.\n",
      "\n",
      "SERA:\n",
      "Store mucantcolle. Well know to on that daurce entenled it fount the's bir.\n",
      "\n",
      "SARAH:\n",
      "Fred, balking the ficked oo like reys it be?\n",
      "\n",
      "L\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000005:\tbatch: 3265/3947\tAverage_loss: 58.9696; Last batch loss: 54.7456\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "ITh CHRUH:\n",
      "I'm joonenth! You kid be Saze.  hat in  treped. Carears you sea7!  Light're preccly wast. THEGHATR:\n",
      "You canch ..\n",
      "\n",
      "DUMMCH:\n",
      "Grom.\n",
      "\n",
      "ALMER:\n",
      "Came on be merut fingsing in whas werl you ghe Can you. We're. I goip being the my that. Shat's on\n",
      "\n",
      "KARLY:\n",
      "Whick to wes the ..\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000006:\tbatch:  318/3947\tAverage_loss: 58.7356; Last batch loss: 50.6513\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "PENRISL FAA V Sto I thingpe?\n",
      "\n",
      "HERRIT:\n",
      "Buthiraned up obpoatina to sats satoce Grear.\n",
      "\n",
      "MRSFANLA:\n",
      "Fun.  our.\n",
      "\n",
      "MR. WHITO:\n",
      "Dud I sall shiflicoure indy that beuss of. wIths?\n",
      "\n",
      "CHARLIN:\n",
      "We'd no.\n",
      "\n",
      "STIIOSER I 'He puss I you toland ind in that hes to the sablam on ar. He's ant by nea\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000006:\tbatch: 1318/3947\tAverage_loss: 58.4711; Last batch loss: 40.4627\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DAs Neve the, you taid good that benatee. Gove lefure.\n",
      "\n",
      "1UFAND:\n",
      "It beare.\n",
      "\n",
      "LEESEN:\n",
      "Ither.\n",
      "\n",
      "DECKARD:\n",
      "I have is on freed untad expeying you the don that got fromy your you fony, ivessand up amblook I houd abant'raykald anyin rege prishern.\n",
      "\n",
      "EDDY:\n",
      "Busm you you you weal!\n",
      "\n",
      "IEL:\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000006:\tbatch: 2318/3947\tAverage_loss: 58.2379; Last batch loss: 43.9435\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "LEESTO:\n",
      "Dome.\n",
      "\n",
      "TERMER:\n",
      "What mether your don't relfe a strifind never word is a verce ass's makes in af you a? MAVE\n",
      "IVE:\n",
      "I flonghing if emfoppened?\n",
      "\n",
      "TORMLHY:\n",
      "Come here.  Tway to get fifuneghing aaps a Siding at's gotine werw them do Joser sormythappepers.  What's a wens pre\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000006:\tbatch: 3318/3947\tAverage_loss: 58.0513; Last batch loss: 55.8909\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "INTT:\n",
      "You bate cir was no. Foe uprestionly? That's coup oun. I drongs. But haven'll wasmis of hlartered the where with, mucptaldind if ing Sreysevoo. Roma right in hip?\n",
      "\n",
      "GONZO:\n",
      "My Glysts it's co pick tomes it they deady.\n",
      "\n",
      "GOCE:\n",
      "Alsiony shop. Loesno. inchiex the creck I me.\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000007:\tbatch:  371/3947\tAverage_loss: 57.8550; Last batch loss: 58.4933\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "ENIEN:\n",
      "I'm a yoh8uth.ung take they you lore migunoting to thit, ainling Dittiaar achater you's for you're they brron'd. You frog ue the go?\n",
      "\n",
      "JlESTY:\n",
      "Well. In our?\n",
      "\n",
      "MR. WHITE:\n",
      "Duter, bace ust she weal I drincen didn't going that pracreappan you'rg ak. Pranb, yiuh. Howlase l\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000007:\tbatch: 1371/3947\tAverage_loss: 57.6288; Last batch loss: 46.9914\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(batch_size=5, sequence_length=30)\n",
    "dataset.preprocess(\"dataset/selected_conversations.txt\")\n",
    "dataset.create_minibatches()\n",
    "rnn = run_language_model(dataset, 100000, sequence_length=dataset.sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded...\n",
      "HAN:\n",
      "Yes?\n",
      "\n",
      "DETE L ANINICK RIAN:\n",
      "Every, you was know.\n",
      "\n",
      "DUKE:\n",
      "Ag I'm  that hahpu that he sce, this boge gorned. Just fell they hupp home now ever very Luke you? We loke fing and are you get to heperis. But guy being my thoued your to cen yiellr more and tell be thathart time work.\n",
      "\n",
      "MCKOR WALDAN:\n",
      "All t\n"
     ]
    }
   ],
   "source": [
    "path = './best_model'\n",
    "with open(path, 'rb') as f:\n",
    "    rnn = pickle.load(f)\n",
    "print(\"Loaded...\")\n",
    "\n",
    "\n",
    "seed = \"HAN:\\nYes?\\n\\n\"\n",
    "n_sample = 300\n",
    "sampled = sample(rnn, seed, n_sample, dataset)\n",
    "print(''.join(sampled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Dumped\n"
     ]
    }
   ],
   "source": [
    "with open(\"best_model\", \"wb\") as f:\n",
    "    pickle.dump(rnn, f)\n",
    "    print('> Dumped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
