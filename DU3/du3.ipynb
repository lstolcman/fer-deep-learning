{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys_version: 3.5.2 (default, Nov 17 2016, 17:05:23) [GCC 5.4.0 20160609]\n",
      "virtual_env None\n",
      "pwd /home/marko/Projects/faks/DU/DU3\n",
      "np  1.11.1\n",
      "tf  0.10.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pdb\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print('sys_version:', sys.version.replace('\\n', ''))\n",
    "print('virtual_env', os.environ.get('VIRTUAL_ENV', 'None'))\n",
    "print('pwd', os.getcwd())\n",
    "print('np ', np.__version__)\n",
    "print('tf ', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    \n",
    "    def __init__(self, batch_size, sequence_length):\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.batch_index = 0\n",
    "    \n",
    "    def preprocess(self, input_file):\n",
    "        with open(input_file, \"r\") as f:\n",
    "            data = f.read()\n",
    "\n",
    "        # count and sort most frequent characters\n",
    "        chars, cnts = np.unique(list(data), return_index=True)\n",
    "        self.sorted_chars = chars[np.argsort(-cnts)]\n",
    "        self.vocab_size = len(self.sorted_chars)\n",
    "        \n",
    "        # other way\n",
    "        #cntr = Counter(data)\n",
    "        #self.sorted_chars = sorted(cntr.keys(), key=cntr.get, reverse=True)\n",
    "\n",
    "        # self.sorted chars contains just the characters ordered descending by frequency\n",
    "        self.char2id = dict(zip(self.sorted_chars, range(len(self.sorted_chars)))) \n",
    "        self.id2char = {k:v for v,k in self.char2id.items()}\n",
    "        self.x = np.array(list(map(self.char2id.get, data)))\n",
    "\n",
    "    def encode(self, sequence):\n",
    "        return np.array([self.char2id[c] for c in sequence], dtype=np.int32)\n",
    "\n",
    "    def decode(self, encoded_sequence):\n",
    "        return [self.id2char[c] for c in encoded_sequence]\n",
    "        \n",
    "    def create_minibatches(self):\n",
    "        data_len = len(self.x)\n",
    "        chars_per_batch = self.batch_size * self.sequence_length\n",
    "        self.num_batches = int((data_len-1) / chars_per_batch) \n",
    " \n",
    "        self.batches = np.zeros([self.num_batches, self.batch_size, self.sequence_length + 1], dtype=np.int32)      \n",
    "        for b in range(self.num_batches):\n",
    "            for s in range(self.batch_size):\n",
    "                sentance_start = s*(self.num_batches*self.sequence_length)\n",
    "                start = b * self.sequence_length + sentance_start\n",
    "                end = start + self.sequence_length + 1 \n",
    "                self.batches[b, s, :] = self.x[start:end]\n",
    "                        \n",
    "        self.batch_index = 0\n",
    "\n",
    "    def next_minibatch(self):\n",
    "        new_epoch = self.batch_index == self.num_batches\n",
    "        if new_epoch:\n",
    "            self.batch_index = 0\n",
    "\n",
    "        batch = self.batches[self.batch_index, :, :]\n",
    "        self.batch_index += 1\n",
    "        \n",
    "        batch_x = batch[:, :-1]\n",
    "        batch_y = batch[:, 1:]\n",
    "        return new_epoch, batch_x, batch_y\n",
    "    \n",
    "    def _as_one_hot(self, x, vocab):\n",
    "        n = len(x)\n",
    "        Yoh = np.zeros((n, vocab))\n",
    "        Yoh[np.arange(n), x] = 1\n",
    "        return Yoh\n",
    "    \n",
    "\n",
    "    def one_hot(self, batch):\n",
    "        if batch.ndim == 1:\n",
    "            return self._as_one_hot(batch, self.vocab_size)\n",
    "        else:\n",
    "            return np.array([self._as_one_hot(s, self.vocab_size) for s in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0\n",
      "False\n",
      "X [['a', 'b', 'c'], ['g', 'h', 'i'], ['m', 'n', 'o']]\n",
      "Y [['b', 'c', 'd'], ['h', 'i', 'j'], ['n', 'o', 'p']]\n",
      "\n",
      "\n",
      "\n",
      "Batch: 1\n",
      "False\n",
      "X [['d', 'e', 'f'], ['j', 'k', 'l'], ['p', 'q', 'r']]\n",
      "Y [['e', 'f', 'g'], ['k', 'l', 'm'], ['q', 'r', 's']]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test 1\n",
    "dat = Dataset(3, 3)\n",
    "dat.preprocess(\"test.txt\")\n",
    "txt = \"hjdhasjdhjasdhja\"\n",
    "assert txt != dat.decode(dat.encode(txt))\n",
    "\n",
    "\n",
    "dat.create_minibatches()\n",
    "for i in range(dat.num_batches):\n",
    "    print(\"Batch:\", i)\n",
    "    f, s, t = dat.next_minibatch()\n",
    "    print(f)\n",
    "    print(\"X\", list(map(dat.decode, s)))\n",
    "    print(\"Y\", list(map(dat.decode, t)))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \n",
    "    def __init__(self, hidden_size, sequence_length, vocab_size, learning_rate):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Xavier\n",
    "        self.U = np.random.normal(size=[vocab_size, hidden_size], scale=1.0 / np.sqrt(hidden_size))  # ... input projection\n",
    "        self.W = np.random.normal(size=[hidden_size, hidden_size], scale=1.0 / np.sqrt(hidden_size))  # ... hidden-to-hidden projection\n",
    "        self.b = np.zeros([1, hidden_size])\n",
    "        \n",
    "        \n",
    "        self.V = np.random.normal(size=[hidden_size, vocab_size], scale=1.0 / np.sqrt(vocab_size))  # ... output projection\n",
    "        self.c = np.zeros([1, vocab_size]) # ... output bias\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        # memory of past gradients - rolling sum of squares for Adagrad\n",
    "        self.memory_U, self.memory_W, self.memory_V = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "        self.memory_b, self.memory_c = np.zeros_like(self.b), np.zeros_like(self.c)\n",
    "    \n",
    "    \n",
    "    def rnn_step_forward(self, x, h_prev, U, W, b):\n",
    "        # A single time step forward of a recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity.\n",
    "\n",
    "        # x - input data (minibatch size x input dimension)\n",
    "        # h_prev - previous hidden state (minibatch size x hidden size)\n",
    "        # U - input projection matrix (input dimension x hidden size)\n",
    "        # W - hidden to hidden projection matrix (hidden size x hidden size)\n",
    "        # b - bias of shape (hidden size x 1).T\n",
    "        \n",
    "        \n",
    "        h_current = np.tanh(np.dot(h_prev, W) + np.dot(x, U) + b)\n",
    "        cache = (W, x, h_prev, h_current)\n",
    "        return h_current, cache\n",
    "    \n",
    "    def rnn_forward(self, x, h0, U, W, b):\n",
    "        # Full unroll forward of the recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity\n",
    "\n",
    "        # x - input data for the whole time-series (minibatch size x sequence_length x input dimension)\n",
    "        # h0 - initial hidden state (minibatch size x hidden size)\n",
    "        # U - input projection matrix (input dimension x hidden size)\n",
    "        # W - hidden to hidden projection matrix (hidden size x hidden size)\n",
    "        # b - bias of shape (hidden size x 1).T\n",
    "        \n",
    "        h, cache = [h0], []\n",
    "        for t in range(self.sequence_length):\n",
    "            data = x[:, t, :] #t-th entry\n",
    "            current_h, current_cache = self.rnn_step_forward(data, h[-1], U, W, b)\n",
    "            h.append(current_h)\n",
    "            cache.append(current_cache)\n",
    "\n",
    "\n",
    "        # return the hidden states for the whole time series (T+1) and a tuple of values needed for the backward step\n",
    "        h = np.array(h).transpose((1, 0, 2))\n",
    "        return h, cache\n",
    "\n",
    "    \n",
    "    \n",
    "    def rnn_step_backward(self, grad_next, cache):\n",
    "        # A single time step backward of a recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity.\n",
    "\n",
    "        # grad_next - upstream gradient of the loss with respect to the next hidden state and current output\n",
    "        # cache - cached information from the forward pass\n",
    "        \n",
    "        W, x, h_prev, h_curr = cache\n",
    "        dz = grad_next * (1 - h_curr**2)\n",
    "        \n",
    "        dh_prev = np.dot(dz, W.T)\n",
    "        dU = np.dot(x.T, dz)\n",
    "        dW = np.dot(h_prev.T, dz)\n",
    "        db = np.sum(dz, axis=0)\n",
    "        \n",
    "        return dh_prev, dU, dW, db\n",
    "\n",
    "\n",
    "    def rnn_backward(self, dh, cache):\n",
    "        # Full unroll forward of the recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity\n",
    "        dU, dW, db = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.b)\n",
    "        \n",
    "\n",
    "        # compute and return gradients with respect to each parameter\n",
    "        # for the whole time series.\n",
    "        upstream_grad = np.zeros_like(dh[0])\n",
    "        for dh_t, cache_t in reversed(list(zip(dh, cache))):\n",
    "            upstream_grad, dU_t, dW_t, db_t = self.rnn_step_backward(dh_t + upstream_grad, cache_t)\n",
    "            dU += dU_t; dW += dW_t; db += db_t; \n",
    "        \n",
    "        clip = lambda x: np.clip(x, -5, 5)\n",
    "        return clip(dU), clip(dW), clip(db)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def output(self, h, V, c):\n",
    "        # Calculate the output probabilities of the network\n",
    "        return np.dot(h, V) + c\n",
    "    \n",
    "    def softmax(self, o):\n",
    "        exp = np.exp(o)\n",
    "        s = exp / np.sum(exp, axis=1, keepdims=True)\n",
    "        return s\n",
    "    \n",
    "    def output_loss_and_grads(self, h, V, c, y):\n",
    "        # Calculate the loss of the network for each of the outputs\n",
    "\n",
    "        # h - hidden states of the network for each timestep. \n",
    "        #     the dimensionality of h is (batch size x sequence length x hidden size (the initial state is irrelevant for the output)\n",
    "        # V - the output projection matrix of dimension hidden size x vocabulary size\n",
    "        # c - the output bias of dimension vocabulary size x 1\n",
    "        # y - the true class distribution - a one-hot vector of dimension \n",
    "        #     vocabulary size x 1 - you need to do this conversion prior to\n",
    "        #     passing the argument. A fast way to create a one-hot vector from\n",
    "        #     an id could be something like the following code:\n",
    "\n",
    "        #   y[timestep] = np.zeros((vocabulary_size, 1))\n",
    "        #   y[timestep][batch_y[timestep]] = 1\n",
    "\n",
    "        #     where y might be a dictionary.\n",
    "\n",
    "        loss, dh, dV, dc = 0.0, [], np.zeros_like(self.V), np.zeros_like(self.c)\n",
    "        for t in range(self.sequence_length):\n",
    "            yp = y[:, t, :]\n",
    "\n",
    "            h_t = h[:, t, :]\n",
    "            \n",
    "            o = self.output(h_t, V, c)\n",
    "            s = self.softmax(o)\n",
    "            \n",
    "            dO = s - yp\n",
    "            \n",
    "            dV += np.dot(h_t.T, dO)\n",
    "            dc += np.sum(dO, axis=0)\n",
    "            \n",
    "            dh_t = np.dot(dO, V.T)\n",
    "            dh.append(dh_t)\n",
    "            loss += -np.sum(np.log(s)*yp)\n",
    "            \n",
    "        loss /= len(h)\n",
    "        return loss, dh, dV, dc\n",
    "    \n",
    "    \n",
    "    \n",
    "    def update(self, batch_size, dU, dW, db, dV, dc):\n",
    "        eps = 1e-7\n",
    "        \n",
    "        # update memory matrices\n",
    "        # perform the Adagrad update of parameters\n",
    "        mean = lambda values: [v/batch_size for v in values]\n",
    "        dU, dW, db, dV, dc = mean([dU, dW, db, dV, dc])\n",
    "        \n",
    "        self.memory_U += np.square(dU)\n",
    "        self.memory_W += np.square(dW)\n",
    "        self.memory_b += np.square(db)\n",
    "        self.memory_V += np.square(dV)\n",
    "        self.memory_c += np.square(dc)\n",
    "        \n",
    "        update_param = lambda dx, mem_x: self.learning_rate * dx / np.sqrt(mem_x + eps)\n",
    "        \n",
    "        self.U -= update_param(dU, self.memory_U)\n",
    "        self.W -= update_param(dW, self.memory_W)\n",
    "        self.b -= update_param(db, self.memory_b)\n",
    "        self.V -= update_param(dV, self.memory_V)\n",
    "        self.c -= update_param(dc, self.memory_c)\n",
    "        \n",
    "    def step(self, h, x, y):\n",
    "        h, cache = self.rnn_forward(x, h, self.U, self.W, self.b)\n",
    "        loss, dh, dV, dc = self.output_loss_and_grads(h, self.V, self.c, y)\n",
    "        dU, dW, db = self.rnn_backward(dh, cache)\n",
    "        self.update(len(x), dU, dW, db, dV, dc)\n",
    "        return loss, h[:, -1, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample(rnn, seed, n_sample, dataset):\n",
    "    h0 = np.zeros([1, rnn.hidden_size])\n",
    "    seed_oh = dataset.one_hot(dataset.encode(seed))\n",
    "    \n",
    "    sampled = []\n",
    "    for c_oh in seed_oh:\n",
    "        h0, _ = rnn.rnn_step_forward(c_oh.reshape([1, -1]), h0, rnn.U, rnn.W, rnn.b)\n",
    "        sampled.append(np.argmax(c_oh))\n",
    "    \n",
    "    for i in range(len(seed), n_sample):\n",
    "        prev_out = np.array([sampled[-1]])\n",
    "        in_oh = dataset.one_hot(prev_out)\n",
    "        h0, _ = rnn.rnn_step_forward(in_oh, h0, rnn.U, rnn.W, rnn.b)\n",
    "        \n",
    "        out = rnn.output(h0, rnn.V, rnn.c)\n",
    "        sampled.append(np.argmax(out))\n",
    "  \n",
    "    return dataset.decode(sampled)\n",
    "\n",
    "\n",
    "def run_language_model(dataset, max_epochs, hidden_size=100, sequence_length=30, learning_rate=1e-1, sample_every=10000):\n",
    "    \n",
    "    vocab_size = len(dataset.sorted_chars)\n",
    "    rnn = RNN(hidden_size, sequence_length, vocab_size, learning_rate)\n",
    "\n",
    "    current_epoch = 0 \n",
    "    batch = 0\n",
    "\n",
    "    h0 = np.zeros((dataset.batch_size, hidden_size))\n",
    "    cum_loss = 0\n",
    "\n",
    "    while current_epoch < max_epochs: \n",
    "        e, x, y = dataset.next_minibatch()\n",
    "        \n",
    "        if e: \n",
    "            current_epoch += 1\n",
    "            h0 = np.zeros((dataset.batch_size, hidden_size))\n",
    "\n",
    "        # One-hot transform the x and y batches\n",
    "        x_oh, y_oh = dataset.one_hot(x), dataset.one_hot(y)\n",
    "\n",
    "\n",
    "        loss, h0 = rnn.step(h0, x_oh, y_oh)\n",
    "        cum_loss += loss\n",
    "        \n",
    "        if batch % sample_every == 0: \n",
    "            seed = \"HAN:\\nIs that good or bad?\\n\\n\"\n",
    "            n_sample = 300\n",
    "            sampled = sample(rnn, seed, n_sample, dataset)\n",
    "            print(''.join(sampled))\n",
    "            print()\n",
    "            \n",
    "           \n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            current_batch = batch % dataset.num_batches\n",
    "            print(\"epoch: %06d:\\tbatch: %4d/%d\\t\" % (current_epoch, current_batch, dataset.num_batches), end=\"\")\n",
    "            print(\"Average_loss: %.4f\" % (cum_loss/batch))\n",
    "            \n",
    "        batch += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "pupPnaupupupPNauaupupdnauawpupxpPNawaupxpPNawaupxpPNawaupxpPNawaupxpPNawaupxpPNawaupxpPNawaupxpPNawaupxpPNawaupxpPNawaupxpPNawaupxpPNawaupxpPNawaupxpPNawaupxpPNawaupxpPNawaupxpPNawaupxpPNawaupxpPNawaupxpPNawaupxpPNawaupxpPNawaupxpPNawaupxpPNawaupxpPNawaupxpPNawaupxpPNawaup\n",
      "\n",
      "epoch: 000000:\tbatch:    0/1315\tAverage_loss: inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py:58: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 000000:\tbatch:  100/1315\tAverage_loss: 66.9471\n",
      "epoch: 000000:\tbatch:  200/1315\tAverage_loss: 59.7389\n",
      "epoch: 000000:\tbatch:  300/1315\tAverage_loss: 56.8510\n",
      "epoch: 000000:\tbatch:  400/1315\tAverage_loss: 55.1005\n",
      "epoch: 000000:\tbatch:  500/1315\tAverage_loss: 53.8051\n",
      "epoch: 000000:\tbatch:  600/1315\tAverage_loss: 52.7578\n",
      "epoch: 000000:\tbatch:  700/1315\tAverage_loss: 51.9106\n",
      "epoch: 000000:\tbatch:  800/1315\tAverage_loss: 51.2204\n",
      "epoch: 000000:\tbatch:  900/1315\tAverage_loss: 50.6568\n",
      "epoch: 000000:\tbatch: 1000/1315\tAverage_loss: 50.1799\n",
      "epoch: 000000:\tbatch: 1100/1315\tAverage_loss: 49.7046\n",
      "epoch: 000000:\tbatch: 1200/1315\tAverage_loss: 49.2734\n",
      "epoch: 000000:\tbatch: 1300/1315\tAverage_loss: 48.8911\n",
      "epoch: 000001:\tbatch:   85/1315\tAverage_loss: 48.5975\n",
      "epoch: 000001:\tbatch:  185/1315\tAverage_loss: 48.3442\n",
      "epoch: 000001:\tbatch:  285/1315\tAverage_loss: 48.1142\n",
      "epoch: 000001:\tbatch:  385/1315\tAverage_loss: 47.9008\n",
      "epoch: 000001:\tbatch:  485/1315\tAverage_loss: 47.7602\n",
      "epoch: 000001:\tbatch:  585/1315\tAverage_loss: 47.6313\n",
      "epoch: 000001:\tbatch:  685/1315\tAverage_loss: 47.4819\n",
      "epoch: 000001:\tbatch:  785/1315\tAverage_loss: 47.3173\n",
      "epoch: 000001:\tbatch:  885/1315\tAverage_loss: 47.1508\n",
      "epoch: 000001:\tbatch:  985/1315\tAverage_loss: 47.0036\n",
      "epoch: 000001:\tbatch: 1085/1315\tAverage_loss: 46.8441\n",
      "epoch: 000001:\tbatch: 1185/1315\tAverage_loss: 46.6964\n",
      "epoch: 000001:\tbatch: 1285/1315\tAverage_loss: 46.5504\n",
      "epoch: 000002:\tbatch:   70/1315\tAverage_loss: 46.4311\n",
      "epoch: 000002:\tbatch:  170/1315\tAverage_loss: 46.3104\n",
      "epoch: 000002:\tbatch:  270/1315\tAverage_loss: 46.1946\n",
      "epoch: 000002:\tbatch:  370/1315\tAverage_loss: 46.0789\n",
      "epoch: 000002:\tbatch:  470/1315\tAverage_loss: 45.9480\n",
      "epoch: 000002:\tbatch:  570/1315\tAverage_loss: 45.8309\n",
      "epoch: 000002:\tbatch:  670/1315\tAverage_loss: 45.7285\n",
      "epoch: 000002:\tbatch:  770/1315\tAverage_loss: 45.6140\n",
      "epoch: 000002:\tbatch:  870/1315\tAverage_loss: 45.5033\n",
      "epoch: 000002:\tbatch:  970/1315\tAverage_loss: 45.4150\n",
      "epoch: 000002:\tbatch: 1070/1315\tAverage_loss: 45.3204\n",
      "epoch: 000002:\tbatch: 1170/1315\tAverage_loss: 45.2334\n",
      "epoch: 000002:\tbatch: 1270/1315\tAverage_loss: 45.1445\n",
      "epoch: 000003:\tbatch:   55/1315\tAverage_loss: 45.0596\n",
      "epoch: 000003:\tbatch:  155/1315\tAverage_loss: 44.9796\n",
      "epoch: 000003:\tbatch:  255/1315\tAverage_loss: 44.9051\n",
      "epoch: 000003:\tbatch:  355/1315\tAverage_loss: 44.8285\n",
      "epoch: 000003:\tbatch:  455/1315\tAverage_loss: 44.7460\n",
      "epoch: 000003:\tbatch:  555/1315\tAverage_loss: 44.6760\n",
      "epoch: 000003:\tbatch:  655/1315\tAverage_loss: 44.6146\n",
      "epoch: 000003:\tbatch:  755/1315\tAverage_loss: 44.5442\n",
      "epoch: 000003:\tbatch:  855/1315\tAverage_loss: 44.4753\n",
      "epoch: 000003:\tbatch:  955/1315\tAverage_loss: 44.4227\n",
      "epoch: 000003:\tbatch: 1055/1315\tAverage_loss: 44.3633\n",
      "epoch: 000003:\tbatch: 1155/1315\tAverage_loss: 44.3099\n",
      "epoch: 000003:\tbatch: 1255/1315\tAverage_loss: 44.2560\n",
      "epoch: 000004:\tbatch:   40/1315\tAverage_loss: 44.2048\n",
      "epoch: 000004:\tbatch:  140/1315\tAverage_loss: 44.1529\n",
      "epoch: 000004:\tbatch:  240/1315\tAverage_loss: 44.1096\n",
      "epoch: 000004:\tbatch:  340/1315\tAverage_loss: 44.0632\n",
      "epoch: 000004:\tbatch:  440/1315\tAverage_loss: 44.0103\n",
      "epoch: 000004:\tbatch:  540/1315\tAverage_loss: 43.9625\n",
      "epoch: 000004:\tbatch:  640/1315\tAverage_loss: 43.9199\n",
      "epoch: 000004:\tbatch:  740/1315\tAverage_loss: 43.8724\n",
      "epoch: 000004:\tbatch:  840/1315\tAverage_loss: 43.8263\n",
      "epoch: 000004:\tbatch:  940/1315\tAverage_loss: 43.7882\n",
      "epoch: 000004:\tbatch: 1040/1315\tAverage_loss: 43.7483\n",
      "epoch: 000004:\tbatch: 1140/1315\tAverage_loss: 43.7115\n",
      "epoch: 000004:\tbatch: 1240/1315\tAverage_loss: 43.6748\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-a474a7f8416c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dataset/selected_conversations.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_minibatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrun_language_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-46-370f15e3c247>\u001b[0m in \u001b[0;36mrun_language_model\u001b[0;34m(dataset, max_epochs, hidden_size, sequence_length, learning_rate, sample_every)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_oh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_oh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mcum_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-ca7cd14255c2>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, h, x, y)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_loss_and_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0mdU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-ca7cd14255c2>\u001b[0m in \u001b[0;36moutput_loss_and_grads\u001b[0;34m(self, h, V, c, y)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mdO\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0myp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mdV\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0mdc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = Dataset(30, 15)\n",
    "dataset.preprocess(\"dataset/selected_conversations.txt\")\n",
    "dataset.create_minibatches()\n",
    "run_language_model(dataset, 100000, sequence_length=dataset.sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
