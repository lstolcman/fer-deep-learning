{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys_version: 3.5.2 (default, Nov 17 2016, 17:05:23) [GCC 5.4.0 20160609]\n",
      "virtual_env None\n",
      "pwd /home/marko/Projects/faks/DU/DU3\n",
      "np  1.11.1\n",
      "tf  0.10.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pdb\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print('sys_version:', sys.version.replace('\\n', ''))\n",
    "print('virtual_env', os.environ.get('VIRTUAL_ENV', 'None'))\n",
    "print('pwd', os.getcwd())\n",
    "print('np ', np.__version__)\n",
    "print('tf ', tf.__version__)\n",
    "\n",
    "\n",
    "from IPython.core.debugger import Tracer\n",
    "BREAK_POINT = lambda: Tracer()()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0\n",
      "False\n",
      "X [['a', 'b', 'c'], ['g', 'h', 'i'], ['m', 'n', 'o']]\n",
      "Y [['b', 'c', 'd'], ['h', 'i', 'j'], ['n', 'o', 'p']]\n",
      "\n",
      "\n",
      "\n",
      "Batch: 1\n",
      "False\n",
      "X [['d', 'e', 'f'], ['j', 'k', 'l'], ['p', 'q', 'r']]\n",
      "Y [['e', 'f', 'g'], ['k', 'l', 'm'], ['q', 'r', 's']]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test 1\n",
    "dat = Dataset(3, 3)\n",
    "dat.preprocess(\"test.txt\")\n",
    "txt = \"hjdhasjdhjasdhja\"\n",
    "assert txt != dat.decode(dat.encode(txt))\n",
    "\n",
    "\n",
    "dat.create_minibatches()\n",
    "for i in range(dat.num_batches):\n",
    "    print(\"Batch:\", i)\n",
    "    f, s, t = dat.next_minibatch()\n",
    "    print(f)\n",
    "    print(\"X\", list(map(dat.decode, s)))\n",
    "    print(\"Y\", list(map(dat.decode, t)))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pdb\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "\n",
    "class RNN:\n",
    "    \n",
    "    def __init__(self, hidden_size, sequence_length, vocab_size, learning_rate):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Xavier\n",
    "        self.U = np.random.normal(size=[vocab_size, hidden_size], scale=1.0 / np.sqrt(hidden_size))  # ... input projection\n",
    "        self.W = np.random.normal(size=[hidden_size, hidden_size], scale=1.0 / np.sqrt(hidden_size))  # ... hidden-to-hidden projection\n",
    "        self.b = np.zeros([1, hidden_size])\n",
    "        \n",
    "        \n",
    "        self.V = np.random.normal(size=[hidden_size, vocab_size], scale=1.0 / np.sqrt(vocab_size))  # ... output projection\n",
    "        self.c = np.zeros([1, vocab_size]) # ... output bias\n",
    "\n",
    "        \n",
    "        # memory of past gradients - rolling sum of squares for Adagrad\n",
    "        self.memory_U, self.memory_W, self.memory_V = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "        self.memory_b, self.memory_c = np.zeros_like(self.b), np.zeros_like(self.c)\n",
    "    \n",
    "    \n",
    "    def rnn_step_forward(self, x, h_prev,  U, W, b):\n",
    "        # A single time step forward of a recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity.\n",
    "\n",
    "        # x - input data (minibatch size x input dimension)\n",
    "        # h_prev - previous hidden state (minibatch size x hidden size)\n",
    "        # U - input projection matrix (input dimension x hidden size)\n",
    "        # W - hidden to hidden projection matrix (hidden size x hidden size)\n",
    "        # b - bias of shape (hidden size x 1).T\n",
    "        \n",
    "        h_current = np.tanh(np.dot(h_prev, W) + np.dot(x, U) + b)\n",
    "        cache = (W, x, h_prev, h_current)\n",
    "        return h_current, cache\n",
    "    \n",
    "    def rnn_forward(self, x, h0,  U, W, b):\n",
    "        # Full unroll forward of the recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity\n",
    "\n",
    "        # x - input data for the whole time-series (minibatch size x sequence_length x input dimension)\n",
    "        # h0 - initial hidden state (minibatch size x hidden size)\n",
    "        # U - input projection matrix (input dimension x hidden size)\n",
    "        # W - hidden to hidden projection matrix (hidden size x hidden size)\n",
    "        # b - bias of shape (hidden size x 1).T\n",
    "        \n",
    "        h, cache = [h0], []\n",
    "        for t in range(self.sequence_length):\n",
    "            data = x[:, t, :] #t-th entry\n",
    "            current_h, current_cache = self.rnn_step_forward(data, h[-1], U, W, b)\n",
    "            h.append(current_h)\n",
    "            cache.append(current_cache)\n",
    "\n",
    "\n",
    "        # return the hidden states for the whole time series (T+1) and a tuple of values needed for the backward step\n",
    "        h = np.array(h[1:]).transpose((1, 0, 2)) # skip initial state\n",
    "        return h, cache\n",
    "\n",
    "    \n",
    "    \n",
    "    def rnn_step_backward(self, grad_next, cache):\n",
    "        # A single time step backward of a recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity.\n",
    "\n",
    "        # grad_next - upstream gradient of the loss with respect to the next hidden state and current output\n",
    "        # cache - cached information from the forward pass\n",
    "        \n",
    "        W, x, h_prev, h_curr = cache\n",
    "        dz = grad_next * (1 - h_curr**2)\n",
    "        \n",
    "        dh_prev = np.dot(dz, W.T)\n",
    "        dU = np.dot(x.T, dz)\n",
    "        dW = np.dot(h_prev.T, dz)\n",
    "        db = np.sum(dz, axis=0)\n",
    "        \n",
    "        return dh_prev, dU, dW, db\n",
    "\n",
    "\n",
    "    def rnn_backward(self, dh, cache):\n",
    "        # Full unroll forward of the recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity\n",
    "        dU, dW, db = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.b)\n",
    "        \n",
    "\n",
    "        # compute and return gradients with respect to each parameter\n",
    "        # for the whole time series.\n",
    "        upstream_grad = np.zeros_like(dh[0])\n",
    "        for dh_t, cache_t in reversed(list(zip(dh, cache))):\n",
    "            upstream_grad, dU_t, dW_t, db_t = self.rnn_step_backward(dh_t + upstream_grad, cache_t)\n",
    "            dU += dU_t\n",
    "            dW += dW_t\n",
    "            db += db_t; \n",
    "\n",
    "        clip = lambda x: np.clip(x, -5, 5)\n",
    "        return clip(dU), clip(dW), clip(db)\n",
    "    \n",
    "    \n",
    "    def output(self, h, V, c):\n",
    "        # Calculate the output probabilities of the network\n",
    "        return np.dot(h, V) + c\n",
    "    \n",
    "    def output_probas(self, h, V, c):\n",
    "        return self.softmax(self.output(h, V, c))\n",
    "    \n",
    "    def softmax(self, o):\n",
    "        exp = np.exp(o)\n",
    "        s = exp / np.sum(exp, axis=1, keepdims=True)\n",
    "        return s\n",
    "    \n",
    "    def output_loss_and_grads(self, h, V, c, y):\n",
    "        # Calculate the loss of the network for each of the outputs\n",
    "\n",
    "        # h - hidden states of the network for each timestep. \n",
    "        #     the dimensionality of h is (batch size x sequence length x hidden size (the initial state is irrelevant for the output)\n",
    "        # V - the output projection matrix of dimension hidden size x vocabulary size\n",
    "        # c - the output bias of dimension vocabulary size x 1\n",
    "        # y - the true class distribution - a one-hot vector of dimension \n",
    "        #     vocabulary size x 1 - you need to do this conversion prior to\n",
    "        #     passing the argument. A fast way to create a one-hot vector from\n",
    "        #     an id could be something like the following code:\n",
    "\n",
    "        #   y[timestep] = np.zeros((vocabulary_size, 1))\n",
    "        #   y[timestep][batch_y[timestep]] = 1\n",
    "\n",
    "        #     where y might be a dictionary.\n",
    "\n",
    "        loss, dh, dV, dc = 0.0, [], np.zeros_like(self.V), np.zeros_like(self.c)\n",
    "        batch_size = len(h)\n",
    "        \n",
    "        for t in range(self.sequence_length):\n",
    "            yp = y[:, t, :]\n",
    "            h_t = h[:, t, :]\n",
    "            \n",
    "            o = self.output(h_t, V, c)\n",
    "            s = self.softmax(o)\n",
    "            \n",
    "            loss += -np.sum(np.log(s)*yp) / batch_size\n",
    "            dO = (s - yp) / batch_size\n",
    "            \n",
    "            dV += np.dot(h_t.T, dO)\n",
    "            dc += np.sum(dO, axis=0)\n",
    "            \n",
    "            dh_t = np.dot(dO, V.T)\n",
    "            dh.append(dh_t)\n",
    "            \n",
    "        return loss, dh, dV, dc\n",
    "    \n",
    "    \n",
    "    \n",
    "    def update(self, batch_size, dU, dW, db, dV, dc):\n",
    "        eps = 1e-7\n",
    "        \n",
    "        # perform the Adagrad update of parameters\n",
    "        params = [self.U, self.W, self.b, self.V, self.c]\n",
    "        ders = [dU, dW, db, dV, dc]\n",
    "        mems = [self.memory_U, self.memory_W, self.memory_b, self.memory_V, self.memory_c]\n",
    "        \n",
    "        for x, dx, mem_x in zip(params, ders, mems):\n",
    "            mem_x += np.square(dx)\n",
    "            x -= self.learning_rate * dx / np.sqrt(mem_x + eps)\n",
    "\n",
    "        \n",
    "    def step(self, h, x, y):\n",
    "        h, cache = self.rnn_forward(x, h, self.U, self.W, self.b)\n",
    "        loss, dh, dV, dc = self.output_loss_and_grads(h, self.V, self.c, y)\n",
    "        dU, dW, db = self.rnn_backward(dh, cache)\n",
    "        self.update(len(x), dU, dW, db, dV, dc)\n",
    "        return loss, h[:, -1, :]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pdb\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "from dataset import Dataset\n",
    "#from rnn import RNN\n",
    "\n",
    "\n",
    "def sample(rnn, seed, n_sample, dataset):\n",
    "    h0 = np.zeros([1, rnn.hidden_size])\n",
    "    seed_oh = dataset.one_hot(dataset.encode(seed))\n",
    "    \n",
    "    sampled = []\n",
    "    \n",
    "    for c_oh in seed_oh:\n",
    "        h0, _ = rnn.rnn_step_forward(c_oh.reshape([1, -1]), h0, rnn.U, rnn.W, rnn.b)\n",
    "        sampled.append(np.argmax(c_oh))\n",
    "        \n",
    "    for i in range(len(seed), n_sample):\n",
    "        prev_out = np.array([sampled[-1]])\n",
    "        in_oh = dataset.one_hot(prev_out)\n",
    "        h0, _ = rnn.rnn_step_forward(in_oh, h0, rnn.U, rnn.W, rnn.b)\n",
    "        \n",
    "        probas = rnn.output_probas(h0, rnn.V, rnn.c)\n",
    "        out_char_oh = np.random.choice(range(dataset.vocab_size), p=probas.ravel()) \n",
    "        sampled.append(out_char_oh)\n",
    "  \n",
    "    return dataset.decode(sampled)\n",
    "\n",
    "import pickle\n",
    "\n",
    "def run_language_model(dataset, max_epochs, hidden_size=100, sequence_length=30, learning_rate=1e-1, sample_every=1000, dump_path='./model'):\n",
    "    \n",
    "    vocab_size = len(dataset.sorted_chars)\n",
    "    rnn = RNN(hidden_size, sequence_length, vocab_size, learning_rate)\n",
    "\n",
    "    current_epoch = 0 \n",
    "    batch = 0\n",
    "\n",
    "    h0 = np.zeros((dataset.batch_size, hidden_size))\n",
    "    cum_loss = 0\n",
    "\n",
    "    while current_epoch < max_epochs: \n",
    "        e, x, y = dataset.next_minibatch()\n",
    "        \n",
    "        if e: \n",
    "            current_epoch += 1\n",
    "            h0 = np.zeros((dataset.batch_size, hidden_size))\n",
    "\n",
    "        # One-hot transform the x and y batches\n",
    "        x_oh, y_oh = dataset.one_hot(x), dataset.one_hot(y)\n",
    "\n",
    "\n",
    "        loss, h0 = rnn.step(h0, x_oh, y_oh)\n",
    "        cum_loss += loss\n",
    "        \n",
    "        if batch % sample_every == 0: \n",
    "            seed = \"HAN:\\nIs that good or bad?\\n\\n\"\n",
    "            n_sample = 300\n",
    "            sampled = sample(rnn, seed, n_sample, dataset)\n",
    "            print(''.join(sampled))\n",
    "            print()\n",
    "            with open(dump_path, \"wb\") as f:\n",
    "                pickle.dump(rnn, f)\n",
    "                print('> Dumped to:', dump_path)\n",
    "            \n",
    "           \n",
    "        \n",
    "        if batch % 1000 == 0:\n",
    "            current_batch = batch % dataset.num_batches\n",
    "            print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "            print(\"epoch: %06d:\\tbatch: %4d/%d\\t\" % (current_epoch, current_batch, dataset.num_batches), end=\"\")\n",
    "            print(\"Average_loss: %.4f; Last batch loss: %.4f\" % (cum_loss/batch, loss))\n",
    "            print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "            \n",
    "        batch += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "svv.\n",
      "mh9ANt:h0sst.oh,ks cgh ffe`h'h.ttthmHftdtohvftKohhaNt0whIvk t gIf:to,oAndhmohft,Ntts,et. monketGwsH1NRTs.juNhcmInd.gwsA1.tThh4tC0hTcio 2hgf4thT9Inn. hBssfN:hh9si0t0gssn.hch!uUY'o`fvtMchHHnnKwmIsLt?gwssueWhv6tnco gsnt8hw fttoghhHetogFsneN\n",
      "yont VdMIHOcwhh se.ohKtst.ghUw\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000000:\tbatch:    0/3947\tAverage_loss: inf; Last batch loss: 128.1473\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py:80: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "cYFTOIR.\n",
      "\n",
      "JEZMAFDR5FFSINKFFRECaAIs tasl\n",
      "\n",
      "OeANewte thanwaiIlPthams dhot'sklly Fit?\n",
      "\n",
      "JEEYET:\n",
      "Yok. TINan's at 'lin\n",
      "Mhat gnh g iwsy Jant lin yon?\n",
      "J wIton.on 6ing ton?\n",
      "\n",
      "JEFuS IA yone sonoy youPs o svint.o ton that doalald .onlik n6stokcank win I yon ghiilll nang to y at. he.\n",
      "\n",
      "D\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000000:\tbatch: 1000/3947\tAverage_loss: 89.4941; Last batch loss: 76.7048\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "EEON:\n",
      ".\n",
      "\n",
      "OMACKRIWhighat ctherbee lome, .\n",
      "\n",
      "JOACB:\n",
      "Deay.b3y t diyd!!\n",
      "\n",
      "DoANKALDl?.\n",
      "\n",
      "DECp'sddive ball aikdasher kibl\n",
      "\n",
      "DEN:\n",
      "You bescseml, bot got?\n",
      "\n",
      "DEH:\n",
      "Art avim. Cn'emes buy yaan,be.\n",
      "\n",
      "\n",
      "JEAND:\n",
      "Dor you wholahlers gore thasmy?\n",
      "\n",
      "FAED4AKD:\n",
      "You thaqd..y bustar mfalme wavo  ig blorip\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000000:\tbatch: 2000/3947\tAverage_loss: 79.2179; Last batch loss: 65.4104\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "EELTOOR:\n",
      "Fon't gek'm she Taue. we cow tell.\n",
      "\n",
      "SOORONR:\n",
      "\n",
      "fo kisthars grt't to hths?\n",
      "\n",
      "JECK:\n",
      "Tht ..  hat afk fas ane bo to thex.\n",
      "\n",
      "BOK:\n",
      "Lith, ul rve mer.  Bor gonk, Bre you gos Soull.\n",
      "\n",
      "\n",
      "TEUMON:\n",
      "Thon to kal ghe not luger tor, nimhst the heafe.\n",
      "\n",
      "LNSOOLBINe? fhthin's Tow lican'm o\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000000:\tbatch: 3000/3947\tAverage_loss: 74.5826; Last batch loss: 58.3299\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "ST:\n",
      "We yo\n",
      "DONgh fwhe you dowa I gocack en ame ceed hor you ler goin y de. I ard you's whit and you gos a hve se home tonc.\n",
      "\n",
      "TO N:\n",
      "I lesl... I lat than't dend. I west. OVEND:\n",
      "I st'r goe the gat you fike thatn stiel wascctry Gofked, man an itul ann wo on afoncy st wuo.... ra\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000001:\tbatch:   53/3947\tAverage_loss: 71.8142; Last batch loss: 69.5997\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "ELECK:\n",
      "I chid her am?\n",
      "\n",
      "CKARDY:\n",
      ". I Womy. Whit.\n",
      "\n",
      "SANDIbPASABMRE:\n",
      "He hat the I zere'cacars I've I alr ar untsh, paaads co fustakne?\n",
      "\n",
      "Dowhaogus, wartto. Yousen. Ton...\n",
      "\n",
      "MA. THARDY:\n",
      "Waming.\n",
      "\n",
      "MR. OOERSI SCE:\n",
      "I hawk a vacd gotrin..\n",
      "T fre hre tof gver orered welly I've.\n",
      "\n",
      "LII FuON\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000001:\tbatch: 1053/3947\tAverage_loss: 69.7830; Last batch loss: 60.0718\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "ELO:\n",
      " walva goke our whade the her Fe'cr but cors.\n",
      "\n",
      "DECKARD! I wistad, Gight nacky be seribers?\n",
      "\n",
      "CEGOMMACKA. I T I 'r.\n",
      "\n",
      "\n",
      "MANAY:\n",
      "Touple.!\n",
      "\n",
      "DEKE:\n",
      "Millilett tald onee this thonkel, ay dael.\n",
      "\n",
      "IOTFRSOCY:\n",
      "Wer your , se Ma leahine was a at bis por ulrseal you'alerlligone, Mas, yo\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000001:\tbatch: 2053/3947\tAverage_loss: 68.2394; Last batch loss: 60.4237\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "FOOAN:\n",
      "Well in must get.\n",
      "\n",
      "FAEDETE:\n",
      "You aber. He r Dor, ony to sals.\n",
      "\n",
      "DONN:\n",
      "Now The hal' the Seing ack in to ckin'. restit St... of.  You kust sime. So thang theoilen if minooing!.. bonoy sey lorsik!d. Hophill yas callerid.....re your shore tapte ve?\n",
      "\n",
      "BENN:\n",
      "You cho you Mish\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000001:\tbatch: 3053/3947\tAverage_loss: 67.1005; Last batch loss: 64.2720\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "EUNER:\n",
      "Walt osts cijus thate in werl yed doter! It'de fatanicring you wectar mr?\n",
      "\n",
      "BVERE:\n",
      "Vosk. What fon't me to to cagoce, se now pure ke tot onl fich oo on hram owkrooling ont youeal mevemets is wh ther it wisime sere?\n",
      "\n",
      "ERNIN:\n",
      "We jof tring wo huEN. Whouce donnstoys, letbm\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000002:\tbatch:  106/3947\tAverage_loss: 66.1572; Last batch loss: 62.3264\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "WERMAY:\n",
      "Oh nat gise wooline.  I mugd.\n",
      "\n",
      "GOMFFREY:\n",
      "What's havy and you de the thy.\n",
      "\n",
      "AVERICK:\n",
      "The?\n",
      "\n",
      "LONN:\n",
      "Arcem.\n",
      "\n",
      "CHIUK.:\n",
      "I toubly thinking ent you you'cO gWer yould.\n",
      "\n",
      "GETE:\n",
      "Thisa dight. Hevey, youn't upper andedern not's a ivits.  I!  he dink of the!p do wet's here overy, yo\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000002:\tbatch: 1106/3947\tAverage_loss: 65.1432; Last batch loss: 54.4365\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "AUMA:\n",
      "INfurth!\n",
      "\n",
      "MAFCKERE:\n",
      "Dorad a oind duekins goommys not ale popem, haply tial...... going.\n",
      "\n",
      "ELAY:\n",
      "Ok nollent a lris.\n",
      "\n",
      "MR.!\n",
      "\n",
      "JED:\n",
      "IAre'd!\n",
      "\n",
      "SARDY:\n",
      "My.\n",
      "\n",
      "MARD:\n",
      "I what pruppors cree?\n",
      "\n",
      "THARESMONDE I prel I.\n",
      "\n",
      "DTEISTHANDY:\n",
      "Wedlin?\n",
      "\n",
      "MOLD:\n",
      "Are amh are you do.\n",
      "\n",
      "ILANY:\n",
      "Araller conl\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000002:\tbatch: 2106/3947\tAverage_loss: 64.3536; Last batch loss: 58.8884\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DEESTY:\n",
      "She is saad tigl this hill thated and Sres.\n",
      "\n",
      "SEREEDEE:\n",
      "Sibly ip golas goootl. qule mu!t forer con't think sob.re to !\n",
      "\n",
      "MRYt I noy lathioms me is it.\n",
      "\n",
      "RUKERLEN:\n",
      "Heal, Callly gainen to I dightu.  I're.\n",
      "\n",
      "KORNY:\n",
      "Docke  upcrake . I'ngsiblup. This teaightue?\n",
      "\n",
      "BACHA:\n",
      "Hase\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000002:\tbatch: 3106/3947\tAverage_loss: 63.7348; Last batch loss: 59.4915\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "MEIZO:\n",
      "Vaik! Ael, elyen sann nome me hids be samen tell you knowel.\n",
      "\n",
      "SEKEN gho I wisn's Lind ever ssir is Jean dop sow ale what Che.\n",
      "\n",
      "JEFFREY:\n",
      "Lo ke me gditlasisn hialing was ellus me te you fance it verd kasiow. Me wan anding is ale ne mon'a arbar be. Wo can ores, Hom the\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000003:\tbatch:  159/3947\tAverage_loss: 63.1791; Last batch loss: 55.3454\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DAVEURENI MANDDUNE:\n",
      "Dton hell a the knen.\n",
      "\n",
      "GURMAN:\n",
      "What a litnt stark? Whos?\n",
      "\n",
      "Jrm there mo the the battagrs. We nond sone, pen's hive an, thit elaik you go do hamp as atle fack.\n",
      "WREFGILIE:\n",
      "You? Well sive?\n",
      "\n",
      "JOFFRIVAN:\n",
      "He Veeplyer to hom you.\n",
      "\n",
      "REDDUTE:\n",
      "If chen the just leflo\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000003:\tbatch: 1159/3947\tAverage_loss: 62.5430; Last batch loss: 50.9483\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DAGETD:\n",
      "Dowite you could the dow thee'VEE:\n",
      "Shexdive thinge freld thep if uathtred ow, have the badjeel tAry.\n",
      "\n",
      "DE INO:\n",
      "Wasm. But the want not futt's in the A doul ald?\n",
      "\n",
      "BASTY:\n",
      "How.\n",
      "\n",
      "TTURA:\n",
      "Dedalwing esuly bead anaise encer to but norins she, herg mey?\n",
      "\n",
      "JIMAR COKEANRY:\n",
      "You'r\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000003:\tbatch: 2159/3947\tAverage_loss: 62.0296; Last batch loss: 49.6413\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "GANDY:\n",
      "Regertid.\n",
      "\n",
      "DON:\n",
      "I hhe tall and we'll Pave My.\n",
      "\n",
      "SALAIA:\n",
      "Bak, Do. I.l the Fust, boc Fring him.\n",
      "\n",
      "SHSTH:\n",
      "We is the withing get bated of cout?\n",
      "\n",
      "SARAH:\n",
      "You geved I'm net forith?!, aboughelsmer.. youf I lemeds....\n",
      "\n",
      "KORNENT:\n",
      "Yes I mat mo and parb... Good it think your lory?\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000003:\tbatch: 3159/3947\tAverage_loss: 61.6097; Last batch loss: 46.7453\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DY:\n",
      "Dlelonions es up aplad's where, me the tho high. What's pand.\n",
      "\n",
      "BUFFFEETOR:\n",
      "Whes you here't wewy allyst...\n",
      "\n",
      "LLIEWTITS:\n",
      "He'd, worlorss, she put your is a fnoes, mo, what lete be id that doont pnotrdloce, wop blan'ssse futn't just tele and bett? I sace I'llsey go . I deph\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000004:\tbatch:  212/3947\tAverage_loss: 61.2358; Last batch loss: 47.8314\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "EELMAN:\n",
      "Sey wis dich a pich puck frur. non real.\n",
      "\n",
      "MANSTY:\n",
      "Five gapke.\n",
      "\n",
      "MR. NHTE:\n",
      "Rugunay go, wave!\n",
      "\n",
      "HENREN:\n",
      "You iny fan off fourtent a jove apded oo yow quod Frive ficke, whit, in mo bwan. Cor showeste to wx ay eresed is aniecty Kire mo your shought came ase just I wassitt\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000004:\tbatch: 1212/3947\tAverage_loss: 60.7886; Last batch loss: 57.3461\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "MESIT:\n",
      "What, abloter wercem lore wend cere here you latevt.\n",
      "SRoned so fucce.\n",
      "\n",
      "LAUR:\n",
      "The beneesth pofpabiline. So sid you I whemmin?\n",
      "\n",
      "BAROL:\n",
      " whem and!\n",
      "\n",
      "BEWI:\n",
      "You youf bun.\n",
      "\n",
      "DECKARD:\n",
      "The tigh. LObor.\n",
      "\n",
      "BETFUSAPOR:\n",
      "You just a miculd is, I mince you haut me to that excomestion\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000004:\tbatch: 2212/3947\tAverage_loss: 60.4076; Last batch loss: 54.2377\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "FREY:\n",
      "Lo you 'kire't?\n",
      "\n",
      "PRE!DA ACHIE:\n",
      "Thought?\n",
      "\n",
      "DOCH:\n",
      "The puckdaple I though.  I dayfond there sor you to you dase mugg inappigor, anduen div.ldomadint of a kiduackine... lot that Can the battabpodpl, mofifelitevive ortments tist the shates, ist mone.\n",
      "\n",
      "THAN:\n",
      "Fnealy.\n",
      "\n",
      "TERMY:\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000004:\tbatch: 3212/3947\tAverage_loss: 60.1074; Last batch loss: 54.5395\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DEIN:\n",
      "OU.\n",
      "\n",
      "GONZO:\n",
      "I wall be aparrleenccarw buck. The they you?\n",
      "\n",
      "MECIVIG:\n",
      "You're Was sut you for hals whick.\n",
      "SR gouded. Whound a renf getod, lle seed fastf.  I know? Yes, was'ad ufhich ad aid yim to know elkn about poks. ACk ass time lell and menaber sonuth the 'll whatire.\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000005:\tbatch:  265/3947\tAverage_loss: 59.8268; Last batch loss: 54.4440\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "ELAIA MANKON:\n",
      "You stods when?\n",
      "\n",
      "FREDDY:\n",
      "Cemerizelingoomey.\n",
      "\n",
      "RURMADEN:\n",
      "You dought onsing ouly?\n",
      "\n",
      "MR. IHABE, SKAMPINED:\n",
      "You aron'er good in rcor onettoon an the meart suer the go don't sid it. Thound a spemser toor the pickagnale?  I knab, Blonto saind.\n",
      "\n",
      "EDDY:\n",
      "He divinan's on \n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000005:\tbatch: 1265/3947\tAverage_loss: 59.4892; Last batch loss: 51.8811\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "MARLIENER:\n",
      "I ken neve of mave lething.irgee of minalby you wakefon it.\n",
      "\n",
      "BANTY:\n",
      "Thoug all you nease that us aceadly you blisn't.\n",
      "\n",
      "RETTY:\n",
      "The lighade.\n",
      "\n",
      "HAMON:\n",
      "Ben't riM.  be the have I 'll lot ust a sech makink not you, fided have but gaine cat you to make nool a licefa on i\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000005:\tbatch: 2265/3947\tAverage_loss: 59.2008; Last batch loss: 53.1279\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "ELVERbous it. I'll about has a qunfilittle Hiver and thine, nest.\n",
      "\n",
      "SAUSE:\n",
      "Rop chaspout roogenmzing in but hix tapdigher methench they.\n",
      "\n",
      "SERA:\n",
      "Store mucantcolle. Well know to on that daurce entenled it fount the's bir.\n",
      "\n",
      "SARAH:\n",
      "Fred, balking the ficked oo like reys it be?\n",
      "\n",
      "L\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000005:\tbatch: 3265/3947\tAverage_loss: 58.9696; Last batch loss: 54.7456\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "ITh CHRUH:\n",
      "I'm joonenth! You kid be Saze.  hat in  treped. Carears you sea7!  Light're preccly wast. THEGHATR:\n",
      "You canch ..\n",
      "\n",
      "DUMMCH:\n",
      "Grom.\n",
      "\n",
      "ALMER:\n",
      "Came on be merut fingsing in whas werl you ghe Can you. We're. I goip being the my that. Shat's on\n",
      "\n",
      "KARLY:\n",
      "Whick to wes the ..\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000006:\tbatch:  318/3947\tAverage_loss: 58.7356; Last batch loss: 50.6513\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "PENRISL FAA V Sto I thingpe?\n",
      "\n",
      "HERRIT:\n",
      "Buthiraned up obpoatina to sats satoce Grear.\n",
      "\n",
      "MRSFANLA:\n",
      "Fun.  our.\n",
      "\n",
      "MR. WHITO:\n",
      "Dud I sall shiflicoure indy that beuss of. wIths?\n",
      "\n",
      "CHARLIN:\n",
      "We'd no.\n",
      "\n",
      "STIIOSER I 'He puss I you toland ind in that hes to the sablam on ar. He's ant by nea\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000006:\tbatch: 1318/3947\tAverage_loss: 58.4711; Last batch loss: 40.4627\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DAs Neve the, you taid good that benatee. Gove lefure.\n",
      "\n",
      "1UFAND:\n",
      "It beare.\n",
      "\n",
      "LEESEN:\n",
      "Ither.\n",
      "\n",
      "DECKARD:\n",
      "I have is on freed untad expeying you the don that got fromy your you fony, ivessand up amblook I houd abant'raykald anyin rege prishern.\n",
      "\n",
      "EDDY:\n",
      "Busm you you you weal!\n",
      "\n",
      "IEL:\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000006:\tbatch: 2318/3947\tAverage_loss: 58.2379; Last batch loss: 43.9435\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "LEESTO:\n",
      "Dome.\n",
      "\n",
      "TERMER:\n",
      "What mether your don't relfe a strifind never word is a verce ass's makes in af you a? MAVE\n",
      "IVE:\n",
      "I flonghing if emfoppened?\n",
      "\n",
      "TORMLHY:\n",
      "Come here.  Tway to get fifuneghing aaps a Siding at's gotine werw them do Joser sormythappepers.  What's a wens pre\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000006:\tbatch: 3318/3947\tAverage_loss: 58.0513; Last batch loss: 55.8909\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "INTT:\n",
      "You bate cir was no. Foe uprestionly? That's coup oun. I drongs. But haven'll wasmis of hlartered the where with, mucptaldind if ing Sreysevoo. Roma right in hip?\n",
      "\n",
      "GONZO:\n",
      "My Glysts it's co pick tomes it they deady.\n",
      "\n",
      "GOCE:\n",
      "Alsiony shop. Loesno. inchiex the creck I me.\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000007:\tbatch:  371/3947\tAverage_loss: 57.8550; Last batch loss: 58.4933\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "ENIEN:\n",
      "I'm a yoh8uth.ung take they you lore migunoting to thit, ainling Dittiaar achater you's for you're they brron'd. You frog ue the go?\n",
      "\n",
      "JlESTY:\n",
      "Well. In our?\n",
      "\n",
      "MR. WHITE:\n",
      "Duter, bace ust she weal I drincen didn't going that pracreappan you'rg ak. Pranb, yiuh. Howlase l\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000007:\tbatch: 1371/3947\tAverage_loss: 57.6288; Last batch loss: 46.9914\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "REEN:\n",
      "I these go it me fick a lone?\n",
      "\n",
      "HAN:\n",
      "Someroblight ame trice prod you cidways hyLredans usting tias ut.  They'm think in thine.\n",
      "\n",
      "STANLA:\n",
      "Thanes the net goind evig...\n",
      "\n",
      "ELAY:\n",
      "Laingerenoun I undinising?\n",
      "\n",
      "YOLLATR:\n",
      "No you mesicelcor in seel.. you.\n",
      "\n",
      "TERMARED:\n",
      "That?\n",
      "\n",
      "RANDA:\n",
      "T\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000007:\tbatch: 2371/3947\tAverage_loss: 57.4370; Last batch loss: 49.9444\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "TARE:\n",
      "I mysthe the room and! Hef bead I sine lack frealy, anda.\n",
      "\n",
      "SIN:\n",
      "Yow, walk, The angernz.\n",
      "\n",
      "HAN:\n",
      "Nood take onatenthing Mappis.\n",
      "\n",
      "HAN:\n",
      "Beeption?\n",
      "\n",
      "DAIN:\n",
      "Ine bosea.\n",
      "\n",
      "MR.\n",
      "ATTE:\n",
      "You?\n",
      "\n",
      "DITE:\n",
      "Good, yevernth the like thing in wants unece us. Butele mogrteFhatpless nowly buckize,\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000007:\tbatch: 3371/3947\tAverage_loss: 57.2864; Last batch loss: 45.2499\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "EDRYS:\n",
      "What you satedyt, proors?\n",
      "\n",
      "LAURE:\n",
      "I upfougot hA VERIKER:\n",
      "Lealy to the scand if M right, Bard Frive Andon think holes out in what see hore is a have are hanter an a good draln the tide it Mad ae all incoml achack is sapin't a stubpase is is kill cUs we haves. The hel\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000008:\tbatch:  424/3947\tAverage_loss: 57.1170; Last batch loss: 44.0650\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "TERMIKE:\n",
      "Freare Yere.\n",
      "\n",
      "IGOR:\n",
      "You kand. You men letties about cragn.\n",
      "\n",
      "FREDDY:\n",
      "Yeagry to have his. What wencate.\n",
      "\n",
      "HENRY:\n",
      "You sackants it that's evet.\n",
      "\n",
      "MARAT:\n",
      "I'll verust them. . But they a getesink. You I was you all stiok magnance?\n",
      "\n",
      "BENNY:\n",
      "Ohand sablusponeyes ando.\n",
      "\n",
      "FREDDY:\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000008:\tbatch: 1424/3947\tAverage_loss: 56.9206; Last batch loss: 50.4480\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "PERN:\n",
      "You're a leam?\n",
      "\n",
      "IONT:\n",
      "Welps.\n",
      "\n",
      "MERAHERTHAT:\n",
      "Ol you're of sobothertait in in my.  The show, that core to held me ogco.  Ture stough if ane's.  Lame at if you heried your rpece food. I wradsil beath oben?\n",
      "\n",
      "THUMON:\n",
      "Sometht!\n",
      "\n",
      "LEIA:\n",
      "Jo about there.\n",
      "\n",
      "THTERA:\n",
      "Mast...\n",
      "\n",
      "TOLLIE\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000008:\tbatch: 2424/3947\tAverage_loss: 56.7583; Last batch loss: 58.7367\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "KARYO:\n",
      "So intto you're the goter, I were, lether. .\n",
      "\n",
      "LIE:\n",
      "Ol are deaso worddy that get the the shermindarrack.  Pera6 freek, youll wa?\n",
      "\n",
      "PUKPEWKIE:\n",
      "Net his, beatereay.\n",
      "\n",
      "HAROT: He're hapt.  neep ento the ges now.  recatebrifing.\n",
      "\n",
      "DUKE:\n",
      "Shough it's if foundn't gut ne.\n",
      "\n",
      "BURA:\n",
      "\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000008:\tbatch: 3424/3947\tAverage_loss: 56.6298; Last batch loss: 47.4121\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "KINKORY:\n",
      "Satsice.. Jactaight roontheace.\n",
      "\n",
      "GONZO:\n",
      "Yese, you got lild beyister. Sor lestack don't get move a cad.\n",
      "\n",
      "LEIA:\n",
      "You ksouve.\n",
      "\n",
      "VIk ate abonvel.\n",
      "\n",
      "ZOINE:\n",
      "Hocl.\n",
      "\n",
      "MESTY:\n",
      "Whyss rest He sop. Fay hippod incot you betut. I way.\n",
      "\n",
      "CHA:\n",
      "Jukensinioul.\n",
      "\n",
      "HETRIEWINDY:\n",
      "Would.\n",
      "\n",
      "MR. OR\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000009:\tbatch:  477/3947\tAverage_loss: 56.4858; Last batch loss: 41.1805\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "CHAR TIt Beller lors.\n",
      "\n",
      "BMIN I whele sond hunding ith me.. so whoy simerery will a toing sly thing.\n",
      "\n",
      "JIMET:\n",
      "I'm right he paying in the lissn.\n",
      "\n",
      "BETTY:\n",
      "I Dlerechin' bel.. just decus.\n",
      "\n",
      "HENRY:\n",
      "And goetn gurfrnethe offing have ar? Herase is had to do.\n",
      "\n",
      "JEFFREY:\n",
      "Halo him you're .\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000009:\tbatch: 1477/3947\tAverage_loss: 56.3151; Last batch loss: 49.4143\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "EDDY:\n",
      "You knew have fors.  meale, you cell that?\n",
      "\n",
      "SILBEN:\n",
      "Shere doine. asn.\n",
      "\n",
      "SARAH:\n",
      "I'lly.  This have vil 5ix I weal your a bllee at you?  What you. I must leated nitrar besile!\n",
      "\n",
      "ER SORSSHER:\n",
      "And bling yeard bert hundempotisee, It's fintthing. I'm to mece?\n",
      "\n",
      "BAG:\n",
      "That's fri\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000009:\tbatch: 2477/3947\tAverage_loss: 56.1743; Last batch loss: 59.4096\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "EOSE:\n",
      "It's notro things on, but cadrad of and am is I stutaly... you least forabnestian yOu wike, For a moveckill arecan atching lang.  It's there on that know rlying now tide you.\n",
      "\n",
      "HAN:\n",
      "They hajw we kerg mave, laningoing have a realy the besty the we.\n",
      "\n",
      "LUKE:\n",
      "Halm ampart.\n",
      "\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000009:\tbatch: 3477/3947\tAverage_loss: 56.0656; Last batch loss: 49.7791\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "ETELAY:\n",
      "Soodmalate has Limpiare twrown.\n",
      "\n",
      "GONZO:\n",
      "F. Lybody, note remus, I'm was toll off that's fo. Hinh up. That's will a any seypeder you're a sund ferten me me amer ither marn.\n",
      "\n",
      "SALLWIA.:\n",
      "But I himary os toplingol, andyUng it. Offol so  I dey'll to !\n",
      "\n",
      "RANDY:\n",
      "I his wants \n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000010:\tbatch:  530/3947\tAverage_loss: 55.9345; Last batch loss: 48.5792\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "INGE Fre anyowt, Pad, we get hel the chanklded.\n",
      "\n",
      "NIAN:\n",
      "Oh.\n",
      "\n",
      "JEY:\n",
      "So appluse. You've here bany he's now sty's lots fary am we handy.\n",
      "\n",
      "HAR:\n",
      "Aut thim your foh my fars like do get are the!t dust atolly the ploder have there donne ut?. Inw one there about we'lly, you to she?\n",
      "\n",
      "F\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000010:\tbatch: 1530/3947\tAverage_loss: 55.7863; Last batch loss: 52.4989\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "TERNIA:\n",
      "MIGLN:\n",
      "They're ractice. ...le.\n",
      "\n",
      "SONTE:\n",
      "I'm now what this frigitinkn to right thot it having the byer....  homight's the hono, you call aplight.\n",
      "\n",
      "INGOH:\n",
      "Amt gire there... but do chegt, Mr.  Wiss.\n",
      "\n",
      "MLTERTY:\n",
      "There.\n",
      "\n",
      "TOUGARETERAI MMMMA HN:\n",
      "There.\n",
      "\n",
      "MRUSPIPE:\n",
      "Lug tommbes\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000010:\tbatch: 2530/3947\tAverage_loss: 55.6657; Last batch loss: 56.9165\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "MAVERETT:\n",
      "I aamged. Bet in to was teel that sedreard all hard?\n",
      "\n",
      "SHER:\n",
      "Mrt you guy.\n",
      "\n",
      "JEENEY:\n",
      "Tere and ared you you out I beryserabpo.\n",
      "\n",
      "TRUMAN2:\n",
      "Paid andoth! Yeap om your shy conse.  Leech, Buch of nctooy s one ..\n",
      "\n",
      "LUKE:\n",
      "Yen yie, where yer, Jedorelly... 're, beter the ding e\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000010:\tbatch: 3530/3947\tAverage_loss: 55.5719; Last batch loss: 55.1927\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "EDELAIE:\n",
      "What's good wan, you, weat them pursurious sidect Pakild horr.\n",
      "\n",
      "RYAN:\n",
      "I never fach same... I'll gondf... gund us. It head recoll fucfivernething. Show stertho riduar, wactow! I'd is him... Blocaly in the my cautil my hough mindes, Jrine, dight, Will a geanes moodb\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000011:\tbatch:  583/3947\tAverage_loss: 55.4507; Last batch loss: 49.2094\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "OLLI.\n",
      "\n",
      "DUKE:\n",
      "We pout.\n",
      "\n",
      "DIE:\n",
      "Wan.\n",
      "\n",
      "DUKE:\n",
      "But you nipese wos up them, mesn't and this seliet!\n",
      "\n",
      "GOGEN:\n",
      "We know Me. I list?\n",
      "\n",
      "JEFER:\n",
      "Doody ah littant!\n",
      "\n",
      "SANGDITE:\n",
      "Dostons, rigrt honoms enther that gove cee, you're a puckind hear so show, to forw know of befuctont?\n",
      "\n",
      "MAD! NGA:\n",
      "You\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000011:\tbatch: 1583/3947\tAverage_loss: 55.3213; Last batch loss: 56.9316\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "ADDA:  I want it?\n",
      "\n",
      "MARMON:\n",
      "Welad just what we ut sticker you for  You?  We panis.\n",
      "\n",
      "ROTET:\n",
      "That's scaulih, De ah?\n",
      "\n",
      "MERYNGA:\n",
      "You stuce  ountys him your am bubll dadder Whick.  Khat exthesica off.\n",
      "\n",
      "LECAARE:\n",
      "Aed drook one saar usting.  see nothing. Whatel?\n",
      "\n",
      "BATY:\n",
      "But you never\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000011:\tbatch: 2583/3947\tAverage_loss: 55.2133; Last batch loss: 43.0356\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "GOFFEENGA:\n",
      "Are Alling. Bom aid us too.\n",
      "\n",
      "DEOSA:\n",
      "IS FAUDDDETHOR:\n",
      "You've grt the ares desst ho doing. ...\n",
      "\n",
      "CINPEP:\n",
      "The know the they say. Sone.  The cattulious kitt do it fany liker.\n",
      "\n",
      "BAN:\n",
      "Get was foust're.\n",
      "\n",
      "DELMGORAH:\n",
      "Leaky lace notsisbo dimet?\n",
      "\n",
      "LANRON:\n",
      "You're trat daytor, T\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000011:\tbatch: 3583/3947\tAverage_loss: 55.1304; Last batch loss: 54.4239\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "SITECIINELRY:\n",
      "What Mante, I miss in pouging of the was we had sight... Drysernt's cfoved you're like you? Thatgun the where... I'd the wast try... I den't dinglid weare to sur and like was erfur?\n",
      "\n",
      "CHERTIL:\n",
      "Jaw? Doobadin' it rixpaliact pop...... I know what mest have in you\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000012:\tbatch:  636/3947\tAverage_loss: 55.0203; Last batch loss: 36.5889\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "ICEL INSIPEOBLIOS MEJ? \n",
      "JEd man. Yes ass.  Paup. shit's mirlys taling to coust of verycam ?\n",
      "\n",
      "ARACKIUG COR I Scouk.\n",
      "\n",
      "IGAR:\n",
      "In's parss hthed it sectit.  I wanthned they do rece for who'dy?\n",
      "\n",
      "TERDYof. Lokiseard dian a canyis un Distory tragst.\n",
      "\n",
      "CINSTY:\n",
      "That if tome you sid s c\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000012:\tbatch: 1636/3947\tAverage_loss: 54.9098; Last batch loss: 50.2631\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DUKE:\n",
      "I litt comaling in rele, Do your buck whade Comesta make of Don you'd shita were in to was, but and going, nixplebaint it.\n",
      "\n",
      "BETTY:\n",
      "I happy.\n",
      "\n",
      "IGOR:\n",
      "I snot behten hunttins this you was to it it was cae neal.  The wrod. I'm neag!\n",
      "\n",
      "TERMICHJ:\n",
      "Scarwers, I don't onding remu\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000012:\tbatch: 2636/3947\tAverage_loss: 54.8088; Last batch loss: 60.9701\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "MAVEREY:\n",
      "THEm bronffetn suyst's it hell Pag to tide!\n",
      "\n",
      "PICO:\n",
      "He's onfunce dowerd!\n",
      "\n",
      "SARAHT:\n",
      "Therel  It'n'ALTHAIANE:\n",
      "I with a muyy going! I would paysee that's up fowd. How you're what staigh!\n",
      "\n",
      "BRESE WEDDY:\n",
      "Yis.\n",
      "\n",
      "MR. ADDE:\n",
      "I those andy, we trigh.? exto. Seip you woh.\n",
      "\n",
      "LUEBER:\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000012:\tbatch: 3636/3947\tAverage_loss: 54.7352; Last batch loss: 52.3388\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "EDDY:\n",
      "Vod happrewn joon...\n",
      "\n",
      "MI. Hom laten is takery saxplaine. Exwemther carnale. what?\n",
      "\n",
      "SANE:\n",
      "So ald thinn.  Comy in we onetthe, me. The want the is look, sively mearerst, There's where, beat you. Fun thitks.  I know there mocar. They're got. One haver'achse siwn of fuck \n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000013:\tbatch:  689/3947\tAverage_loss: 54.6343; Last batch loss: 53.4749\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "ARAN:\n",
      "Every?\n",
      "\n",
      "IGOR:\n",
      "Me tell but did you don't dene bestio thit's tadcer rece? I wough of the mistment goess moshot wasch this must sise you raincrody, but you will.  Freed to who dide lick shorse the killy's thatmerint goet farraSt fucking?\n",
      "\n",
      "SAUD ER.\n",
      "\n",
      "DESMOLD:\n",
      "They is am m\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000013:\tbatch: 1689/3947\tAverage_loss: 54.5358; Last batch loss: 53.1677\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "EOMEN:\n",
      "Tered it for roward bole cace for the froy.  Just madting whike she I me looks right Jived.  They'm heso in thes do alking do surfued aro is I soblile the the is somethy Night bring this fruck, fake... No.\n",
      "\n",
      "MARONGE:\n",
      "I've goe peifer fught he said, anug, lelt, a Yow h\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000013:\tbatch: 2689/3947\tAverage_loss: 54.4452; Last batch loss: 66.2860\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "SINDER:\n",
      "Oh. brot.\n",
      "\n",
      "SENGO:\n",
      "It's no pamatbody soppinctle bady from Tele... not Cormaned you thought.  countergidntatian.  St tround you astue the him herver was for this toing and Hell of the the live.\n",
      "\n",
      "DRYT Pamm you think I down. Howla the herybody! ACs tide am doget of is \n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000013:\tbatch: 3689/3947\tAverage_loss: 54.3768; Last batch loss: 52.1704\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "GOFFEHRIE:\n",
      "Then you Day, plotsate.  Then I me. I'm oun ach no Mould disscaibsiont, some the?\n",
      "\n",
      "DR. IGERICE:\n",
      "You fait Than. She mowt helry, strinker much ree he don't pack, I know.\n",
      "\n",
      "DA WBET EROSA:\n",
      "Jwents un think your zort and jusid, whition?\n",
      "\n",
      "HAN:\n",
      "Oh stabraven your to they'\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000014:\tbatch:  742/3947\tAverage_loss: 54.2856; Last batch loss: 48.8503\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DUKE:\n",
      "Fistel.\n",
      "\n",
      "JED:\n",
      "Chere never up, sterrudis what inT a yosticques.\n",
      "\n",
      "DDA FAR.\n",
      "GHAT HEEM... YOPAND:\n",
      "You're sops. We's leresting gomind with all ruseled toll. Holver that lah your from ut you, yourceblly?\n",
      "\n",
      "SANDY:\n",
      "Souod betall hall you. Wants goud fuf.\n",
      "\n",
      "DAK:\n",
      "Walk nucd the co\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000014:\tbatch: 1742/3947\tAverage_loss: 54.1965; Last batch loss: 49.4476\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "EPRIghipfrot I've out, your minda winde's himit. De. And it, Morter to filitanse you is lower, Came to know stankings the con andent gotay ccomed in to atally.\n",
      "\n",
      "BIGLO:\n",
      "Yeah, whive reh?\n",
      "\n",
      "BETTY:\n",
      "Alsetmet!\n",
      "\n",
      "SICTOR:\n",
      "You hare, the byorer tere......or dene make a witler, no troc\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000014:\tbatch: 2742/3947\tAverage_loss: 54.1153; Last batch loss: 50.7689\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DETER:\n",
      "I'n ECHERY:\n",
      "Fus of a oun on stick.therstindy.\n",
      "\n",
      "LONWIG:\n",
      "I you have to fuctine your the a sackingend ... it.\n",
      "\n",
      "CAROSE:\n",
      "Can yeass arire llarp... MU Brong are your Donote it not fo yean to wass save to will of to this eas.\n",
      "\n",
      "FRINDY:\n",
      "Reslit.?!  I don't hey, of you was come\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000014:\tbatch: 3742/3947\tAverage_loss: 54.0526; Last batch loss: 53.3844\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "MIMENTE'SO:\n",
      "Not.\n",
      "\n",
      "MWK:\n",
      "Tiedent to wasting I know... you.\n",
      "\n",
      "ELAITE:\n",
      "What're conesn't worl tcarts.\n",
      "\n",
      "ERMy hoM, you beamine. I'm a gonna barkenthing out. Freap, doond of the make doch.\n",
      "\n",
      "ETEMON:\n",
      "As we cangeet just head.\n",
      "\n",
      "GOAN:\n",
      "Yesr.\n",
      "\n",
      "PMIE:\n",
      "On of it. I'l' BYSGUVERAY:\n",
      "Gold it. We'\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000015:\tbatch:  795/3947\tAverage_loss: 53.9692; Last batch loss: 48.1055\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "ELD:\n",
      "Neg you've think that make plunce get takenture is have on hurdol?\n",
      "\n",
      "DYSE.\n",
      "\n",
      "CHRINGONSTON:\n",
      "Then you dort pres sccoure.\n",
      "\n",
      "IST FAMGY:\n",
      "I got for sighy while that'ss'st siber poy paces?\n",
      "\n",
      "RACd SOPTO hord tome oney now ah! Look and Siman.\n",
      "\n",
      "ELPINeChems just got the was is it. S\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000015:\tbatch: 1795/3947\tAverage_loss: 53.8865; Last batch loss: 42.2309\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "MATTEm MA.\n",
      "\n",
      "TBUNGER:\n",
      "You've man's trat is so Dibomy. I'w I have the be hiDt unded.\n",
      "\n",
      "JOFF:\n",
      "Moring get it ofway.\n",
      "\n",
      "TRY:\n",
      "Mad.. Yout to do only on you hear.\n",
      "\n",
      "JECSISS:\n",
      "Jady lixe of me is not! Alase atces.\n",
      "\n",
      "JEETLA OKATH:\n",
      "What... kill aget who be know?\n",
      "\n",
      "IDATDY:\n",
      "Ansthore, gue?\n",
      "\n",
      "JOH\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000015:\tbatch: 2795/3947\tAverage_loss: 53.8112; Last batch loss: 51.8486\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "FES:\n",
      "I's. Ruckingeney, Vasts to sh.awid of they befuf...or \n",
      "KOMFFFE:\n",
      "As a stops's that's to finiiseld trice.\n",
      "\n",
      "CHABENS:\n",
      "Maveral. We don't know what wes forove was out.\n",
      "\n",
      "CINK:\n",
      "Partneate, goym could stree mmenseal. 're!\n",
      "\n",
      "SARAN:\n",
      "Oh!\n",
      "\n",
      "TYEXCOO:\n",
      "Throm. But the haure Dellablenory.\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000015:\tbatch: 3795/3947\tAverage_loss: 53.7555; Last batch loss: 50.0737\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "SAR IU V incumingulin't day us thath up is on the mesed yire papbed Wedry?\n",
      "\n",
      "JEFFREY:\n",
      "Where for you, drame you! I's some padies for helpase it.\n",
      "\n",
      "MR. WHITE:\n",
      "Shit wry, I look stoctitatoin' say on I've hear a ciss mices I know Jofubpievina I chere. Good any picked why sometoe \n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000016:\tbatch:  848/3947\tAverage_loss: 53.6753; Last batch loss: 50.7915\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "LECHA:\n",
      "Whatn'll.\n",
      "\n",
      "BEN:\n",
      "Diod to down?.. I liccolm, you Jederds.\n",
      "\n",
      "FREDDY:\n",
      "Shere...\n",
      "\n",
      "STA:\n",
      "ICA The wastly deal in.\n",
      "\n",
      "HENRY:\n",
      "That's this and It's I liserdy bicking, got u\n",
      "\n",
      "ELANDER:\n",
      "What was there's in you know me's you'n' LIA ESSG ENGANK:\n",
      "The ach?\n",
      "\n",
      "DECMAN:\n",
      "We golbre ment werl. m\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000016:\tbatch: 1848/3947\tAverage_loss: 53.6003; Last batch loss: 42.5370\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "TARO:\n",
      "Madelrat you Ciller a meed Sarverning.\n",
      "\n",
      "MRUGER:\n",
      "Who...\n",
      "\n",
      "TRUKARETON:\n",
      "Hoo scare, anyo.\n",
      "\n",
      "BEWE:\n",
      "I'll want of to did Sop afresn't to veriest whited for had the Sreer?\n",
      "\n",
      "MERICA:\n",
      "Hello.  Where on mishen.\n",
      "\n",
      "SERAH:\n",
      "Sord herry. Secr the allo, me?\n",
      "\n",
      "LUKE:\n",
      "I don't coutt oCt foitlin\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000016:\tbatch: 2848/3947\tAverage_loss: 53.5327; Last batch loss: 44.3704\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "REESY:\n",
      "I'm kit of a natrip son get you facn me the cather.  I fart up this been thing xopelued it they alroution the gustle to to deed . !\n",
      "\n",
      "BIGHTON:\n",
      "Why gonnay.\n",
      "\n",
      "PENIE:\n",
      "Yey's to get?\n",
      "\n",
      "DONNOE':\n",
      "Thit, had ie, you hat Prees, it's a fleah your becprather in some that are namie\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000016:\tbatch: 3848/3947\tAverage_loss: 53.4813; Last batch loss: 42.7270\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "ALLIACIMONIT:\n",
      "Yem eastted the saks for, juctiter the mese ssown arouguambem cowfucty. You know Detn't hure magient suctteld of in he must now chome.\n",
      "\n",
      "SANDY:\n",
      "Thinking ot so asts! I Amssion thinK.  You're the out him.\n",
      "\n",
      "ZOOR:\n",
      "M3CKETICE\n",
      "TORO:\n",
      "Sew approving pooling a lived the \n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000017:\tbatch:  901/3947\tAverage_loss: 53.4043; Last batch loss: 43.3624\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "EDDY:\n",
      "Fuckict. Hucre very fired tire, a cutt, theaki.\n",
      "\n",
      "ROKE:\n",
      "Whited now what fet the sieppore wither of sen I got are to ever. I cab.\n",
      "\n",
      "HEN:\n",
      "Bace outs the going.\n",
      "\n",
      "TED:\n",
      "What itror right. Fordove.\n",
      "\n",
      "WIGAR:\n",
      "OF!\n",
      "\n",
      "DESMAN:\n",
      "Rever you're a life. Then antt acr we're a preccant.\n",
      "\n",
      "MARE\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000017:\tbatch: 1901/3947\tAverage_loss: 53.3372; Last batch loss: 40.9610\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "ECTERTE:\n",
      "Wele do I hinconcay is fig.\n",
      "\n",
      "TROMY:\n",
      "Shere what all irl.  We called to ankill yela in do?\n",
      "\n",
      "HANSSO:\n",
      "Yes. Gray to decal anyher. Tell a cated you.\n",
      "\n",
      "LAIT:\n",
      "I mets?\n",
      "\n",
      "DOMMY:\n",
      "What's happrom!\n",
      "\n",
      "TRUMAN:\n",
      "Eeame.\n",
      "\n",
      "SONGOO:\n",
      "Wike they befusto but oking inthe to guesn't iS.  You can\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000017:\tbatch: 2901/3947\tAverage_loss: 53.2736; Last batch loss: 56.4182\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DESMOND...MMMMMCT:\n",
      "Comp?\n",
      "\n",
      "VADER:\n",
      "Hall wolld I col your cantusa ghing.  A funk assed.\n",
      "\n",
      "DONNIE:\n",
      "What whougurther trenta say than what sto aid my bould he pay. Yeahcath! We tell you you're away!\n",
      "\n",
      "CHEETSY:\n",
      "I'm the steml iH musp.\n",
      "\n",
      "STEVEIEN:\n",
      "Niok all they even something to crun'\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000017:\tbatch: 3901/3947\tAverage_loss: 53.2267; Last batch loss: 50.8792\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "JEFFREY:\n",
      "What's all lever.\n",
      "\n",
      "MAVERICK:\n",
      "Ully and yon that have you bave here drod about not intuse work. You well titn the was the shy as us about!\n",
      "\n",
      "DUKO:\n",
      "Sued over the clocn, No The figlor Ewses meand?\n",
      "\n",
      "STWIELECKA, MANDE:\n",
      "Now. That's aGd of in from yout finas. Nevering no. \n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000018:\tbatch:  954/3947\tAverage_loss: 53.1531; Last batch loss: 49.9878\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "ADDY:\n",
      "Not goteadsn'ted, it him.\n",
      "\n",
      "VIDAY:\n",
      "Eoo.\n",
      "\n",
      "FREND:\n",
      " Stit.\n",
      "\n",
      "JOFFFEED:\n",
      "And the with me. Nexkanto.\n",
      "\n",
      "DAMON:\n",
      "I don't know can to foor muss outly Frankiagured, Bull do thing.\n",
      "\n",
      "LEIA:\n",
      "Yes, yex not I me. Telace you get a don't make in contefile your. But Bloteating to faire.. out\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000018:\tbatch: 1954/3947\tAverage_loss: 53.0915; Last batch loss: 50.6627\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "SERACE:\n",
      "HNo knece you could need...\n",
      "\n",
      "HERMIO:\n",
      "You.\n",
      "\n",
      "BETTOR:\n",
      "Shoura sid your them that quere deatene.\n",
      "\n",
      "RAY:\n",
      "So all butom shior the look you can't do then that to side.\n",
      "\n",
      "RIGORLENTOR:\n",
      "What anatamenta havioges file, Bags to be sure pops. And in me.\n",
      "\n",
      "CORNELIU?\n",
      "\n",
      "COOPT:\n",
      "The sotess\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000018:\tbatch: 2954/3947\tAverage_loss: 53.0343; Last batch loss: 46.7507\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "EOGOTR DOOL GOA'D UCOLLYAUD:\n",
      "Tight I'm no I get a gudeace.  What shing, Elles id, we dene call my abater.\n",
      "\n",
      "MR. ACH0ELLY:\n",
      "What's a Gunhter?\n",
      "\n",
      "RUKE:\n",
      "Nuckin undernam. We'll defr fance you, Dudser unda bevace.  Gom und, Pompor it, you late of in over somedonchind who hid toory \n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000019:\tbatch:    7/3947\tAverage_loss: 52.9898; Last batch loss: 49.9300\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "IVEVEY:\n",
      "Yeah freet time fated...\n",
      "\n",
      "INGUS:\n",
      "Ho the promes.\n",
      "\n",
      "VURA:\n",
      "Yeah, Ht's rarlow.\n",
      "\n",
      "VAR:\n",
      "Well, your to hewn't's cot can any's say whento gut blacty Crisep. You've booss, gomar. You, and what prange your give something.\n",
      "\n",
      "FRAN:\n",
      "Jesel, the or nothing.\n",
      "\n",
      "VADSRI FURMAN:\n",
      "Ohay.\n",
      "\n",
      "PE\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000019:\tbatch: 1007/3947\tAverage_loss: 52.9196; Last batch loss: 44.1523\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "MASERMANDY:\n",
      "It the wart for to if it?\n",
      "\n",
      "DECKARD:\n",
      "Lain.\n",
      "\n",
      "BACOSCHEN:\n",
      "Yoursyon, one was veep it You were they letlase to remon.\n",
      "\n",
      "FREDDY:\n",
      "Broorty do, I stmy to to camices tom, herm'o and lonl enture tow you wever this alurission. bCy if ghi Chtuckes forele?\n",
      "\n",
      "EREETHY:\n",
      "There him \n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000019:\tbatch: 2007/3947\tAverage_loss: 52.8627; Last batch loss: 49.3134\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "BELNZA:\n",
      "I inyontarner, and eass it Cinkars reclol like up.\n",
      "\n",
      "HERO:\n",
      "I live 49,!\n",
      "\n",
      "VARES:\n",
      "Oh munboind about of the we lif!  IS MANATO:\n",
      "That's you... guind.  buefore going the see shture. Sinly .\n",
      "\n",
      "BOLTON:\n",
      "Cambern't me big.\n",
      "\n",
      "DONNIE:\n",
      "I deauce themust ot fath you'd..... just was e\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000019:\tbatch: 3007/3947\tAverage_loss: 52.8102; Last batch loss: 49.2425\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DOMCOOVE:\n",
      "It's a moled fahat a gon't go.  What for all to morrany  held.\n",
      "\n",
      "GONZO:\n",
      "If you want rly to were goin\n",
      "\n",
      "LELEER:\n",
      "Whated on me teere all the fing, oon, the must for, ever to a Bae!\n",
      "\n",
      "ADAMAOTEU INGU...0ACHNANDINE WOOHE? HAN:\n",
      "THout Cation.\n",
      "\n",
      "MR. WHITERE CHROFR. AR POWPITE\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000020:\tbatch:   60/3947\tAverage_loss: 52.7679; Last batch loss: 55.8899\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "KOROTHY:\n",
      "It's freach minte.\n",
      "\n",
      "MR. BLICO:\n",
      "What fib your you time?\n",
      "\n",
      "ELAVED RBUND:\n",
      "I retued him, and Rcire?\n",
      "\n",
      "JEFFREY:\n",
      "White know just this pry the ead. No, clead soed sayones.\n",
      "\n",
      "KERICES:\n",
      "You just sogn, to a got ham  Leked so thing strown off dandy. I fiknel. Twassed anything??\n",
      "\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000020:\tbatch: 1060/3947\tAverage_loss: 52.7005; Last batch loss: 45.6159\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "EDDIE:\n",
      "Whly haver.\n",
      "\n",
      "SLOFREDDA:\n",
      "Are meth.\n",
      "\n",
      "YODY:\n",
      "How not to for am and but we, The stat amn'tre aflertamn w14EN 2OOL COknown?\n",
      "\n",
      "REEF:\n",
      "Buck folly.\n",
      "\n",
      "LUKE:\n",
      "Hillor.\n",
      "\n",
      "LEIA:\n",
      "They tane, Miback?\n",
      "\n",
      "DELMAR:\n",
      "Nopody having bolk dip an.\n",
      "\n",
      "TED:\n",
      "Tome broustergele blourics.  I face.\n",
      "\n",
      "CORNZO:\n",
      "\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000020:\tbatch: 2060/3947\tAverage_loss: 52.6485; Last batch loss: 50.3361\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "RAMO:\n",
      "I was hilh whInf pook...chine noting fur freed.\n",
      "\n",
      "SARAH:\n",
      "Fure doy sixat was wasthere hey mend?\n",
      "\n",
      "LAURA:\n",
      "Cormben thyo chaun.\n",
      "\n",
      "LEY:\n",
      "Everybody. I' LOLA MMMMY:\n",
      "Eorot do god degghto it ome. Hatt ace is we carr, walk to be pim getsand on you way, but IFres thelpufted thout a\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000020:\tbatch: 3060/3947\tAverage_loss: 52.5995; Last batch loss: 40.2173\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "MINSTORE IOG CERISWO:\n",
      "When I'll just bect that you his. The claVy no over quir what it lo, strid!\n",
      "\n",
      "ELAY:\n",
      "Donne to just this doect it's up they're figl crapaic, rabody gonna besen you'llome be entucful yourayme strong it, you know have, I'm trake?\n",
      "\n",
      "JEFFREY:\n",
      "I'm onfule  or a\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000021:\tbatch:  113/3947\tAverage_loss: 52.5594; Last batch loss: 47.9347\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "SINDY:\n",
      "My was they're onajied rimed your the about beal ferfut upablenternould?\n",
      "\n",
      "HOTHER:\n",
      "Dalkem, eas to as it.\n",
      "\n",
      "BIGMAR:\n",
      "Thing interes. Now to neTer thy ship now this anyordy....ted with wat's I'll to see you.\n",
      "\n",
      "JEFFRAY:\n",
      "Lake theys foned. Find ever?\n",
      "\n",
      "ELAINE:\n",
      "How amy to fight\n",
      "\n",
      "> Dumped to: ./model\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 000021:\tbatch: 1113/3947\tAverage_loss: 52.4951; Last batch loss: 48.8798\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-4c900fe7a241>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dataset/selected_conversations.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_minibatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_language_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-44-a58ec07378f7>\u001b[0m in \u001b[0;36mrun_language_model\u001b[0;34m(dataset, max_epochs, hidden_size, sequence_length, learning_rate, sample_every, dump_path)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_oh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_oh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mcum_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-c06dc588da4f>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, h, x, y)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_loss_and_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mdU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-c06dc588da4f>\u001b[0m in \u001b[0;36moutput_loss_and_grads\u001b[0;34m(self, h, V, c, y)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0myp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-c06dc588da4f>\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = Dataset(batch_size=5, sequence_length=30)\n",
    "dataset.preprocess(\"dataset/selected_conversations.txt\")\n",
    "dataset.create_minibatches()\n",
    "rnn = run_language_model(dataset, 100000, sequence_length=dataset.sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loaded rnn from  ./best_model\n",
      "> Sampled from rnn:\n",
      "HAN:\n",
      "42?\n",
      "\n",
      "PIGOISE:\n",
      ".. ach there.\n",
      "\n",
      "MAVERICK:\n",
      "We hear we'light, but there?\n",
      "\n",
      "CHARLIE:\n",
      "Forgery gont tell him?\n",
      "\n",
      "HENRY:\n",
      "You you up hover!  Dutazy, a miecce with as.\n",
      "\n",
      "EURA:\n",
      "Doentar you, fath do do papping, I becan' I tide to you feep you long and you doos for pist it. I get it?\n",
      "\n",
      "MAVERIKER:\n",
      "Tway to do it Go\n"
     ]
    }
   ],
   "source": [
    "path = './best_model'\n",
    "\n",
    "\n",
    "with open(path, 'rb') as f:\n",
    "    rnn = pickle.load(f)\n",
    "print(\"> Loaded rnn from \", path)\n",
    "\n",
    "\n",
    "seed = \"HAN:\\n42?\\n\\n\"\n",
    "n_sample = 300\n",
    "sampled = sample(rnn, seed, n_sample, dataset)\n",
    "print('> Sampled from rnn:')\n",
    "print(''.join(sampled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from ./model\n",
      "Dumped to ./best_model\n"
     ]
    }
   ],
   "source": [
    "# load last model\n",
    "with open('./model', 'rb') as f:\n",
    "    rnn = pickle.load(f)\n",
    "print('Loaded from ./model')\n",
    "\n",
    "# dump to seperate location\n",
    "with open(\"best_model\", \"wb\") as f:\n",
    "    pickle.dump(rnn, f)\n",
    "print('Dumped to ./best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
