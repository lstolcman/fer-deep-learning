{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import skimage as ski\n",
    "import skimage.io\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Util methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_dir(d):\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)\n",
    "        \n",
    "def save_image(img, path, mean, std):\n",
    "  img = img.copy()\n",
    "  img *= std\n",
    "  img += mean\n",
    "  img = img.astype(np.uint8)\n",
    "  ski.io.imsave(path, img)\n",
    "\n",
    "def show_image(img, mean, std):\n",
    "  img = img.copy()\n",
    "  img *= std\n",
    "  img += mean\n",
    "  img = img.astype(np.uint8)\n",
    "  ski.io.imshow(img)\n",
    "  ski.io.show()\n",
    "\n",
    "def shuffle_data(data_x, data_y):\n",
    "  indices = np.arange(data_x.shape[0])\n",
    "  np.random.shuffle(indices)\n",
    "  shuffled_data_x = np.ascontiguousarray(data_x[indices])\n",
    "  shuffled_data_y = np.ascontiguousarray(data_y[indices])\n",
    "  return shuffled_data_x, shuffled_data_y\n",
    "\n",
    "def unpickle(file):\n",
    "    import cPickle\n",
    "    fo = open(file, 'rb')\n",
    "    dict = cPickle.load(fo)\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/mratkovic/deeplearning/datasets/cifar-10-batches-py/'\n",
    "SAVE_DIR = '/home/mratkovic/deeplearning/outputs/cifar10/'\n",
    "init_dir(SAVE_DIR)\n",
    "\n",
    "config = {}\n",
    "config['max_epochs'] = 40\n",
    "config['batch_size'] = 50\n",
    "config['save_dir'] = SAVE_DIR\n",
    "config['weight_decay'] = 1e-4\n",
    "lr_initial = 0.01\n",
    "config['lr_policy'] = {e:{'lr':(0.9**e)*lr_initial} for e in range(1, config['max_epochs']+1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "img_height, img_width, num_channels = 32, 32, 3\n",
    "train_x = np.ndarray((0, img_height * img_width * num_channels), dtype=np.float32)\n",
    "train_y = []\n",
    "for i in range(1, 6):\n",
    "  subset = unpickle(os.path.join(DATA_DIR, 'data_batch_%d' % i))\n",
    "  train_x = np.vstack((train_x, subset['data']))\n",
    "  train_y += subset['labels']\n",
    "train_x = train_x.reshape((-1, num_channels, img_height, img_width)).transpose(0,2,3,1)\n",
    "train_y = np.array(train_y, dtype=np.int32)\n",
    "\n",
    "subset = unpickle(os.path.join(DATA_DIR, 'test_batch'))\n",
    "test_x = subset['data'].reshape((-1, num_channels, img_height, img_width)).transpose(0,2,3,1).astype(np.float32)\n",
    "test_y = np.array(subset['labels'], dtype=np.int32)\n",
    "\n",
    "valid_size = 5000\n",
    "train_x, train_y = shuffle_data(train_x, train_y)\n",
    "valid_x = train_x[:valid_size, ...]\n",
    "valid_y = train_y[:valid_size, ...]\n",
    "train_x = train_x[valid_size:, ...]\n",
    "train_y = train_y[valid_size:, ...]\n",
    "data_mean = train_x.mean((0,1,2))\n",
    "data_std = train_x.std((0,1,2))\n",
    "\n",
    "train_x = (train_x - data_mean) / data_std\n",
    "valid_x = (valid_x - data_mean) / data_std\n",
    "test_x = (test_x - data_mean) / data_std\n",
    "print(train_x.shape)\n",
    "\n",
    "weight_decay = config['weight_decay']\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Util methods for tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l2_loss(weights):\n",
    "    regularizers = 0;\n",
    "    for w in weights:\n",
    "        regularizers += tf.nn.l2_loss(w)\n",
    "    return regularizers\n",
    "        \n",
    "\n",
    "def conv2d(x, W, b, activation=tf.nn.relu, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return activation(x)\n",
    "\n",
    "def maxpool2d(x, k=2, stride=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, stride, stride, 1], padding='SAME')\n",
    "\n",
    "def fc(x, W, b, activation=None):\n",
    "    x = tf.reshape(x, [-1, W.get_shape().as_list()[0]])\n",
    "    if activation :\n",
    "        return activation(tf.matmul(x, W) +  b)    \n",
    "    return tf.matmul(x, W) +  b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import xavier_initializer_conv2d as xavier_conv2d\n",
    "from tensorflow.contrib.layers import xavier_initializer as xavier\n",
    "tf.reset_default_graph()\n",
    "\n",
    "weights = {\n",
    "    'conv1': tf.get_variable('w_conv1', [5, 5, 3, 16], initializer=xavier_conv2d()),\n",
    "    'conv2': tf.get_variable('w_conv2', [5, 5, 16, 32], initializer=xavier_conv2d()),\n",
    "    \n",
    "    'fc3': tf.get_variable('w_fc3', [8*8*32, 256], initializer=xavier()),\n",
    "    'fc4': tf.get_variable('w_fc4', [256, 128], initializer=xavier()),\n",
    "    'fc5': tf.get_variable('w_fc5', [128, n_classes], initializer=xavier())\n",
    "    \n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'conv1': tf.Variable(tf.zeros([16]), name='b_conv1'),\n",
    "    'conv2': tf.Variable(tf.zeros([32]), name='b_conv2'),\n",
    "    'fc3': tf.Variable(tf.zeros([256]), name='b_fc3'),\n",
    "    'fc4': tf.Variable(tf.zeros([128]), name='b_fc4'),\n",
    "    'fc5': tf.Variable(tf.zeros([n_classes]), name='b_fc5')\n",
    "}\n",
    "\n",
    "#conv(16,5) -> pool(3,2) -> conv(32,5) -> pool(3,2) -> fc(256) -> fc(128) -> fc(10)\n",
    "def convnet(x, weights, biases):\n",
    "    x = tf.reshape(x, shape=[-1, img_height, img_width, num_channels])\n",
    "    net = conv2d(x, weights['conv1'], biases['conv1'], tf.nn.relu)\n",
    "    net = maxpool2d(net, k=3, stride=2)\n",
    "    \n",
    "    net = conv2d(net, weights['conv2'], biases['conv2'], tf.nn.relu)\n",
    "    net = maxpool2d(net, k=3, stride=2)\n",
    "    \n",
    "    net = fc(net, weights['fc3'],  biases['fc3'], tf.nn.relu)\n",
    "    net = fc(net, weights['fc4'],  biases['fc4'], tf.nn.relu)\n",
    "    net = fc(net, weights['fc5'],  biases['fc5'])\n",
    "    return net\n",
    "\n",
    "\n",
    "# Graph\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "Y_ = tf.placeholder(tf.int32, [None,])\n",
    "logits = convnet(X, weights, biases)\n",
    "\n",
    "# loss\n",
    "regularizers = l2_loss(weights.values())\n",
    "data_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, Y_))\n",
    "loss = data_loss + weight_decay*regularizers\n",
    "\n",
    "lr = tf.placeholder(tf.float32)\n",
    "train_step =  tf.train.GradientDescentOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(session, train_x, train_y, valid_x, valid_y, config):\n",
    "    plot_data = {}\n",
    "    plot_data['train_loss'] = []\n",
    "    plot_data['valid_loss'] = []\n",
    "    plot_data['train_acc'] = []\n",
    "    plot_data['valid_acc'] = []\n",
    "    plot_data['lr'] = []\n",
    "    \n",
    "    session.run(tf.initialize_all_variables())\n",
    "    \n",
    "    lr_policy = config['lr_policy']\n",
    "    batch_size = config['batch_size']\n",
    "    max_epochs = config['max_epochs']\n",
    "    save_dir = config['save_dir']\n",
    "    num_examples = train_x.shape[0]\n",
    "    assert num_examples % batch_size == 0\n",
    "    num_batches = num_examples // batch_size\n",
    "    \n",
    "    for epoch_num in range(1, max_epochs + 1):\n",
    "      epoch_start = time.time()\n",
    "      train_x, train_y = shuffle_data(train_x, train_y)\n",
    "      if epoch_num in lr_policy:\n",
    "          solver_config = lr_policy[epoch_num]\n",
    "    \n",
    "      for step in range(num_batches):\n",
    "        offset = step * batch_size \n",
    "        \n",
    "        batch_x = train_x[offset:(offset + batch_size), ...]\n",
    "        batch_y = train_y[offset:(offset + batch_size), ...]\n",
    "        \n",
    "        feed_dict = {X: batch_x, Y_: batch_y, lr:solver_config['lr']}\n",
    "        start_time = time.time()\n",
    "        ret_val = session.run([train_step, loss, logits], feed_dict=feed_dict)\n",
    "        _, loss_val, logits_val = ret_val\n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        if (step+1) % 50 == 0:\n",
    "          sec_per_batch = float(duration)\n",
    "          format_str = 'epoch %d, step %d / %d, loss = %.2f (%.3f sec/batch)'\n",
    "          print(format_str % (epoch_num, step+1, num_batches, loss_val, sec_per_batch))\n",
    "        \n",
    "        if (step+1) % 100 == 0:\n",
    "            w = session.run(weights['conv1'])\n",
    "            draw_conv_filters(epoch_num, step+1, w, save_dir)\n",
    "\n",
    "      print('Train error:')\n",
    "      train_loss, train_acc = evaluate(session, train_x, train_y, config)\n",
    "      print('Validation error:')\n",
    "      valid_loss, valid_acc = evaluate(session, valid_x, valid_y, config)\n",
    "      print('Epoch time:', time.time() - epoch_start)\n",
    "      plot_data['train_loss'] += [train_loss]\n",
    "      plot_data['valid_loss'] += [valid_loss]\n",
    "      plot_data['train_acc'] += [train_acc]\n",
    "      plot_data['valid_acc'] += [valid_acc]\n",
    "      plot_data['lr'] += [solver_config['lr']]\n",
    "      plot_training_progress(SAVE_DIR, plot_data)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(session, x, y, config):\n",
    "  batch_size = config['batch_size']\n",
    "  num_examples = x.shape[0]\n",
    "  assert num_examples % batch_size == 0\n",
    "  num_batches = num_examples // batch_size\n",
    "  cnt_correct = 0\n",
    "  loss_avg = 0\n",
    "\n",
    "\n",
    "  for i in range(num_batches):\n",
    "    batch_x = x[i*batch_size:(i+1)*batch_size, ...]\n",
    "    batch_y = y[i*batch_size:(i+1)*batch_size, ...]\n",
    "    \n",
    "    data_dict = {X: batch_x, Y_: batch_y}\n",
    "    logits_val, loss_val = session.run([logits, loss] ,feed_dict=data_dict)\n",
    "    \n",
    "    yp = np.argmax(logits_val, 1)\n",
    "    yt = batch_y\n",
    "    cnt_correct += (yp == yt).sum()\n",
    "\n",
    "    loss_avg += loss_val\n",
    "  valid_acc = cnt_correct / num_examples * 100\n",
    "  loss_avg /= num_batches\n",
    "    \n",
    "  print(\" accuracy = %.2f\" % valid_acc)\n",
    "  print(\" avg loss = %.2f\\n\" % loss_avg)\n",
    "  return loss_avg, valid_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_conv_filters(epoch, step, weights, save_dir):\n",
    "  w = weights.copy()\n",
    "  num_filters = w.shape[3]\n",
    "  num_channels = w.shape[2]\n",
    "  k = w.shape[0]\n",
    "  assert w.shape[0] == w.shape[1]\n",
    "  w = w.reshape(k, k, num_channels, num_filters)\n",
    "  w -= w.min()\n",
    "  w /= w.max()\n",
    "  border = 1\n",
    "  cols = 8\n",
    "  rows = math.ceil(num_filters / cols)\n",
    "  width = cols * k + (cols-1) * border\n",
    "  height = rows * k + (rows-1) * border\n",
    "  img = np.zeros([height, width, num_channels])\n",
    "  for i in range(num_filters):\n",
    "    r = int(i / cols) * (k + border)\n",
    "    c = int(i % cols) * (k + border)\n",
    "    img[r:r+k,c:c+k,:] = w[:,:,:,i]\n",
    "  filename = 'epoch_%02d_step_%06d.png' % (epoch, step)\n",
    "  ski.io.imsave(os.path.join(save_dir, filename), img)\n",
    "\n",
    "def plot_training_progress(save_dir, data):\n",
    "  fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16,8))\n",
    "  linewidth = 2\n",
    "  legend_size = 10\n",
    "  train_color = 'm'\n",
    "  val_color = 'c'\n",
    "\n",
    "  num_points = len(data['train_loss'])\n",
    "  x_data = np.linspace(1, num_points, num_points)\n",
    "  ax1.set_title('Cross-entropy loss')\n",
    "  ax1.plot(x_data, data['train_loss'], marker='o', color=train_color,\n",
    "           linewidth=linewidth, linestyle='-', label='train')\n",
    "  ax1.plot(x_data, data['valid_loss'], marker='o', color=val_color,\n",
    "           linewidth=linewidth, linestyle='-', label='validation')\n",
    "  ax1.legend(loc='upper right', fontsize=legend_size)\n",
    "  ax2.set_title('Average class accuracy')\n",
    "  ax2.plot(x_data, data['train_acc'], marker='o', color=train_color,\n",
    "           linewidth=linewidth, linestyle='-', label='train')\n",
    "  ax2.plot(x_data, data['valid_acc'], marker='o', color=val_color,\n",
    "           linewidth=linewidth, linestyle='-', label='validation')\n",
    "  ax2.legend(loc='upper left', fontsize=legend_size)\n",
    "  ax3.set_title('Learning rate')\n",
    "  ax3.plot(x_data, data['lr'], marker='o', color=train_color,\n",
    "           linewidth=linewidth, linestyle='-', label='learning_rate')\n",
    "  ax3.legend(loc='upper left', fontsize=legend_size)\n",
    "\n",
    "  save_path = os.path.join(save_dir, 'training_plot.pdf')\n",
    "  print('Plotting in: ', save_path)\n",
    "  plt.savefig(save_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 50 / 900, loss = 2.24 (0.006 sec/batch)\n",
      "epoch 1, step 100 / 900, loss = 1.98 (0.006 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mratkovic/.virtualenvs/py27_tf_env/lib/python2.7/site-packages/ipykernel/__main__.py:15: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 150 / 900, loss = 1.86 (0.006 sec/batch)\n",
      "epoch 1, step 200 / 900, loss = 1.84 (0.006 sec/batch)\n",
      "epoch 1, step 250 / 900, loss = 1.57 (0.006 sec/batch)\n",
      "epoch 1, step 300 / 900, loss = 1.54 (0.006 sec/batch)\n",
      "epoch 1, step 350 / 900, loss = 1.95 (0.006 sec/batch)\n",
      "epoch 1, step 400 / 900, loss = 1.65 (0.006 sec/batch)\n",
      "epoch 1, step 450 / 900, loss = 1.65 (0.006 sec/batch)\n",
      "epoch 1, step 500 / 900, loss = 1.56 (0.006 sec/batch)\n",
      "epoch 1, step 550 / 900, loss = 1.62 (0.006 sec/batch)\n",
      "epoch 1, step 600 / 900, loss = 1.67 (0.006 sec/batch)\n",
      "epoch 1, step 650 / 900, loss = 1.68 (0.006 sec/batch)\n",
      "epoch 1, step 700 / 900, loss = 1.55 (0.006 sec/batch)\n",
      "epoch 1, step 750 / 900, loss = 1.41 (0.006 sec/batch)\n",
      "epoch 1, step 800 / 900, loss = 1.42 (0.006 sec/batch)\n",
      "epoch 1, step 850 / 900, loss = 1.48 (0.006 sec/batch)\n",
      "epoch 1, step 900 / 900, loss = 1.53 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 48.88\n",
      " avg loss = 1.46\n",
      "\n",
      "Validation error:\n",
      " accuracy = 46.66\n",
      " avg loss = 1.50\n",
      "\n",
      "Epoch time: 7.20840907097\n",
      "Plotting in:  /home/mratkovic/deeplearning/outputs/cifar10/training_plot.pdf\n",
      "epoch 2, step 50 / 900, loss = 1.44 (0.006 sec/batch)\n",
      "epoch 2, step 100 / 900, loss = 1.34 (0.006 sec/batch)\n",
      "epoch 2, step 150 / 900, loss = 1.27 (0.006 sec/batch)\n",
      "epoch 2, step 200 / 900, loss = 1.20 (0.006 sec/batch)\n",
      "epoch 2, step 250 / 900, loss = 1.46 (0.006 sec/batch)\n",
      "epoch 2, step 300 / 900, loss = 1.14 (0.006 sec/batch)\n",
      "epoch 2, step 350 / 900, loss = 1.47 (0.006 sec/batch)\n",
      "epoch 2, step 400 / 900, loss = 1.51 (0.006 sec/batch)\n",
      "epoch 2, step 450 / 900, loss = 1.42 (0.006 sec/batch)\n",
      "epoch 2, step 500 / 900, loss = 1.25 (0.006 sec/batch)\n",
      "epoch 2, step 550 / 900, loss = 1.49 (0.006 sec/batch)\n",
      "epoch 2, step 600 / 900, loss = 1.31 (0.006 sec/batch)\n",
      "epoch 2, step 650 / 900, loss = 1.27 (0.006 sec/batch)\n",
      "epoch 2, step 700 / 900, loss = 1.18 (0.006 sec/batch)\n",
      "epoch 2, step 750 / 900, loss = 1.23 (0.006 sec/batch)\n",
      "epoch 2, step 800 / 900, loss = 1.16 (0.006 sec/batch)\n",
      "epoch 2, step 850 / 900, loss = 1.08 (0.006 sec/batch)\n",
      "epoch 2, step 900 / 900, loss = 1.15 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.79\n",
      " avg loss = 1.30\n",
      "\n",
      "Validation error:\n",
      " accuracy = 55.34\n",
      " avg loss = 1.34\n",
      "\n",
      "Epoch time: 7.16539978981\n",
      "Plotting in:  /home/mratkovic/deeplearning/outputs/cifar10/training_plot.pdf\n",
      "epoch 3, step 50 / 900, loss = 1.49 (0.006 sec/batch)\n",
      "epoch 3, step 100 / 900, loss = 1.32 (0.006 sec/batch)\n",
      "epoch 3, step 150 / 900, loss = 1.18 (0.006 sec/batch)\n",
      "epoch 3, step 200 / 900, loss = 1.11 (0.006 sec/batch)\n",
      "epoch 3, step 250 / 900, loss = 1.23 (0.006 sec/batch)\n",
      "epoch 3, step 300 / 900, loss = 0.83 (0.006 sec/batch)\n",
      "epoch 3, step 350 / 900, loss = 1.51 (0.006 sec/batch)\n",
      "epoch 3, step 400 / 900, loss = 1.21 (0.006 sec/batch)\n",
      "epoch 3, step 450 / 900, loss = 0.93 (0.006 sec/batch)\n",
      "epoch 3, step 500 / 900, loss = 1.15 (0.006 sec/batch)\n",
      "epoch 3, step 550 / 900, loss = 1.10 (0.006 sec/batch)\n",
      "epoch 3, step 600 / 900, loss = 0.93 (0.006 sec/batch)\n",
      "epoch 3, step 650 / 900, loss = 1.23 (0.006 sec/batch)\n",
      "epoch 3, step 700 / 900, loss = 1.16 (0.006 sec/batch)\n",
      "epoch 3, step 750 / 900, loss = 1.39 (0.006 sec/batch)\n",
      "epoch 3, step 800 / 900, loss = 1.20 (0.006 sec/batch)\n",
      "epoch 3, step 850 / 900, loss = 1.12 (0.006 sec/batch)\n",
      "epoch 3, step 900 / 900, loss = 1.20 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 62.33\n",
      " avg loss = 1.10\n",
      "\n",
      "Validation error:\n",
      " accuracy = 60.30\n",
      " avg loss = 1.16\n",
      "\n",
      "Epoch time: 7.17197799683\n",
      "Plotting in:  /home/mratkovic/deeplearning/outputs/cifar10/training_plot.pdf\n",
      "epoch 4, step 50 / 900, loss = 0.98 (0.006 sec/batch)\n",
      "epoch 4, step 100 / 900, loss = 1.21 (0.007 sec/batch)\n",
      "epoch 4, step 150 / 900, loss = 1.22 (0.006 sec/batch)\n",
      "epoch 4, step 200 / 900, loss = 1.08 (0.006 sec/batch)\n",
      "epoch 4, step 250 / 900, loss = 1.18 (0.006 sec/batch)\n",
      "epoch 4, step 300 / 900, loss = 1.25 (0.007 sec/batch)\n",
      "epoch 4, step 350 / 900, loss = 1.17 (0.006 sec/batch)\n",
      "epoch 4, step 400 / 900, loss = 1.20 (0.006 sec/batch)\n",
      "epoch 4, step 450 / 900, loss = 1.07 (0.006 sec/batch)\n",
      "epoch 4, step 500 / 900, loss = 1.28 (0.006 sec/batch)\n",
      "epoch 4, step 550 / 900, loss = 1.04 (0.006 sec/batch)\n",
      "epoch 4, step 600 / 900, loss = 1.45 (0.006 sec/batch)\n",
      "epoch 4, step 650 / 900, loss = 0.81 (0.006 sec/batch)\n",
      "epoch 4, step 700 / 900, loss = 0.80 (0.006 sec/batch)\n",
      "epoch 4, step 750 / 900, loss = 1.13 (0.006 sec/batch)\n",
      "epoch 4, step 800 / 900, loss = 0.94 (0.006 sec/batch)\n",
      "epoch 4, step 850 / 900, loss = 0.86 (0.006 sec/batch)\n",
      "epoch 4, step 900 / 900, loss = 1.27 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 64.99\n",
      " avg loss = 1.04\n",
      "\n",
      "Validation error:\n",
      " accuracy = 62.96\n",
      " avg loss = 1.11\n",
      "\n",
      "Epoch time: 7.17639017105\n",
      "Plotting in:  /home/mratkovic/deeplearning/outputs/cifar10/training_plot.pdf\n",
      "epoch 5, step 50 / 900, loss = 1.07 (0.006 sec/batch)\n",
      "epoch 5, step 100 / 900, loss = 0.96 (0.006 sec/batch)\n",
      "epoch 5, step 150 / 900, loss = 0.91 (0.006 sec/batch)\n",
      "epoch 5, step 200 / 900, loss = 1.05 (0.006 sec/batch)\n",
      "epoch 5, step 250 / 900, loss = 1.24 (0.006 sec/batch)\n",
      "epoch 5, step 300 / 900, loss = 0.92 (0.006 sec/batch)\n",
      "epoch 5, step 350 / 900, loss = 0.77 (0.006 sec/batch)\n",
      "epoch 5, step 400 / 900, loss = 0.95 (0.006 sec/batch)\n",
      "epoch 5, step 450 / 900, loss = 0.93 (0.006 sec/batch)\n",
      "epoch 5, step 500 / 900, loss = 0.84 (0.006 sec/batch)\n",
      "epoch 5, step 550 / 900, loss = 0.79 (0.006 sec/batch)\n",
      "epoch 5, step 600 / 900, loss = 0.79 (0.006 sec/batch)\n",
      "epoch 5, step 650 / 900, loss = 1.00 (0.006 sec/batch)\n",
      "epoch 5, step 700 / 900, loss = 0.94 (0.007 sec/batch)\n",
      "epoch 5, step 750 / 900, loss = 0.83 (0.006 sec/batch)\n",
      "epoch 5, step 800 / 900, loss = 0.97 (0.006 sec/batch)\n",
      "epoch 5, step 850 / 900, loss = 1.18 (0.006 sec/batch)\n",
      "epoch 5, step 900 / 900, loss = 1.09 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 68.24\n",
      " avg loss = 0.95\n",
      "\n",
      "Validation error:\n",
      " accuracy = 64.68\n",
      " avg loss = 1.04\n",
      "\n",
      "Epoch time: 7.17915415764\n",
      "Plotting in:  /home/mratkovic/deeplearning/outputs/cifar10/training_plot.pdf\n",
      "epoch 6, step 50 / 900, loss = 1.02 (0.006 sec/batch)\n",
      "epoch 6, step 100 / 900, loss = 1.07 (0.006 sec/batch)\n",
      "epoch 6, step 150 / 900, loss = 0.87 (0.006 sec/batch)\n",
      "epoch 6, step 200 / 900, loss = 0.91 (0.006 sec/batch)\n",
      "epoch 6, step 250 / 900, loss = 0.93 (0.006 sec/batch)\n",
      "epoch 6, step 300 / 900, loss = 0.94 (0.006 sec/batch)\n",
      "epoch 6, step 350 / 900, loss = 1.16 (0.006 sec/batch)\n",
      "epoch 6, step 400 / 900, loss = 0.95 (0.006 sec/batch)\n",
      "epoch 6, step 450 / 900, loss = 1.14 (0.006 sec/batch)\n",
      "epoch 6, step 500 / 900, loss = 0.97 (0.006 sec/batch)\n",
      "epoch 6, step 550 / 900, loss = 0.95 (0.006 sec/batch)\n",
      "epoch 6, step 600 / 900, loss = 0.85 (0.006 sec/batch)\n",
      "epoch 6, step 650 / 900, loss = 1.04 (0.006 sec/batch)\n",
      "epoch 6, step 700 / 900, loss = 0.95 (0.006 sec/batch)\n",
      "epoch 6, step 750 / 900, loss = 1.01 (0.006 sec/batch)\n",
      "epoch 6, step 800 / 900, loss = 1.09 (0.006 sec/batch)\n",
      "epoch 6, step 850 / 900, loss = 0.83 (0.006 sec/batch)\n",
      "epoch 6, step 900 / 900, loss = 1.08 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 69.96\n",
      " avg loss = 0.90\n",
      "\n",
      "Validation error:\n",
      " accuracy = 66.10\n",
      " avg loss = 1.01\n",
      "\n",
      "Epoch time: 7.16561818123\n",
      "Plotting in:  /home/mratkovic/deeplearning/outputs/cifar10/training_plot.pdf\n",
      "epoch 7, step 50 / 900, loss = 1.00 (0.006 sec/batch)\n",
      "epoch 7, step 100 / 900, loss = 0.74 (0.007 sec/batch)\n",
      "epoch 7, step 150 / 900, loss = 0.69 (0.006 sec/batch)\n",
      "epoch 7, step 200 / 900, loss = 1.19 (0.006 sec/batch)\n",
      "epoch 7, step 250 / 900, loss = 0.75 (0.006 sec/batch)\n",
      "epoch 7, step 300 / 900, loss = 0.77 (0.006 sec/batch)\n",
      "epoch 7, step 350 / 900, loss = 0.95 (0.006 sec/batch)\n",
      "epoch 7, step 400 / 900, loss = 1.15 (0.006 sec/batch)\n",
      "epoch 7, step 450 / 900, loss = 0.80 (0.006 sec/batch)\n",
      "epoch 7, step 500 / 900, loss = 1.03 (0.006 sec/batch)\n",
      "epoch 7, step 550 / 900, loss = 1.00 (0.006 sec/batch)\n",
      "epoch 7, step 600 / 900, loss = 1.08 (0.006 sec/batch)\n",
      "epoch 7, step 650 / 900, loss = 1.03 (0.006 sec/batch)\n",
      "epoch 7, step 700 / 900, loss = 1.11 (0.006 sec/batch)\n",
      "epoch 7, step 750 / 900, loss = 0.86 (0.006 sec/batch)\n",
      "epoch 7, step 800 / 900, loss = 0.75 (0.006 sec/batch)\n",
      "epoch 7, step 850 / 900, loss = 0.79 (0.007 sec/batch)\n",
      "epoch 7, step 900 / 900, loss = 0.70 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 71.95\n",
      " avg loss = 0.85\n",
      "\n",
      "Validation error:\n",
      " accuracy = 67.24\n",
      " avg loss = 0.97\n",
      "\n",
      "Epoch time: 7.17195796967\n",
      "Plotting in:  /home/mratkovic/deeplearning/outputs/cifar10/training_plot.pdf\n",
      "epoch 8, step 50 / 900, loss = 1.11 (0.006 sec/batch)\n",
      "epoch 8, step 100 / 900, loss = 1.05 (0.006 sec/batch)\n",
      "epoch 8, step 150 / 900, loss = 0.84 (0.006 sec/batch)\n",
      "epoch 8, step 200 / 900, loss = 0.93 (0.006 sec/batch)\n",
      "epoch 8, step 250 / 900, loss = 0.91 (0.006 sec/batch)\n",
      "epoch 8, step 300 / 900, loss = 0.95 (0.006 sec/batch)\n",
      "epoch 8, step 350 / 900, loss = 0.89 (0.006 sec/batch)\n",
      "epoch 8, step 400 / 900, loss = 0.97 (0.006 sec/batch)\n",
      "epoch 8, step 450 / 900, loss = 0.86 (0.006 sec/batch)\n",
      "epoch 8, step 500 / 900, loss = 0.96 (0.006 sec/batch)\n",
      "epoch 8, step 550 / 900, loss = 0.83 (0.006 sec/batch)\n",
      "epoch 8, step 600 / 900, loss = 0.86 (0.006 sec/batch)\n",
      "epoch 8, step 650 / 900, loss = 0.78 (0.006 sec/batch)\n",
      "epoch 8, step 700 / 900, loss = 1.08 (0.007 sec/batch)\n",
      "epoch 8, step 750 / 900, loss = 0.75 (0.006 sec/batch)\n",
      "epoch 8, step 800 / 900, loss = 0.52 (0.006 sec/batch)\n",
      "epoch 8, step 850 / 900, loss = 0.66 (0.006 sec/batch)\n",
      "epoch 8, step 900 / 900, loss = 0.84 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 72.80\n",
      " avg loss = 0.82\n",
      "\n",
      "Validation error:\n",
      " accuracy = 67.92\n",
      " avg loss = 0.96\n",
      "\n",
      "Epoch time: 7.1743710041\n",
      "Plotting in:  /home/mratkovic/deeplearning/outputs/cifar10/training_plot.pdf\n",
      "epoch 9, step 50 / 900, loss = 0.71 (0.006 sec/batch)\n",
      "epoch 9, step 100 / 900, loss = 0.79 (0.006 sec/batch)\n",
      "epoch 9, step 150 / 900, loss = 0.69 (0.006 sec/batch)\n",
      "epoch 9, step 200 / 900, loss = 0.75 (0.006 sec/batch)\n",
      "epoch 9, step 250 / 900, loss = 0.55 (0.006 sec/batch)\n",
      "epoch 9, step 300 / 900, loss = 0.64 (0.006 sec/batch)\n",
      "epoch 9, step 350 / 900, loss = 0.72 (0.006 sec/batch)\n",
      "epoch 9, step 400 / 900, loss = 0.92 (0.006 sec/batch)\n",
      "epoch 9, step 450 / 900, loss = 0.89 (0.007 sec/batch)\n",
      "epoch 9, step 500 / 900, loss = 0.80 (0.006 sec/batch)\n",
      "epoch 9, step 550 / 900, loss = 0.68 (0.006 sec/batch)\n",
      "epoch 9, step 600 / 900, loss = 0.89 (0.006 sec/batch)\n",
      "epoch 9, step 650 / 900, loss = 0.99 (0.006 sec/batch)\n",
      "epoch 9, step 700 / 900, loss = 0.78 (0.007 sec/batch)\n",
      "epoch 9, step 750 / 900, loss = 0.97 (0.006 sec/batch)\n",
      "epoch 9, step 800 / 900, loss = 0.96 (0.006 sec/batch)\n",
      "epoch 9, step 850 / 900, loss = 0.95 (0.006 sec/batch)\n",
      "epoch 9, step 900 / 900, loss = 0.73 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 73.48\n",
      " avg loss = 0.79\n",
      "\n",
      "Validation error:\n",
      " accuracy = 68.08\n",
      " avg loss = 0.95\n",
      "\n",
      "Epoch time: 7.15140318871\n",
      "Plotting in:  /home/mratkovic/deeplearning/outputs/cifar10/training_plot.pdf\n",
      "epoch 10, step 50 / 900, loss = 0.66 (0.006 sec/batch)\n",
      "epoch 10, step 100 / 900, loss = 1.00 (0.006 sec/batch)\n",
      "epoch 10, step 150 / 900, loss = 0.94 (0.006 sec/batch)\n",
      "epoch 10, step 200 / 900, loss = 1.04 (0.006 sec/batch)\n",
      "epoch 10, step 250 / 900, loss = 0.70 (0.006 sec/batch)\n",
      "epoch 10, step 300 / 900, loss = 0.76 (0.006 sec/batch)\n",
      "epoch 10, step 350 / 900, loss = 0.85 (0.006 sec/batch)\n",
      "epoch 10, step 400 / 900, loss = 0.71 (0.006 sec/batch)\n",
      "epoch 10, step 450 / 900, loss = 0.52 (0.006 sec/batch)\n",
      "epoch 10, step 500 / 900, loss = 0.62 (0.006 sec/batch)\n",
      "epoch 10, step 550 / 900, loss = 0.82 (0.006 sec/batch)\n",
      "epoch 10, step 600 / 900, loss = 1.02 (0.006 sec/batch)\n",
      "epoch 10, step 650 / 900, loss = 0.97 (0.006 sec/batch)\n",
      "epoch 10, step 700 / 900, loss = 0.88 (0.006 sec/batch)\n",
      "epoch 10, step 750 / 900, loss = 0.69 (0.006 sec/batch)\n",
      "epoch 10, step 800 / 900, loss = 0.93 (0.006 sec/batch)\n",
      "epoch 10, step 850 / 900, loss = 0.74 (0.006 sec/batch)\n",
      "epoch 10, step 900 / 900, loss = 1.07 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 74.60\n",
      " avg loss = 0.77\n",
      "\n",
      "Validation error:\n",
      " accuracy = 68.22\n",
      " avg loss = 0.94\n",
      "\n",
      "Epoch time: 7.1779191494\n",
      "Plotting in:  /home/mratkovic/deeplearning/outputs/cifar10/training_plot.pdf\n",
      "epoch 11, step 50 / 900, loss = 0.84 (0.006 sec/batch)\n",
      "epoch 11, step 100 / 900, loss = 0.71 (0.006 sec/batch)\n",
      "epoch 11, step 150 / 900, loss = 0.76 (0.006 sec/batch)\n",
      "epoch 11, step 200 / 900, loss = 0.66 (0.006 sec/batch)\n",
      "epoch 11, step 250 / 900, loss = 0.69 (0.006 sec/batch)\n",
      "epoch 11, step 300 / 900, loss = 0.79 (0.006 sec/batch)\n",
      "epoch 11, step 350 / 900, loss = 0.65 (0.006 sec/batch)\n",
      "epoch 11, step 400 / 900, loss = 0.55 (0.006 sec/batch)\n",
      "epoch 11, step 450 / 900, loss = 0.90 (0.006 sec/batch)\n",
      "epoch 11, step 500 / 900, loss = 0.79 (0.006 sec/batch)\n",
      "epoch 11, step 550 / 900, loss = 0.59 (0.006 sec/batch)\n",
      "epoch 11, step 600 / 900, loss = 0.55 (0.006 sec/batch)\n",
      "epoch 11, step 650 / 900, loss = 0.77 (0.006 sec/batch)\n",
      "epoch 11, step 700 / 900, loss = 0.83 (0.006 sec/batch)\n",
      "epoch 11, step 750 / 900, loss = 0.60 (0.006 sec/batch)\n",
      "epoch 11, step 800 / 900, loss = 0.86 (0.006 sec/batch)\n",
      "epoch 11, step 850 / 900, loss = 0.57 (0.006 sec/batch)\n",
      "epoch 11, step 900 / 900, loss = 0.81 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 75.54\n",
      " avg loss = 0.75\n",
      "\n",
      "Validation error:\n",
      " accuracy = 69.16\n",
      " avg loss = 0.94\n",
      "\n",
      "Epoch time: 7.18777894974\n",
      "Plotting in:  /home/mratkovic/deeplearning/outputs/cifar10/training_plot.pdf\n",
      "epoch 12, step 50 / 900, loss = 0.69 (0.006 sec/batch)\n",
      "epoch 12, step 100 / 900, loss = 0.71 (0.006 sec/batch)\n",
      "epoch 12, step 150 / 900, loss = 0.77 (0.006 sec/batch)\n",
      "epoch 12, step 200 / 900, loss = 0.57 (0.006 sec/batch)\n",
      "epoch 12, step 250 / 900, loss = 0.71 (0.006 sec/batch)\n",
      "epoch 12, step 300 / 900, loss = 0.86 (0.006 sec/batch)\n",
      "epoch 12, step 350 / 900, loss = 0.56 (0.006 sec/batch)\n",
      "epoch 12, step 400 / 900, loss = 0.64 (0.006 sec/batch)\n",
      "epoch 12, step 450 / 900, loss = 0.58 (0.006 sec/batch)\n",
      "epoch 12, step 500 / 900, loss = 0.49 (0.006 sec/batch)\n",
      "epoch 12, step 550 / 900, loss = 0.70 (0.006 sec/batch)\n",
      "epoch 12, step 600 / 900, loss = 0.87 (0.006 sec/batch)\n",
      "epoch 12, step 650 / 900, loss = 0.83 (0.006 sec/batch)\n",
      "epoch 12, step 700 / 900, loss = 0.65 (0.006 sec/batch)\n",
      "epoch 12, step 750 / 900, loss = 0.71 (0.007 sec/batch)\n",
      "epoch 12, step 800 / 900, loss = 0.83 (0.006 sec/batch)\n",
      "epoch 12, step 850 / 900, loss = 0.56 (0.006 sec/batch)\n",
      "epoch 12, step 900 / 900, loss = 0.68 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 76.39\n",
      " avg loss = 0.72\n",
      "\n",
      "Validation error:\n",
      " accuracy = 69.38\n",
      " avg loss = 0.93\n",
      "\n",
      "Epoch time: 7.16974210739\n",
      "Plotting in:  /home/mratkovic/deeplearning/outputs/cifar10/training_plot.pdf\n",
      "epoch 13, step 50 / 900, loss = 0.91 (0.006 sec/batch)\n",
      "epoch 13, step 100 / 900, loss = 0.62 (0.006 sec/batch)\n",
      "epoch 13, step 150 / 900, loss = 0.59 (0.006 sec/batch)\n",
      "epoch 13, step 200 / 900, loss = 0.56 (0.006 sec/batch)\n",
      "epoch 13, step 250 / 900, loss = 0.83 (0.007 sec/batch)\n",
      "epoch 13, step 300 / 900, loss = 0.57 (0.006 sec/batch)\n",
      "epoch 13, step 350 / 900, loss = 0.65 (0.006 sec/batch)\n",
      "epoch 13, step 400 / 900, loss = 0.54 (0.006 sec/batch)\n",
      "epoch 13, step 450 / 900, loss = 0.66 (0.006 sec/batch)\n",
      "epoch 13, step 500 / 900, loss = 0.51 (0.006 sec/batch)\n",
      "epoch 13, step 550 / 900, loss = 0.61 (0.006 sec/batch)\n",
      "epoch 13, step 600 / 900, loss = 0.55 (0.006 sec/batch)\n",
      "epoch 13, step 650 / 900, loss = 0.75 (0.006 sec/batch)\n",
      "epoch 13, step 700 / 900, loss = 1.03 (0.006 sec/batch)\n",
      "epoch 13, step 750 / 900, loss = 0.77 (0.006 sec/batch)\n",
      "epoch 13, step 800 / 900, loss = 0.74 (0.006 sec/batch)\n",
      "epoch 13, step 850 / 900, loss = 0.77 (0.006 sec/batch)\n",
      "epoch 13, step 900 / 900, loss = 0.72 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 77.07\n",
      " avg loss = 0.70\n",
      "\n",
      "Validation error:\n",
      " accuracy = 69.24\n",
      " avg loss = 0.91\n",
      "\n",
      "Epoch time: 7.18053412437\n",
      "Plotting in:  /home/mratkovic/deeplearning/outputs/cifar10/training_plot.pdf\n",
      "epoch 14, step 50 / 900, loss = 0.93 (0.006 sec/batch)\n",
      "epoch 14, step 100 / 900, loss = 0.54 (0.006 sec/batch)\n",
      "epoch 14, step 150 / 900, loss = 0.72 (0.006 sec/batch)\n",
      "epoch 14, step 200 / 900, loss = 0.88 (0.006 sec/batch)\n",
      "epoch 14, step 250 / 900, loss = 0.61 (0.006 sec/batch)\n",
      "epoch 14, step 300 / 900, loss = 0.81 (0.006 sec/batch)\n",
      "epoch 14, step 350 / 900, loss = 0.57 (0.006 sec/batch)\n",
      "epoch 14, step 400 / 900, loss = 0.55 (0.006 sec/batch)\n",
      "epoch 14, step 450 / 900, loss = 0.46 (0.006 sec/batch)\n",
      "epoch 14, step 500 / 900, loss = 0.96 (0.006 sec/batch)\n",
      "epoch 14, step 550 / 900, loss = 0.80 (0.007 sec/batch)\n",
      "epoch 14, step 600 / 900, loss = 0.58 (0.006 sec/batch)\n",
      "epoch 14, step 650 / 900, loss = 0.73 (0.006 sec/batch)\n",
      "epoch 14, step 700 / 900, loss = 0.66 (0.006 sec/batch)\n",
      "epoch 14, step 750 / 900, loss = 1.00 (0.006 sec/batch)\n",
      "epoch 14, step 800 / 900, loss = 0.78 (0.006 sec/batch)\n",
      "epoch 14, step 850 / 900, loss = 0.76 (0.006 sec/batch)\n",
      "epoch 14, step 900 / 900, loss = 0.65 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 77.82\n",
      " avg loss = 0.68\n",
      "\n",
      "Validation error:\n",
      " accuracy = 69.42\n",
      " avg loss = 0.91\n",
      "\n",
      "Epoch time: 7.16836595535\n",
      "Plotting in:  /home/mratkovic/deeplearning/outputs/cifar10/training_plot.pdf\n",
      "epoch 15, step 50 / 900, loss = 0.61 (0.006 sec/batch)\n",
      "epoch 15, step 100 / 900, loss = 0.84 (0.006 sec/batch)\n",
      "epoch 15, step 150 / 900, loss = 0.66 (0.006 sec/batch)\n",
      "epoch 15, step 200 / 900, loss = 0.71 (0.006 sec/batch)\n",
      "epoch 15, step 250 / 900, loss = 0.61 (0.006 sec/batch)\n",
      "epoch 15, step 300 / 900, loss = 0.85 (0.006 sec/batch)\n",
      "epoch 15, step 350 / 900, loss = 0.55 (0.006 sec/batch)\n",
      "epoch 15, step 400 / 900, loss = 0.70 (0.007 sec/batch)\n",
      "epoch 15, step 450 / 900, loss = 0.72 (0.006 sec/batch)\n",
      "epoch 15, step 500 / 900, loss = 0.62 (0.006 sec/batch)\n",
      "epoch 15, step 550 / 900, loss = 0.97 (0.006 sec/batch)\n",
      "epoch 15, step 600 / 900, loss = 0.65 (0.006 sec/batch)\n",
      "epoch 15, step 650 / 900, loss = 0.81 (0.006 sec/batch)\n",
      "epoch 15, step 700 / 900, loss = 0.66 (0.006 sec/batch)\n",
      "epoch 15, step 750 / 900, loss = 0.76 (0.006 sec/batch)\n",
      "epoch 15, step 800 / 900, loss = 0.69 (0.006 sec/batch)\n",
      "epoch 15, step 850 / 900, loss = 0.45 (0.006 sec/batch)\n",
      "epoch 15, step 900 / 900, loss = 1.03 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 78.82\n",
      " avg loss = 0.66\n",
      "\n",
      "Validation error:\n",
      " accuracy = 69.74\n",
      " avg loss = 0.89\n",
      "\n",
      "Epoch time: 7.19134616852\n",
      "Plotting in:  /home/mratkovic/deeplearning/outputs/cifar10/training_plot.pdf\n",
      "epoch 16, step 50 / 900, loss = 0.74 (0.006 sec/batch)\n",
      "epoch 16, step 100 / 900, loss = 0.52 (0.006 sec/batch)\n",
      "epoch 16, step 150 / 900, loss = 0.73 (0.006 sec/batch)\n",
      "epoch 16, step 200 / 900, loss = 0.79 (0.006 sec/batch)\n",
      "epoch 16, step 250 / 900, loss = 0.73 (0.006 sec/batch)\n",
      "epoch 16, step 300 / 900, loss = 0.66 (0.006 sec/batch)\n",
      "epoch 16, step 350 / 900, loss = 0.62 (0.006 sec/batch)\n",
      "epoch 16, step 400 / 900, loss = 0.63 (0.007 sec/batch)\n",
      "epoch 16, step 450 / 900, loss = 0.79 (0.006 sec/batch)\n",
      "epoch 16, step 500 / 900, loss = 0.44 (0.006 sec/batch)\n",
      "epoch 16, step 550 / 900, loss = 0.71 (0.006 sec/batch)\n",
      "epoch 16, step 600 / 900, loss = 0.70 (0.006 sec/batch)\n",
      "epoch 16, step 650 / 900, loss = 0.54 (0.006 sec/batch)\n",
      "epoch 16, step 700 / 900, loss = 0.58 (0.006 sec/batch)\n",
      "epoch 16, step 750 / 900, loss = 0.85 (0.006 sec/batch)\n",
      "epoch 16, step 800 / 900, loss = 0.72 (0.006 sec/batch)\n",
      "epoch 16, step 850 / 900, loss = 0.83 (0.006 sec/batch)\n",
      "epoch 16, step 900 / 900, loss = 0.51 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 78.85\n",
      " avg loss = 0.65\n",
      "\n",
      "Validation error:\n",
      " accuracy = 69.82\n",
      " avg loss = 0.90\n",
      "\n",
      "Epoch time: 7.17393612862\n",
      "Plotting in:  /home/mratkovic/deeplearning/outputs/cifar10/training_plot.pdf\n",
      "epoch 17, step 50 / 900, loss = 0.69 (0.006 sec/batch)\n",
      "epoch 17, step 100 / 900, loss = 0.94 (0.006 sec/batch)\n",
      "epoch 17, step 150 / 900, loss = 0.63 (0.006 sec/batch)\n",
      "epoch 17, step 200 / 900, loss = 0.48 (0.006 sec/batch)\n",
      "epoch 17, step 250 / 900, loss = 0.75 (0.006 sec/batch)\n",
      "epoch 17, step 300 / 900, loss = 0.54 (0.006 sec/batch)\n",
      "epoch 17, step 350 / 900, loss = 0.66 (0.007 sec/batch)\n",
      "epoch 17, step 400 / 900, loss = 0.58 (0.006 sec/batch)\n",
      "epoch 17, step 450 / 900, loss = 0.54 (0.006 sec/batch)\n",
      "epoch 17, step 500 / 900, loss = 0.75 (0.006 sec/batch)\n",
      "epoch 17, step 550 / 900, loss = 0.54 (0.006 sec/batch)\n",
      "epoch 17, step 600 / 900, loss = 0.46 (0.006 sec/batch)\n",
      "epoch 17, step 650 / 900, loss = 0.64 (0.006 sec/batch)\n",
      "epoch 17, step 700 / 900, loss = 0.55 (0.006 sec/batch)\n",
      "epoch 17, step 750 / 900, loss = 0.82 (0.006 sec/batch)\n",
      "epoch 17, step 800 / 900, loss = 0.85 (0.006 sec/batch)\n",
      "epoch 17, step 850 / 900, loss = 0.63 (0.006 sec/batch)\n",
      "epoch 17, step 900 / 900, loss = 0.68 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 79.65\n",
      " avg loss = 0.63\n",
      "\n",
      "Validation error:\n",
      " accuracy = 70.08\n",
      " avg loss = 0.90\n",
      "\n",
      "Epoch time: 7.18186092377\n",
      "Plotting in:  /home/mratkovic/deeplearning/outputs/cifar10/training_plot.pdf\n",
      "epoch 18, step 50 / 900, loss = 0.57 (0.006 sec/batch)\n",
      "epoch 18, step 100 / 900, loss = 0.67 (0.006 sec/batch)\n",
      "epoch 18, step 150 / 900, loss = 0.62 (0.006 sec/batch)\n",
      "epoch 18, step 200 / 900, loss = 0.58 (0.006 sec/batch)\n",
      "epoch 18, step 250 / 900, loss = 0.77 (0.006 sec/batch)\n",
      "epoch 18, step 300 / 900, loss = 0.79 (0.006 sec/batch)\n",
      "epoch 18, step 350 / 900, loss = 0.88 (0.006 sec/batch)\n",
      "epoch 18, step 400 / 900, loss = 0.84 (0.006 sec/batch)\n",
      "epoch 18, step 450 / 900, loss = 0.97 (0.006 sec/batch)\n",
      "epoch 18, step 500 / 900, loss = 0.39 (0.006 sec/batch)\n",
      "epoch 18, step 550 / 900, loss = 0.72 (0.006 sec/batch)\n",
      "epoch 18, step 600 / 900, loss = 0.79 (0.006 sec/batch)\n",
      "epoch 18, step 650 / 900, loss = 0.77 (0.006 sec/batch)\n",
      "epoch 18, step 700 / 900, loss = 0.76 (0.006 sec/batch)\n",
      "epoch 18, step 750 / 900, loss = 0.58 (0.006 sec/batch)\n",
      "epoch 18, step 800 / 900, loss = 0.62 (0.006 sec/batch)\n",
      "epoch 18, step 850 / 900, loss = 0.45 (0.006 sec/batch)\n",
      "epoch 18, step 900 / 900, loss = 0.50 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 80.24\n",
      " avg loss = 0.62\n",
      "\n",
      "Validation error:\n",
      " accuracy = 69.82\n",
      " avg loss = 0.89\n",
      "\n",
      "Epoch time: 7.18870687485\n",
      "Plotting in:  /home/mratkovic/deeplearning/outputs/cifar10/training_plot.pdf\n",
      "epoch 19, step 50 / 900, loss = 0.57 (0.006 sec/batch)\n",
      "epoch 19, step 100 / 900, loss = 0.51 (0.006 sec/batch)\n",
      "epoch 19, step 150 / 900, loss = 0.47 (0.006 sec/batch)\n",
      "epoch 19, step 200 / 900, loss = 0.65 (0.006 sec/batch)\n",
      "epoch 19, step 250 / 900, loss = 0.82 (0.006 sec/batch)\n",
      "epoch 19, step 300 / 900, loss = 0.76 (0.006 sec/batch)\n",
      "epoch 19, step 350 / 900, loss = 0.60 (0.006 sec/batch)\n",
      "epoch 19, step 400 / 900, loss = 0.68 (0.006 sec/batch)\n",
      "epoch 19, step 450 / 900, loss = 0.71 (0.006 sec/batch)\n",
      "epoch 19, step 500 / 900, loss = 0.61 (0.006 sec/batch)\n",
      "epoch 19, step 550 / 900, loss = 0.47 (0.006 sec/batch)\n",
      "epoch 19, step 600 / 900, loss = 0.64 (0.006 sec/batch)\n",
      "epoch 19, step 650 / 900, loss = 0.51 (0.006 sec/batch)\n",
      "epoch 19, step 700 / 900, loss = 0.63 (0.006 sec/batch)\n",
      "epoch 19, step 750 / 900, loss = 0.52 (0.006 sec/batch)\n",
      "epoch 19, step 800 / 900, loss = 0.65 (0.006 sec/batch)\n",
      "epoch 19, step 850 / 900, loss = 0.63 (0.006 sec/batch)\n",
      "epoch 19, step 900 / 900, loss = 0.53 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 80.43\n",
      " avg loss = 0.61\n",
      "\n",
      "Validation error:\n",
      " accuracy = 70.18\n",
      " avg loss = 0.89\n",
      "\n",
      "Epoch time: 7.18633413315\n",
      "Plotting in:  /home/mratkovic/deeplearning/outputs/cifar10/training_plot.pdf\n",
      "epoch 20, step 50 / 900, loss = 0.57 (0.006 sec/batch)\n",
      "epoch 20, step 100 / 900, loss = 0.83 (0.006 sec/batch)\n",
      "epoch 20, step 150 / 900, loss = 0.56 (0.006 sec/batch)\n",
      "epoch 20, step 200 / 900, loss = 0.55 (0.006 sec/batch)\n",
      "epoch 20, step 250 / 900, loss = 0.71 (0.006 sec/batch)\n",
      "epoch 20, step 300 / 900, loss = 0.34 (0.006 sec/batch)\n",
      "epoch 20, step 350 / 900, loss = 0.64 (0.006 sec/batch)\n",
      "epoch 20, step 400 / 900, loss = 0.45 (0.006 sec/batch)\n",
      "epoch 20, step 450 / 900, loss = 0.47 (0.006 sec/batch)\n",
      "epoch 20, step 500 / 900, loss = 0.56 (0.006 sec/batch)\n",
      "epoch 20, step 550 / 900, loss = 0.67 (0.006 sec/batch)\n",
      "epoch 20, step 600 / 900, loss = 0.59 (0.006 sec/batch)\n",
      "epoch 20, step 650 / 900, loss = 0.43 (0.006 sec/batch)\n",
      "epoch 20, step 700 / 900, loss = 0.42 (0.006 sec/batch)\n",
      "epoch 20, step 750 / 900, loss = 0.74 (0.006 sec/batch)\n",
      "epoch 20, step 800 / 900, loss = 0.48 (0.006 sec/batch)\n",
      "epoch 20, step 850 / 900, loss = 0.68 (0.006 sec/batch)\n",
      "epoch 20, step 900 / 900, loss = 0.64 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 81.32\n",
      " avg loss = 0.59\n",
      "\n",
      "Validation error:\n",
      " accuracy = 70.58\n",
      " avg loss = 0.88\n",
      "\n",
      "Epoch time: 7.17362999916\n",
      "Plotting in:  /home/mratkovic/deeplearning/outputs/cifar10/training_plot.pdf\n",
      "epoch 21, step 50 / 900, loss = 0.70 (0.006 sec/batch)\n",
      "epoch 21, step 100 / 900, loss = 0.61 (0.006 sec/batch)\n",
      "epoch 21, step 150 / 900, loss = 0.91 (0.006 sec/batch)\n",
      "epoch 21, step 200 / 900, loss = 0.44 (0.006 sec/batch)\n",
      "epoch 21, step 250 / 900, loss = 0.56 (0.006 sec/batch)\n",
      "epoch 21, step 300 / 900, loss = 0.93 (0.006 sec/batch)\n",
      "epoch 21, step 350 / 900, loss = 0.56 (0.006 sec/batch)\n",
      "epoch 21, step 400 / 900, loss = 0.58 (0.006 sec/batch)\n",
      "epoch 21, step 450 / 900, loss = 0.54 (0.006 sec/batch)\n",
      "epoch 21, step 500 / 900, loss = 0.48 (0.006 sec/batch)\n",
      "epoch 21, step 550 / 900, loss = 0.54 (0.006 sec/batch)\n",
      "epoch 21, step 600 / 900, loss = 0.64 (0.007 sec/batch)\n",
      "epoch 21, step 650 / 900, loss = 0.52 (0.006 sec/batch)\n",
      "epoch 21, step 700 / 900, loss = 0.55 (0.006 sec/batch)\n",
      "epoch 21, step 750 / 900, loss = 0.67 (0.006 sec/batch)\n",
      "epoch 21, step 800 / 900, loss = 0.47 (0.006 sec/batch)\n",
      "epoch 21, step 850 / 900, loss = 0.52 (0.006 sec/batch)\n",
      "epoch 21, step 900 / 900, loss = 0.63 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 81.31\n",
      " avg loss = 0.59\n",
      "\n",
      "Validation error:\n",
      " accuracy = 70.36\n",
      " avg loss = 0.90\n",
      "\n",
      "Epoch time: 7.16821503639\n",
      "Plotting in:  /home/mratkovic/deeplearning/outputs/cifar10/training_plot.pdf\n",
      "epoch 22, step 50 / 900, loss = 0.55 (0.006 sec/batch)\n",
      "epoch 22, step 100 / 900, loss = 0.72 (0.006 sec/batch)\n",
      "epoch 22, step 150 / 900, loss = 0.36 (0.006 sec/batch)\n",
      "epoch 22, step 200 / 900, loss = 0.46 (0.006 sec/batch)\n",
      "epoch 22, step 250 / 900, loss = 0.54 (0.006 sec/batch)\n",
      "epoch 22, step 300 / 900, loss = 0.63 (0.006 sec/batch)\n",
      "epoch 22, step 350 / 900, loss = 0.80 (0.006 sec/batch)\n",
      "epoch 22, step 400 / 900, loss = 0.68 (0.007 sec/batch)\n",
      "epoch 22, step 450 / 900, loss = 0.49 (0.006 sec/batch)\n",
      "epoch 22, step 500 / 900, loss = 0.69 (0.006 sec/batch)\n",
      "epoch 22, step 550 / 900, loss = 0.67 (0.006 sec/batch)\n",
      "epoch 22, step 600 / 900, loss = 0.59 (0.006 sec/batch)\n",
      "epoch 22, step 650 / 900, loss = 0.71 (0.006 sec/batch)\n",
      "epoch 22, step 700 / 900, loss = 0.73 (0.006 sec/batch)\n",
      "epoch 22, step 750 / 900, loss = 0.45 (0.006 sec/batch)\n",
      "epoch 22, step 800 / 900, loss = 0.51 (0.007 sec/batch)\n",
      "epoch 22, step 850 / 900, loss = 0.58 (0.006 sec/batch)\n",
      "epoch 22, step 900 / 900, loss = 0.49 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 81.78\n",
      " avg loss = 0.57\n",
      "\n",
      "Validation error:\n",
      " accuracy = 70.70\n",
      " avg loss = 0.89\n",
      "\n",
      "Epoch time: 7.17656588554\n",
      "Plotting in:  /home/mratkovic/deeplearning/outputs/cifar10/training_plot.pdf\n",
      "epoch 23, step 50 / 900, loss = 0.57 (0.006 sec/batch)\n",
      "epoch 23, step 100 / 900, loss = 0.61 (0.006 sec/batch)\n",
      "epoch 23, step 150 / 900, loss = 0.46 (0.006 sec/batch)\n",
      "epoch 23, step 200 / 900, loss = 0.37 (0.006 sec/batch)\n",
      "epoch 23, step 250 / 900, loss = 0.70 (0.006 sec/batch)\n",
      "epoch 23, step 300 / 900, loss = 0.49 (0.006 sec/batch)\n",
      "epoch 23, step 350 / 900, loss = 0.65 (0.006 sec/batch)\n",
      "epoch 23, step 400 / 900, loss = 0.48 (0.006 sec/batch)\n",
      "epoch 23, step 450 / 900, loss = 0.50 (0.006 sec/batch)\n",
      "epoch 23, step 500 / 900, loss = 0.62 (0.006 sec/batch)\n",
      "epoch 23, step 550 / 900, loss = 0.80 (0.006 sec/batch)\n",
      "epoch 23, step 600 / 900, loss = 0.66 (0.006 sec/batch)\n",
      "epoch 23, step 650 / 900, loss = 0.65 (0.006 sec/batch)\n",
      "epoch 23, step 700 / 900, loss = 0.49 (0.006 sec/batch)\n",
      "epoch 23, step 750 / 900, loss = 0.61 (0.006 sec/batch)\n",
      "epoch 23, step 800 / 900, loss = 0.74 (0.006 sec/batch)\n",
      "epoch 23, step 850 / 900, loss = 0.76 (0.006 sec/batch)\n",
      "epoch 23, step 900 / 900, loss = 0.61 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 81.92\n",
      " avg loss = 0.57\n",
      "\n",
      "Validation error:\n",
      " accuracy = 70.82\n",
      " avg loss = 0.89\n",
      "\n",
      "Epoch time: 7.16797614098\n",
      "Plotting in:  /home/mratkovic/deeplearning/outputs/cifar10/training_plot.pdf\n",
      "epoch 24, step 50 / 900, loss = 0.61 (0.006 sec/batch)\n",
      "epoch 24, step 100 / 900, loss = 0.58 (0.006 sec/batch)\n",
      "epoch 24, step 150 / 900, loss = 0.46 (0.006 sec/batch)\n",
      "epoch 24, step 200 / 900, loss = 0.64 (0.006 sec/batch)\n",
      "epoch 24, step 250 / 900, loss = 0.54 (0.006 sec/batch)\n",
      "epoch 24, step 300 / 900, loss = 0.54 (0.006 sec/batch)\n",
      "epoch 24, step 350 / 900, loss = 0.37 (0.006 sec/batch)\n",
      "epoch 24, step 400 / 900, loss = 0.67 (0.006 sec/batch)\n",
      "epoch 24, step 450 / 900, loss = 0.58 (0.006 sec/batch)\n",
      "epoch 24, step 500 / 900, loss = 0.55 (0.006 sec/batch)\n",
      "epoch 24, step 550 / 900, loss = 0.42 (0.006 sec/batch)\n",
      "epoch 24, step 600 / 900, loss = 0.50 (0.006 sec/batch)\n",
      "epoch 24, step 650 / 900, loss = 0.58 (0.006 sec/batch)\n",
      "epoch 24, step 700 / 900, loss = 0.61 (0.006 sec/batch)\n",
      "epoch 24, step 750 / 900, loss = 0.60 (0.006 sec/batch)\n",
      "epoch 24, step 800 / 900, loss = 0.85 (0.006 sec/batch)\n",
      "epoch 24, step 850 / 900, loss = 0.52 (0.006 sec/batch)\n",
      "epoch 24, step 900 / 900, loss = 0.48 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 82.11\n",
      " avg loss = 0.56\n",
      "\n",
      "Validation error:\n",
      " accuracy = 70.90\n",
      " avg loss = 0.89\n",
      "\n",
      "Epoch time: 7.1698179245\n",
      "Plotting in:  /home/mratkovic/deeplearning/outputs/cifar10/training_plot.pdf\n",
      "epoch 25, step 50 / 900, loss = 0.53 (0.006 sec/batch)\n",
      "epoch 25, step 100 / 900, loss = 0.57 (0.006 sec/batch)\n",
      "epoch 25, step 150 / 900, loss = 0.67 (0.006 sec/batch)\n",
      "epoch 25, step 200 / 900, loss = 0.46 (0.007 sec/batch)\n",
      "epoch 25, step 250 / 900, loss = 0.58 (0.007 sec/batch)\n",
      "epoch 25, step 300 / 900, loss = 0.44 (0.006 sec/batch)\n",
      "epoch 25, step 350 / 900, loss = 0.55 (0.006 sec/batch)\n",
      "epoch 25, step 400 / 900, loss = 0.69 (0.006 sec/batch)\n",
      "epoch 25, step 450 / 900, loss = 0.58 (0.006 sec/batch)\n",
      "epoch 25, step 500 / 900, loss = 0.82 (0.006 sec/batch)\n",
      "epoch 25, step 550 / 900, loss = 0.66 (0.006 sec/batch)\n",
      "epoch 25, step 600 / 900, loss = 0.34 (0.006 sec/batch)\n",
      "epoch 25, step 650 / 900, loss = 0.38 (0.006 sec/batch)\n",
      "epoch 25, step 700 / 900, loss = 0.56 (0.006 sec/batch)\n",
      "epoch 25, step 750 / 900, loss = 0.51 (0.006 sec/batch)\n",
      "epoch 25, step 800 / 900, loss = 0.63 (0.006 sec/batch)\n",
      "epoch 25, step 850 / 900, loss = 0.67 (0.006 sec/batch)\n",
      "epoch 25, step 900 / 900, loss = 0.74 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 82.38\n",
      " avg loss = 0.56\n",
      "\n",
      "Validation error:\n",
      " accuracy = 70.72\n",
      " avg loss = 0.89\n",
      "\n",
      "Epoch time: 7.17004513741\n",
      "Plotting in:  /home/mratkovic/deeplearning/outputs/cifar10/training_plot.pdf\n",
      "epoch 26, step 50 / 900, loss = 0.53 (0.006 sec/batch)\n",
      "epoch 26, step 100 / 900, loss = 0.70 (0.006 sec/batch)\n",
      "epoch 26, step 150 / 900, loss = 0.54 (0.006 sec/batch)\n",
      "epoch 26, step 200 / 900, loss = 0.34 (0.006 sec/batch)\n",
      "epoch 26, step 250 / 900, loss = 0.59 (0.006 sec/batch)\n",
      "epoch 26, step 300 / 900, loss = 0.49 (0.006 sec/batch)\n",
      "epoch 26, step 350 / 900, loss = 0.81 (0.006 sec/batch)\n",
      "epoch 26, step 400 / 900, loss = 0.58 (0.006 sec/batch)\n",
      "epoch 26, step 450 / 900, loss = 0.69 (0.006 sec/batch)\n",
      "epoch 26, step 500 / 900, loss = 0.42 (0.006 sec/batch)\n",
      "epoch 26, step 550 / 900, loss = 0.51 (0.006 sec/batch)\n",
      "epoch 26, step 600 / 900, loss = 0.74 (0.006 sec/batch)\n",
      "epoch 26, step 650 / 900, loss = 0.39 (0.006 sec/batch)\n",
      "epoch 26, step 700 / 900, loss = 0.69 (0.006 sec/batch)\n",
      "epoch 26, step 750 / 900, loss = 0.63 (0.006 sec/batch)\n",
      "epoch 26, step 800 / 900, loss = 0.50 (0.006 sec/batch)\n",
      "epoch 26, step 850 / 900, loss = 0.68 (0.006 sec/batch)\n",
      "epoch 26, step 900 / 900, loss = 0.52 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 82.74\n",
      " avg loss = 0.55\n",
      "\n",
      "Validation error:\n",
      " accuracy = 70.96\n",
      " avg loss = 0.89\n",
      "\n",
      "Epoch time: 7.18672204018\n",
      "Plotting in:  /home/mratkovic/deeplearning/outputs/cifar10/training_plot.pdf\n",
      "epoch 27, step 50 / 900, loss = 0.84 (0.006 sec/batch)\n",
      "epoch 27, step 100 / 900, loss = 0.48 (0.006 sec/batch)\n",
      "epoch 27, step 150 / 900, loss = 0.55 (0.006 sec/batch)\n",
      "epoch 27, step 200 / 900, loss = 0.57 (0.006 sec/batch)\n",
      "epoch 27, step 250 / 900, loss = 0.62 (0.006 sec/batch)\n",
      "epoch 27, step 300 / 900, loss = 0.52 (0.006 sec/batch)\n",
      "epoch 27, step 350 / 900, loss = 0.55 (0.006 sec/batch)\n",
      "epoch 27, step 400 / 900, loss = 0.35 (0.006 sec/batch)\n",
      "epoch 27, step 450 / 900, loss = 0.44 (0.006 sec/batch)\n",
      "epoch 27, step 500 / 900, loss = 0.48 (0.006 sec/batch)\n",
      "epoch 27, step 550 / 900, loss = 0.38 (0.006 sec/batch)\n",
      "epoch 27, step 600 / 900, loss = 0.37 (0.007 sec/batch)\n",
      "epoch 27, step 650 / 900, loss = 0.61 (0.006 sec/batch)\n",
      "epoch 27, step 700 / 900, loss = 0.51 (0.006 sec/batch)\n",
      "epoch 27, step 750 / 900, loss = 0.65 (0.006 sec/batch)\n",
      "epoch 27, step 800 / 900, loss = 0.49 (0.006 sec/batch)\n",
      "epoch 27, step 850 / 900, loss = 0.46 (0.006 sec/batch)\n",
      "epoch 27, step 900 / 900, loss = 0.49 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 83.01\n",
      " avg loss = 0.54\n",
      "\n",
      "Validation error:\n",
      " accuracy = 70.88\n",
      " avg loss = 0.89\n",
      "\n",
      "Epoch time: 7.17462086678\n",
      "Plotting in:  /home/mratkovic/deeplearning/outputs/cifar10/training_plot.pdf\n",
      "epoch 28, step 50 / 900, loss = 0.59 (0.006 sec/batch)\n",
      "epoch 28, step 100 / 900, loss = 0.45 (0.006 sec/batch)\n",
      "epoch 28, step 150 / 900, loss = 0.40 (0.006 sec/batch)\n",
      "epoch 28, step 200 / 900, loss = 0.41 (0.006 sec/batch)\n",
      "epoch 28, step 250 / 900, loss = 0.46 (0.006 sec/batch)\n",
      "epoch 28, step 300 / 900, loss = 0.70 (0.006 sec/batch)\n",
      "epoch 28, step 350 / 900, loss = 0.74 (0.006 sec/batch)\n",
      "epoch 28, step 400 / 900, loss = 0.48 (0.006 sec/batch)\n",
      "epoch 28, step 450 / 900, loss = 0.85 (0.006 sec/batch)\n",
      "epoch 28, step 500 / 900, loss = 0.77 (0.006 sec/batch)\n",
      "epoch 28, step 550 / 900, loss = 0.56 (0.006 sec/batch)\n",
      "epoch 28, step 600 / 900, loss = 0.56 (0.006 sec/batch)\n",
      "epoch 28, step 650 / 900, loss = 0.57 (0.006 sec/batch)\n",
      "epoch 28, step 700 / 900, loss = 0.53 (0.006 sec/batch)\n",
      "epoch 28, step 750 / 900, loss = 0.55 (0.006 sec/batch)\n",
      "epoch 28, step 800 / 900, loss = 0.56 (0.006 sec/batch)\n",
      "epoch 28, step 850 / 900, loss = 0.48 (0.006 sec/batch)\n",
      "epoch 28, step 900 / 900, loss = 0.60 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 83.34\n",
      " avg loss = 0.54\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.06\n",
      " avg loss = 0.89\n",
      "\n",
      "Epoch time: 7.18389892578\n",
      "Plotting in:  /home/mratkovic/deeplearning/outputs/cifar10/training_plot.pdf\n",
      "epoch 29, step 50 / 900, loss = 0.47 (0.006 sec/batch)\n",
      "epoch 29, step 100 / 900, loss = 0.58 (0.006 sec/batch)\n",
      "epoch 29, step 150 / 900, loss = 0.54 (0.006 sec/batch)\n",
      "epoch 29, step 200 / 900, loss = 0.58 (0.006 sec/batch)\n",
      "epoch 29, step 250 / 900, loss = 0.72 (0.006 sec/batch)\n",
      "epoch 29, step 300 / 900, loss = 0.74 (0.006 sec/batch)\n",
      "epoch 29, step 350 / 900, loss = 0.60 (0.006 sec/batch)\n",
      "epoch 29, step 400 / 900, loss = 0.62 (0.006 sec/batch)\n",
      "epoch 29, step 450 / 900, loss = 0.39 (0.006 sec/batch)\n",
      "epoch 29, step 500 / 900, loss = 0.39 (0.006 sec/batch)\n",
      "epoch 29, step 550 / 900, loss = 0.57 (0.006 sec/batch)\n",
      "epoch 29, step 600 / 900, loss = 0.44 (0.006 sec/batch)\n",
      "epoch 29, step 650 / 900, loss = 0.61 (0.006 sec/batch)\n",
      "epoch 29, step 700 / 900, loss = 0.61 (0.006 sec/batch)\n",
      "epoch 29, step 750 / 900, loss = 0.45 (0.006 sec/batch)\n",
      "epoch 29, step 800 / 900, loss = 0.63 (0.006 sec/batch)\n",
      "epoch 29, step 850 / 900, loss = 0.40 (0.006 sec/batch)\n",
      "epoch 29, step 900 / 900, loss = 0.61 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 83.37\n",
      " avg loss = 0.53\n",
      "\n",
      "Validation error:\n",
      " accuracy = 70.84\n",
      " avg loss = 0.89\n",
      "\n",
      "Epoch time: 7.16975903511\n",
      "Plotting in:  /home/mratkovic/deeplearning/outputs/cifar10/training_plot.pdf\n",
      "epoch 30, step 50 / 900, loss = 0.67 (0.007 sec/batch)\n",
      "epoch 30, step 100 / 900, loss = 0.59 (0.007 sec/batch)\n",
      "epoch 30, step 150 / 900, loss = 0.54 (0.006 sec/batch)\n",
      "epoch 30, step 200 / 900, loss = 0.72 (0.006 sec/batch)\n",
      "epoch 30, step 250 / 900, loss = 0.49 (0.006 sec/batch)\n",
      "epoch 30, step 300 / 900, loss = 0.56 (0.007 sec/batch)\n",
      "epoch 30, step 350 / 900, loss = 0.30 (0.006 sec/batch)\n",
      "epoch 30, step 400 / 900, loss = 0.48 (0.006 sec/batch)\n",
      "epoch 30, step 450 / 900, loss = 0.62 (0.006 sec/batch)\n",
      "epoch 30, step 500 / 900, loss = 0.69 (0.006 sec/batch)\n",
      "epoch 30, step 550 / 900, loss = 0.59 (0.006 sec/batch)\n",
      "epoch 30, step 600 / 900, loss = 0.63 (0.006 sec/batch)\n",
      "epoch 30, step 650 / 900, loss = 0.67 (0.006 sec/batch)\n",
      "epoch 30, step 700 / 900, loss = 0.69 (0.006 sec/batch)\n",
      "epoch 30, step 750 / 900, loss = 0.63 (0.006 sec/batch)\n",
      "epoch 30, step 800 / 900, loss = 0.59 (0.006 sec/batch)\n",
      "epoch 30, step 850 / 900, loss = 0.55 (0.006 sec/batch)\n",
      "epoch 30, step 900 / 900, loss = 0.44 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 83.50\n",
      " avg loss = 0.53\n",
      "\n",
      "Validation error:\n",
      " accuracy = 70.78\n",
      " avg loss = 0.89\n",
      "\n",
      "Epoch time: 7.17000317574\n",
      "Plotting in:  /home/mratkovic/deeplearning/outputs/cifar10/training_plot.pdf\n",
      "epoch 31, step 50 / 900, loss = 0.54 (0.006 sec/batch)\n",
      "epoch 31, step 100 / 900, loss = 0.62 (0.006 sec/batch)\n",
      "epoch 31, step 150 / 900, loss = 0.59 (0.006 sec/batch)\n",
      "epoch 31, step 200 / 900, loss = 0.54 (0.006 sec/batch)\n",
      "epoch 31, step 250 / 900, loss = 0.66 (0.006 sec/batch)\n",
      "epoch 31, step 300 / 900, loss = 0.57 (0.006 sec/batch)\n",
      "epoch 31, step 350 / 900, loss = 0.75 (0.006 sec/batch)\n",
      "epoch 31, step 400 / 900, loss = 0.50 (0.006 sec/batch)\n",
      "epoch 31, step 450 / 900, loss = 0.34 (0.006 sec/batch)\n",
      "epoch 31, step 500 / 900, loss = 0.61 (0.006 sec/batch)\n",
      "epoch 31, step 550 / 900, loss = 0.48 (0.006 sec/batch)\n",
      "epoch 31, step 600 / 900, loss = 0.64 (0.006 sec/batch)\n",
      "epoch 31, step 650 / 900, loss = 0.55 (0.006 sec/batch)\n",
      "epoch 31, step 700 / 900, loss = 0.45 (0.006 sec/batch)\n",
      "epoch 31, step 750 / 900, loss = 0.35 (0.006 sec/batch)\n",
      "epoch 31, step 800 / 900, loss = 0.43 (0.006 sec/batch)\n",
      "epoch 31, step 850 / 900, loss = 0.53 (0.006 sec/batch)\n",
      "epoch 31, step 900 / 900, loss = 0.51 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 83.56\n",
      " avg loss = 0.53\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.20\n",
      " avg loss = 0.89\n",
      "\n",
      "Epoch time: 7.16818284988\n",
      "Plotting in:  /home/mratkovic/deeplearning/outputs/cifar10/training_plot.pdf\n",
      "epoch 32, step 50 / 900, loss = 0.39 (0.006 sec/batch)\n",
      "epoch 32, step 100 / 900, loss = 0.41 (0.006 sec/batch)\n",
      "epoch 32, step 150 / 900, loss = 0.50 (0.006 sec/batch)\n",
      "epoch 32, step 200 / 900, loss = 0.62 (0.006 sec/batch)\n",
      "epoch 32, step 250 / 900, loss = 0.38 (0.006 sec/batch)\n",
      "epoch 32, step 300 / 900, loss = 0.38 (0.006 sec/batch)\n",
      "epoch 32, step 350 / 900, loss = 0.51 (0.006 sec/batch)\n",
      "epoch 32, step 400 / 900, loss = 0.58 (0.006 sec/batch)\n",
      "epoch 32, step 450 / 900, loss = 0.70 (0.007 sec/batch)\n",
      "epoch 32, step 500 / 900, loss = 0.42 (0.006 sec/batch)\n",
      "epoch 32, step 550 / 900, loss = 0.47 (0.006 sec/batch)\n",
      "epoch 32, step 600 / 900, loss = 0.52 (0.006 sec/batch)\n",
      "epoch 32, step 650 / 900, loss = 0.74 (0.006 sec/batch)\n",
      "epoch 32, step 700 / 900, loss = 0.47 (0.006 sec/batch)\n",
      "epoch 32, step 750 / 900, loss = 0.50 (0.006 sec/batch)\n",
      "epoch 32, step 800 / 900, loss = 0.55 (0.006 sec/batch)\n",
      "epoch 32, step 850 / 900, loss = 0.38 (0.006 sec/batch)\n",
      "epoch 32, step 900 / 900, loss = 0.58 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 83.71\n",
      " avg loss = 0.52\n",
      "\n",
      "Validation error:\n",
      " accuracy = 70.98\n",
      " avg loss = 0.89\n",
      "\n",
      "Epoch time: 7.16568088531\n",
      "Plotting in:  /home/mratkovic/deeplearning/outputs/cifar10/training_plot.pdf\n",
      "epoch 33, step 50 / 900, loss = 0.52 (0.006 sec/batch)\n",
      "epoch 33, step 100 / 900, loss = 0.53 (0.006 sec/batch)\n",
      "epoch 33, step 150 / 900, loss = 0.56 (0.007 sec/batch)\n",
      "epoch 33, step 200 / 900, loss = 0.76 (0.006 sec/batch)\n",
      "epoch 33, step 250 / 900, loss = 0.49 (0.006 sec/batch)\n",
      "epoch 33, step 300 / 900, loss = 0.46 (0.006 sec/batch)\n",
      "epoch 33, step 350 / 900, loss = 0.38 (0.006 sec/batch)\n",
      "epoch 33, step 400 / 900, loss = 0.50 (0.006 sec/batch)\n",
      "epoch 33, step 450 / 900, loss = 0.52 (0.006 sec/batch)\n",
      "epoch 33, step 500 / 900, loss = 0.41 (0.006 sec/batch)\n",
      "epoch 33, step 550 / 900, loss = 0.46 (0.006 sec/batch)\n",
      "epoch 33, step 600 / 900, loss = 0.79 (0.006 sec/batch)\n",
      "epoch 33, step 650 / 900, loss = 0.49 (0.007 sec/batch)\n",
      "epoch 33, step 700 / 900, loss = 0.45 (0.006 sec/batch)\n",
      "epoch 33, step 750 / 900, loss = 0.50 (0.006 sec/batch)\n",
      "epoch 33, step 800 / 900, loss = 0.37 (0.006 sec/batch)\n",
      "epoch 33, step 850 / 900, loss = 0.35 (0.006 sec/batch)\n",
      "epoch 33, step 900 / 900, loss = 0.89 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 83.77\n",
      " avg loss = 0.52\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.24\n",
      " avg loss = 0.89\n",
      "\n",
      "Epoch time: 7.17364215851\n",
      "Plotting in:  /home/mratkovic/deeplearning/outputs/cifar10/training_plot.pdf\n",
      "epoch 34, step 50 / 900, loss = 0.46 (0.006 sec/batch)\n",
      "epoch 34, step 100 / 900, loss = 0.68 (0.006 sec/batch)\n",
      "epoch 34, step 150 / 900, loss = 0.51 (0.006 sec/batch)\n",
      "epoch 34, step 200 / 900, loss = 0.53 (0.006 sec/batch)\n",
      "epoch 34, step 250 / 900, loss = 0.44 (0.006 sec/batch)\n",
      "epoch 34, step 300 / 900, loss = 0.58 (0.006 sec/batch)\n",
      "epoch 34, step 350 / 900, loss = 0.57 (0.007 sec/batch)\n",
      "epoch 34, step 400 / 900, loss = 0.61 (0.006 sec/batch)\n",
      "epoch 34, step 450 / 900, loss = 0.67 (0.006 sec/batch)\n",
      "epoch 34, step 500 / 900, loss = 0.28 (0.006 sec/batch)\n",
      "epoch 34, step 550 / 900, loss = 0.75 (0.006 sec/batch)\n",
      "epoch 34, step 600 / 900, loss = 0.47 (0.006 sec/batch)\n",
      "epoch 34, step 650 / 900, loss = 0.41 (0.006 sec/batch)\n",
      "epoch 34, step 700 / 900, loss = 0.62 (0.006 sec/batch)\n",
      "epoch 34, step 750 / 900, loss = 0.53 (0.006 sec/batch)\n",
      "epoch 34, step 800 / 900, loss = 0.57 (0.007 sec/batch)\n",
      "epoch 34, step 850 / 900, loss = 0.49 (0.006 sec/batch)\n",
      "epoch 34, step 900 / 900, loss = 0.52 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 83.91\n",
      " avg loss = 0.52\n",
      "\n",
      "Validation error:\n",
      " accuracy = 70.82\n",
      " avg loss = 0.89\n",
      "\n",
      "Epoch time: 7.18232393265\n",
      "Plotting in:  /home/mratkovic/deeplearning/outputs/cifar10/training_plot.pdf\n",
      "epoch 35, step 50 / 900, loss = 0.42 (0.006 sec/batch)\n",
      "epoch 35, step 100 / 900, loss = 0.41 (0.006 sec/batch)\n",
      "epoch 35, step 150 / 900, loss = 0.42 (0.006 sec/batch)\n",
      "epoch 35, step 200 / 900, loss = 0.48 (0.006 sec/batch)\n",
      "epoch 35, step 250 / 900, loss = 0.48 (0.006 sec/batch)\n",
      "epoch 35, step 300 / 900, loss = 0.52 (0.006 sec/batch)\n",
      "epoch 35, step 350 / 900, loss = 0.58 (0.006 sec/batch)\n",
      "epoch 35, step 400 / 900, loss = 0.59 (0.006 sec/batch)\n",
      "epoch 35, step 450 / 900, loss = 0.46 (0.006 sec/batch)\n",
      "epoch 35, step 500 / 900, loss = 0.54 (0.006 sec/batch)\n",
      "epoch 35, step 550 / 900, loss = 0.54 (0.006 sec/batch)\n",
      "epoch 35, step 600 / 900, loss = 0.48 (0.006 sec/batch)\n",
      "epoch 35, step 650 / 900, loss = 0.66 (0.006 sec/batch)\n",
      "epoch 35, step 700 / 900, loss = 0.63 (0.006 sec/batch)\n",
      "epoch 35, step 750 / 900, loss = 0.62 (0.006 sec/batch)\n",
      "epoch 35, step 800 / 900, loss = 0.46 (0.006 sec/batch)\n",
      "epoch 35, step 850 / 900, loss = 0.56 (0.006 sec/batch)\n",
      "epoch 35, step 900 / 900, loss = 0.66 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 84.00\n",
      " avg loss = 0.52\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.14\n",
      " avg loss = 0.89\n",
      "\n",
      "Epoch time: 7.19017004967\n",
      "Plotting in:  /home/mratkovic/deeplearning/outputs/cifar10/training_plot.pdf\n",
      "epoch 36, step 50 / 900, loss = 0.68 (0.006 sec/batch)\n",
      "epoch 36, step 100 / 900, loss = 0.60 (0.006 sec/batch)\n",
      "epoch 36, step 150 / 900, loss = 0.60 (0.006 sec/batch)\n",
      "epoch 36, step 200 / 900, loss = 0.49 (0.006 sec/batch)\n",
      "epoch 36, step 250 / 900, loss = 0.51 (0.006 sec/batch)\n",
      "epoch 36, step 300 / 900, loss = 0.62 (0.006 sec/batch)\n",
      "epoch 36, step 350 / 900, loss = 0.41 (0.006 sec/batch)\n",
      "epoch 36, step 400 / 900, loss = 0.54 (0.006 sec/batch)\n",
      "epoch 36, step 450 / 900, loss = 0.61 (0.007 sec/batch)\n",
      "epoch 36, step 500 / 900, loss = 0.52 (0.006 sec/batch)\n",
      "epoch 36, step 550 / 900, loss = 0.58 (0.006 sec/batch)\n",
      "epoch 36, step 600 / 900, loss = 0.50 (0.006 sec/batch)\n",
      "epoch 36, step 650 / 900, loss = 0.62 (0.006 sec/batch)\n",
      "epoch 36, step 700 / 900, loss = 0.60 (0.006 sec/batch)\n",
      "epoch 36, step 750 / 900, loss = 0.43 (0.006 sec/batch)\n",
      "epoch 36, step 800 / 900, loss = 0.60 (0.006 sec/batch)\n",
      "epoch 36, step 850 / 900, loss = 0.65 (0.006 sec/batch)\n",
      "epoch 36, step 900 / 900, loss = 0.44 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 84.02\n",
      " avg loss = 0.51\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.36\n",
      " avg loss = 0.89\n",
      "\n",
      "Epoch time: 7.15866303444\n",
      "Plotting in:  /home/mratkovic/deeplearning/outputs/cifar10/training_plot.pdf\n",
      "epoch 37, step 50 / 900, loss = 0.33 (0.006 sec/batch)\n",
      "epoch 37, step 100 / 900, loss = 0.35 (0.006 sec/batch)\n",
      "epoch 37, step 150 / 900, loss = 0.31 (0.006 sec/batch)\n",
      "epoch 37, step 200 / 900, loss = 0.26 (0.006 sec/batch)\n",
      "epoch 37, step 250 / 900, loss = 0.53 (0.006 sec/batch)\n",
      "epoch 37, step 300 / 900, loss = 0.47 (0.006 sec/batch)\n",
      "epoch 37, step 350 / 900, loss = 0.28 (0.006 sec/batch)\n",
      "epoch 37, step 400 / 900, loss = 0.48 (0.006 sec/batch)\n",
      "epoch 37, step 450 / 900, loss = 0.40 (0.006 sec/batch)\n",
      "epoch 37, step 500 / 900, loss = 0.49 (0.006 sec/batch)\n",
      "epoch 37, step 550 / 900, loss = 0.48 (0.006 sec/batch)\n",
      "epoch 37, step 600 / 900, loss = 0.72 (0.006 sec/batch)\n",
      "epoch 37, step 650 / 900, loss = 0.57 (0.006 sec/batch)\n",
      "epoch 37, step 700 / 900, loss = 0.45 (0.006 sec/batch)\n",
      "epoch 37, step 750 / 900, loss = 0.39 (0.006 sec/batch)\n",
      "epoch 37, step 800 / 900, loss = 0.56 (0.006 sec/batch)\n",
      "epoch 37, step 850 / 900, loss = 0.84 (0.006 sec/batch)\n",
      "epoch 37, step 900 / 900, loss = 0.55 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 84.16\n",
      " avg loss = 0.51\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.12\n",
      " avg loss = 0.89\n",
      "\n",
      "Epoch time: 7.16441392899\n",
      "Plotting in:  /home/mratkovic/deeplearning/outputs/cifar10/training_plot.pdf\n",
      "epoch 38, step 50 / 900, loss = 0.46 (0.006 sec/batch)\n",
      "epoch 38, step 100 / 900, loss = 0.27 (0.006 sec/batch)\n",
      "epoch 38, step 150 / 900, loss = 0.37 (0.007 sec/batch)\n",
      "epoch 38, step 200 / 900, loss = 0.77 (0.006 sec/batch)\n",
      "epoch 38, step 250 / 900, loss = 0.31 (0.006 sec/batch)\n",
      "epoch 38, step 300 / 900, loss = 0.40 (0.006 sec/batch)\n",
      "epoch 38, step 350 / 900, loss = 0.71 (0.006 sec/batch)\n",
      "epoch 38, step 400 / 900, loss = 0.48 (0.006 sec/batch)\n",
      "epoch 38, step 450 / 900, loss = 0.48 (0.006 sec/batch)\n",
      "epoch 38, step 500 / 900, loss = 0.40 (0.006 sec/batch)\n",
      "epoch 38, step 550 / 900, loss = 0.49 (0.006 sec/batch)\n",
      "epoch 38, step 600 / 900, loss = 0.55 (0.006 sec/batch)\n",
      "epoch 38, step 650 / 900, loss = 0.37 (0.006 sec/batch)\n",
      "epoch 38, step 700 / 900, loss = 0.38 (0.006 sec/batch)\n",
      "epoch 38, step 750 / 900, loss = 0.50 (0.006 sec/batch)\n",
      "epoch 38, step 800 / 900, loss = 0.53 (0.006 sec/batch)\n",
      "epoch 38, step 850 / 900, loss = 0.40 (0.006 sec/batch)\n",
      "epoch 38, step 900 / 900, loss = 0.46 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 84.19\n",
      " avg loss = 0.51\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.26\n",
      " avg loss = 0.89\n",
      "\n",
      "Epoch time: 7.16463398933\n",
      "Plotting in:  /home/mratkovic/deeplearning/outputs/cifar10/training_plot.pdf\n",
      "epoch 39, step 50 / 900, loss = 0.50 (0.006 sec/batch)\n",
      "epoch 39, step 100 / 900, loss = 0.57 (0.006 sec/batch)\n",
      "epoch 39, step 150 / 900, loss = 0.57 (0.006 sec/batch)\n",
      "epoch 39, step 200 / 900, loss = 0.46 (0.006 sec/batch)\n",
      "epoch 39, step 250 / 900, loss = 0.55 (0.006 sec/batch)\n",
      "epoch 39, step 300 / 900, loss = 0.48 (0.006 sec/batch)\n",
      "epoch 39, step 350 / 900, loss = 0.63 (0.006 sec/batch)\n",
      "epoch 39, step 400 / 900, loss = 0.35 (0.006 sec/batch)\n",
      "epoch 39, step 450 / 900, loss = 0.40 (0.006 sec/batch)\n",
      "epoch 39, step 500 / 900, loss = 0.66 (0.006 sec/batch)\n",
      "epoch 39, step 550 / 900, loss = 0.43 (0.006 sec/batch)\n",
      "epoch 39, step 600 / 900, loss = 0.55 (0.006 sec/batch)\n",
      "epoch 39, step 650 / 900, loss = 0.53 (0.006 sec/batch)\n",
      "epoch 39, step 700 / 900, loss = 0.70 (0.006 sec/batch)\n",
      "epoch 39, step 750 / 900, loss = 0.60 (0.006 sec/batch)\n",
      "epoch 39, step 800 / 900, loss = 0.74 (0.006 sec/batch)\n",
      "epoch 39, step 850 / 900, loss = 0.41 (0.006 sec/batch)\n",
      "epoch 39, step 900 / 900, loss = 0.33 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 84.19\n",
      " avg loss = 0.51\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.16\n",
      " avg loss = 0.89\n",
      "\n",
      "Epoch time: 7.17116594315\n",
      "Plotting in:  /home/mratkovic/deeplearning/outputs/cifar10/training_plot.pdf\n",
      "epoch 40, step 50 / 900, loss = 0.77 (0.006 sec/batch)\n",
      "epoch 40, step 100 / 900, loss = 0.43 (0.006 sec/batch)\n",
      "epoch 40, step 150 / 900, loss = 0.38 (0.006 sec/batch)\n",
      "epoch 40, step 200 / 900, loss = 0.93 (0.006 sec/batch)\n",
      "epoch 40, step 250 / 900, loss = 0.49 (0.006 sec/batch)\n",
      "epoch 40, step 300 / 900, loss = 0.45 (0.006 sec/batch)\n",
      "epoch 40, step 350 / 900, loss = 0.79 (0.006 sec/batch)\n",
      "epoch 40, step 400 / 900, loss = 0.74 (0.006 sec/batch)\n",
      "epoch 40, step 450 / 900, loss = 0.45 (0.006 sec/batch)\n",
      "epoch 40, step 500 / 900, loss = 0.44 (0.006 sec/batch)\n",
      "epoch 40, step 550 / 900, loss = 0.48 (0.006 sec/batch)\n",
      "epoch 40, step 600 / 900, loss = 0.61 (0.006 sec/batch)\n",
      "epoch 40, step 650 / 900, loss = 0.65 (0.006 sec/batch)\n",
      "epoch 40, step 700 / 900, loss = 0.35 (0.006 sec/batch)\n",
      "epoch 40, step 750 / 900, loss = 0.42 (0.006 sec/batch)\n",
      "epoch 40, step 800 / 900, loss = 0.44 (0.006 sec/batch)\n",
      "epoch 40, step 850 / 900, loss = 0.66 (0.006 sec/batch)\n",
      "epoch 40, step 900 / 900, loss = 0.59 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 84.36\n",
      " avg loss = 0.51\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.16\n",
      " avg loss = 0.89\n",
      "\n",
      "Epoch time: 7.16504788399\n",
      "Plotting in:  /home/mratkovic/deeplearning/outputs/cifar10/training_plot.pdf\n",
      "Total train time: 301.064818144\n"
     ]
    }
   ],
   "source": [
    "session = tf.Session()\n",
    "train_start = time.time()\n",
    "train(session, train_x, train_y, valid_x, valid_y, config)\n",
    "\n",
    "print(\"Total train time:\", time.time() - train_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45000, 32, 32, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAGFCAYAAAAW1j91AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7790110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAGFCAYAAAAW1j91AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOAAAIv8fkv5pY6JC2JwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7b81e50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAGFCAYAAAAW1j91AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngAjFAQAQ+f9jiXCaJVef0AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x80c1ad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(train_x[0], data_mean, data_std)\n",
    "show_image(train_x[1], data_mean, data_std)\n",
    "show_image(train_x[2], data_mean, data_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
